# ğŸ¼ pandas DataFrame å®Ÿå‹™å®Œå…¨ãƒã‚¹ã‚¿ãƒ¼è¬›åº§ï¼šAdvanced ç‰ˆ

> **å¯¾è±¡è€…**: Python åŸºç¤çŸ¥è­˜ã‚ã‚Šãƒ»å®Ÿå‹™ã§ãƒ‡ãƒ¼ã‚¿åˆ†æã‚’è¡Œã„ãŸã„æ–¹  
> **æœŸé–“**: 8 é€±é–“ã§å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã¾ã§  
> **é‡ç‚¹åˆ†é‡**: å®Ÿå‹™ãƒ‡ãƒ¼ã‚¿å‡¦ç†ï¼ˆ70%ï¼‰+ é«˜åº¦ãªåˆ†ææ‰‹æ³•ï¼ˆ30%ï¼‰

---

## ğŸ“‹ å­¦ç¿’ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

### ğŸ¯ Week 1: pandas åŸºç¤ã¨ DataFrame å®Œå…¨ç†è§£

### ğŸ¯ Week 2: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ“ä½œã¨ãƒ‡ãƒ¼ã‚¿é¸æŠã®æ¥µæ„

### ğŸ¯ Week 3: ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã¨å‰å‡¦ç†ã®å®Ÿè·µ

### ğŸ¯ Week 4: ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆapply/lambda æ´»ç”¨ï¼‰

### ğŸ¯ Week 5: ãƒ‡ãƒ¼ã‚¿çµåˆãƒã‚¹ã‚¿ãƒ¼ï¼ˆconcat/merge å®Œå…¨æ”»ç•¥ï¼‰

### ğŸ¯ Week 6: é›†è¨ˆãƒ»ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ãƒ»æ™‚ç³»åˆ—åˆ†æ

### ğŸ¯ Week 7: é«˜åº¦ãªå¯è¦–åŒ–ã¨çµ±è¨ˆåˆ†æ

### ğŸ¯ Week 8: å®Ÿå‹™ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç·åˆæ¼”ç¿’

---

## Week 1: pandas åŸºç¤ã¨ DataFrame å®Œå…¨ç†è§£

### ğŸ¯ **ã“ã®é€±ã®ã‚´ãƒ¼ãƒ«**

- DataFrame ã®å†…éƒ¨æ§‹é€ ã‚’å®Œå…¨ç†è§£ã™ã‚‹
- ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ„è­˜ã§ãã‚‹
- å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ä½œæˆãƒ»èª­ã¿è¾¼ã¿ã‚’ãƒã‚¹ã‚¿ãƒ¼ã™ã‚‹

---

### **Day 1-2: DataFrame ã®å†…éƒ¨æ§‹é€ ã¨åŠ¹ç‡çš„ãªä½œæˆ**

```python
print("ğŸ—ï¸ pandas DataFrame å®Ÿå‹™å®Œå…¨ãƒã‚¹ã‚¿ãƒ¼è¬›åº§ - Advancedç‰ˆ")
print("=" * 60)

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

print("1. DataFrameã®å†…éƒ¨æ§‹é€ ã‚’ç†è§£ã™ã‚‹")
print("=" * 40)

# DataFrameã®åŸºæœ¬æ§‹é€ 
sample_data = {
    "ID": [1, 2, 3, 4, 5],
    "åå‰": ["ç”°ä¸­", "ä½è—¤", "éˆ´æœ¨", "é«˜æ©‹", "å±±ç”°"],
    "å¹´é½¢": [25, 30, 35, 28, 32],
    "çµ¦ä¸": [400000, 550000, 600000, 480000, 520000],
    "éƒ¨ç½²": ["å–¶æ¥­", "é–‹ç™º", "å–¶æ¥­", "äººäº‹", "é–‹ç™º"]
}

df = pd.DataFrame(sample_data)
print("ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«DataFrame:")
print(df)

print("\nğŸ” DataFrameã®å†…éƒ¨æ§‹é€ :")
print(f"å½¢çŠ¶ï¼ˆshapeï¼‰: {df.shape}")
print(f"æ¬¡å…ƒæ•°ï¼ˆndimï¼‰: {df.ndim}")
print(f"è¦ç´ æ•°ï¼ˆsizeï¼‰: {df.size}")
print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {df.memory_usage(deep=True).sum()} bytes")

print("\nğŸ“‹ è©³ç´°ãªæƒ…å ±:")
df.info(memory_usage='deep')

print("\nğŸ’¾ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–:")
# ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–å‰
print("æœ€é©åŒ–å‰ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡:")
print(df.memory_usage(deep=True))

# æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®æœ€é©åŒ–
df_optimized = df.copy()
df_optimized['ID'] = df_optimized['ID'].astype('int8')  # 1-255ã®ç¯„å›²ãªã‚‰
df_optimized['å¹´é½¢'] = df_optimized['å¹´é½¢'].astype('int8')  # 0-127ã®ç¯„å›²ãªã‚‰
df_optimized['çµ¦ä¸'] = df_optimized['çµ¦ä¸'].astype('int32')  # å¤§ããªæ•°å€¤

# ã‚«ãƒ†ã‚´ãƒªå‹ã®æ´»ç”¨
df_optimized['éƒ¨ç½²'] = df_optimized['éƒ¨ç½²'].astype('category')

print("\næœ€é©åŒ–å¾Œã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡:")
print(df_optimized.memory_usage(deep=True))

print("\nãƒ‡ãƒ¼ã‚¿å‹ã®å¤‰åŒ–:")
print(pd.DataFrame({
    'å…ƒã®ãƒ‡ãƒ¼ã‚¿å‹': df.dtypes,
    'æœ€é©åŒ–å¾Œ': df_optimized.dtypes,
    'å…ƒã®ãƒ¡ãƒ¢ãƒª': df.memory_usage(deep=True),
    'æœ€é©åŒ–å¾Œãƒ¡ãƒ¢ãƒª': df_optimized.memory_usage(deep=True)
}))

print("\n2. åŠ¹ç‡çš„ãªDataFrameä½œæˆæ–¹æ³•")
print("=" * 40)

# æ–¹æ³•1: å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®åŠ¹ç‡çš„ä½œæˆ
print("ğŸ“ˆ å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®åŠ¹ç‡çš„ä½œæˆ:")

# éåŠ¹ç‡ãªæ–¹æ³•ï¼ˆé¿ã‘ã‚‹ã¹ãï¼‰
import time
start_time = time.time()
df_inefficient = pd.DataFrame()
for i in range(1000):
    new_row = pd.DataFrame({'å€¤': [i]})
    df_inefficient = pd.concat([df_inefficient, new_row], ignore_index=True)
inefficient_time = time.time() - start_time

# åŠ¹ç‡çš„ãªæ–¹æ³•
start_time = time.time()
data_list = [{'å€¤': i} for i in range(1000)]
df_efficient = pd.DataFrame(data_list)
efficient_time = time.time() - start_time

print(f"éåŠ¹ç‡ãªæ–¹æ³•: {inefficient_time:.4f}ç§’")
print(f"åŠ¹ç‡çš„ãªæ–¹æ³•: {efficient_time:.4f}ç§’")
print(f"é€Ÿåº¦æ”¹å–„: {inefficient_time/efficient_time:.1f}å€é«˜é€Ÿ")

# æ–¹æ³•2: NumPyé…åˆ—ã‹ã‚‰ã®ä½œæˆï¼ˆæœ€é€Ÿï¼‰
print("\nâš¡ NumPyé…åˆ—ã‹ã‚‰ã®ä½œæˆ:")
np.random.seed(42)

start_time = time.time()
# 100ä¸‡è¡Œã®ãƒ‡ãƒ¼ã‚¿
large_data = {
    'A': np.random.randint(1, 100, 1000000),
    'B': np.random.normal(50, 15, 1000000),
    'C': np.random.choice(['X', 'Y', 'Z'], 1000000)
}
df_large = pd.DataFrame(large_data)
numpy_time = time.time() - start_time

print(f"100ä¸‡è¡Œãƒ‡ãƒ¼ã‚¿ä½œæˆæ™‚é–“: {numpy_time:.4f}ç§’")
print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {df_large.memory_usage(deep=True).sum() / 1024**2:.1f} MB")

print("\n3. å®Ÿå‹™çš„ãªãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿")
print("=" * 40)

# CSVèª­ã¿è¾¼ã¿ã®æœ€é©åŒ–
print("ğŸ“ CSVèª­ã¿è¾¼ã¿ã®æœ€é©åŒ–:")

# ã‚µãƒ³ãƒ—ãƒ«CSVãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
sample_csv_data = {
    'date': pd.date_range('2024-01-01', periods=10000, freq='H'),
    'value1': np.random.normal(100, 20, 10000),
    'value2': np.random.randint(1, 1000, 10000),
    'category': np.random.choice(['A', 'B', 'C', 'D'], 10000),
    'flag': np.random.choice([True, False], 10000)
}
sample_df = pd.DataFrame(sample_csv_data)

# CSVä¿å­˜
sample_df.to_csv('sample_data.csv', index=False)

print("ğŸ”§ èª­ã¿è¾¼ã¿æœ€é©åŒ–ã®ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯:")

# åŸºæœ¬çš„ãªèª­ã¿è¾¼ã¿
basic_df = pd.read_csv('sample_data.csv')
print(f"åŸºæœ¬èª­ã¿è¾¼ã¿: {basic_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB")

# æœ€é©åŒ–ã•ã‚ŒãŸèª­ã¿è¾¼ã¿
optimized_df = pd.read_csv(
    'sample_data.csv',
    dtype={
        'value2': 'int16',  # ãƒ‡ãƒ¼ã‚¿å‹ã‚’æŒ‡å®š
        'category': 'category',  # ã‚«ãƒ†ã‚´ãƒªå‹ã§èª­ã¿è¾¼ã¿
        'flag': 'bool'
    },
    parse_dates=['date'],  # æ—¥ä»˜å‹ã¨ã—ã¦è§£æ
    chunksize=None  # ãƒãƒ£ãƒ³ã‚¯èª­ã¿è¾¼ã¿ã‚‚å¯èƒ½
)
print(f"æœ€é©åŒ–èª­ã¿è¾¼ã¿: {optimized_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB")

# å¿…è¦ãªåˆ—ã®ã¿èª­ã¿è¾¼ã¿
selected_df = pd.read_csv('sample_data.csv', usecols=['date', 'value1', 'category'])
print(f"é¸æŠèª­ã¿è¾¼ã¿: {selected_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB")

print("\n4. é«˜åº¦ãªDataFrameæ“ä½œ")
print("=" * 40)

# ãƒã‚§ãƒ¼ãƒ³ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆMethod Chainingï¼‰
print("ğŸ”— ãƒ¡ã‚½ãƒƒãƒ‰ãƒã‚§ãƒ¼ãƒ³ã®æ´»ç”¨:")

result = (df
    .assign(
        å¹´é½¢ã‚«ãƒ†ã‚´ãƒª=lambda x: pd.cut(x['å¹´é½¢'], bins=[0, 30, 40, 100],
                               labels=['è‹¥æ‰‹', 'ä¸­å …', 'ãƒ™ãƒ†ãƒ©ãƒ³']),
        çµ¦ä¸ãƒ¬ãƒ™ãƒ«=lambda x: pd.qcut(x['çµ¦ä¸'], q=3, labels=['ä½', 'ä¸­', 'é«˜'])
    )
    .query('å¹´é½¢ >= 25')
    .sort_values('çµ¦ä¸', ascending=False)
    .reset_index(drop=True)
)

print("ãƒ¡ã‚½ãƒƒãƒ‰ãƒã‚§ãƒ¼ãƒ³çµæœ:")
print(result)

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†
print("\nğŸ”€ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†:")

def add_bonus(df):
    """ãƒœãƒ¼ãƒŠã‚¹åˆ—ã‚’è¿½åŠ """
    df = df.copy()
    df['ãƒœãƒ¼ãƒŠã‚¹'] = df['çµ¦ä¸'] * 2.5
    return df

def add_total_compensation(df):
    """å¹´åã‚’è¨ˆç®—"""
    df = df.copy()
    df['å¹´å'] = (df['çµ¦ä¸'] * 12) + df['ãƒœãƒ¼ãƒŠã‚¹']
    return df

pipeline_result = (df
    .pipe(add_bonus)
    .pipe(add_total_compensation)
    .round(0)
)

print("ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†çµæœ:")
print(pipeline_result[['åå‰', 'çµ¦ä¸', 'ãƒœãƒ¼ãƒŠã‚¹', 'å¹´å']])

print("\n5. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒ‡ãƒãƒƒã‚°")
print("=" * 40)

# ä¸€èˆ¬çš„ãªã‚¨ãƒ©ãƒ¼ã¨å¯¾å‡¦æ³•
print("âš ï¸ ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã¨å¯¾å‡¦æ³•:")

# KeyErrorå¯¾ç­–
try:
    value = df['å­˜åœ¨ã—ãªã„åˆ—']
except KeyError as e:
    print(f"KeyErrorç™ºç”Ÿ: {e}")
    print("å¯¾å‡¦æ³•: åˆ—ã®å­˜åœ¨ç¢ºèª")
    if 'å­˜åœ¨ã—ãªã„åˆ—' in df.columns:
        value = df['å­˜åœ¨ã—ãªã„åˆ—']
    else:
        print("åˆ—ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚åˆ©ç”¨å¯èƒ½ãªåˆ—:", df.columns.tolist())

# ãƒ‡ãƒ¼ã‚¿å‹ã‚¨ãƒ©ãƒ¼å¯¾ç­–
try:
    result = df['åå‰'] + df['å¹´é½¢']  # æ–‡å­—åˆ— + æ•°å€¤
except TypeError as e:
    print(f"\nTypeErrorç™ºç”Ÿ: æ–‡å­—åˆ—ã¨æ•°å€¤ã‚’çµåˆã—ã‚ˆã†ã¨ã—ã¾ã—ãŸ")
    print("å¯¾å‡¦æ³•: ãƒ‡ãƒ¼ã‚¿å‹ã‚’çµ±ä¸€")
    result = df['åå‰'] + df['å¹´é½¢'].astype(str) + "æ­³"
    print("ä¿®æ­£çµæœ:", result.tolist())

# æ¬ æå€¤ã‚¨ãƒ©ãƒ¼å¯¾ç­–
df_with_na = df.copy()
df_with_na.loc[2, 'å¹´é½¢'] = np.nan

print(f"\næ¬ æå€¤ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ã§ã®è¨ˆç®—:")
print(f"å¹³å‡å¹´é½¢: {df_with_na['å¹´é½¢'].mean():.1f}")  # æ¬ æå€¤ã¯è‡ªå‹•ã§é™¤å¤–
print(f"æ¬ æå€¤ã®å€‹æ•°: {df_with_na['å¹´é½¢'].isna().sum()}")

print("\nâœ… Week 1 å®Œäº†ï¼šDataFrameã®åŸºç¤ã‚’å®Œå…¨ç†è§£ã—ã¾ã—ãŸï¼")
```

---

## Week 2: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ“ä½œã¨ãƒ‡ãƒ¼ã‚¿é¸æŠã®æ¥µæ„

### ğŸ¯ **ã“ã®é€±ã®ã‚´ãƒ¼ãƒ«**

- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æ¦‚å¿µã‚’å®Œå…¨ç†è§£ã™ã‚‹
- éšå±¤ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è‡ªåœ¨ã«æ“ã‚‹
- é«˜é€Ÿãªãƒ‡ãƒ¼ã‚¿é¸æŠãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã‚’ãƒã‚¹ã‚¿ãƒ¼ã™ã‚‹

---

### **Day 1-3: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å®Œå…¨ç†è§£**

```python
print("\nğŸ“‡ Week 2: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ“ä½œã¨ãƒ‡ãƒ¼ã‚¿é¸æŠã®æ¥µæ„")
print("=" * 50)

print("1. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã¯ä½•ã‹ï¼Ÿ")
print("=" * 30)

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®åŸºæœ¬æ¦‚å¿µ
df = pd.DataFrame({
    'å•†å“å': ['ã‚Šã‚“ã”', 'ãƒãƒŠãƒŠ', 'ã‚ªãƒ¬ãƒ³ã‚¸', 'ã¶ã©ã†'],
    'ä¾¡æ ¼': [100, 80, 120, 200],
    'åœ¨åº«': [50, 30, 25, 15]
})

print("ğŸ“Š ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹:")
print(df)
print(f"\nã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {df.index}")
print(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å‹: {type(df.index)}")
print(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®åå‰: {df.index.name}")

print("\nğŸ” ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®è©³ç´°:")
print(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å€¤: {df.index.tolist()}")
print(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ‡ãƒ¼ã‚¿å‹: {df.index.dtype}")
print(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®é•·ã•: {len(df.index)}")
print(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¯ä¸€æ„ã‹: {df.index.is_unique}")

print("\nğŸ“‹ ã‚«ãƒ©ãƒ ã«ã¤ã„ã¦:")
print(f"ã‚«ãƒ©ãƒ : {df.columns}")
print(f"ã‚«ãƒ©ãƒ ã®å‹: {type(df.columns)}")
print(f"ã‚«ãƒ©ãƒ æ•°: {len(df.columns)}")

print("\n2. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®è¨­å®šã¨æ“ä½œ")
print("=" * 30)

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®è¨­å®š
print("ğŸ”§ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®è¨­å®šæ–¹æ³•:")

# æ–¹æ³•1: set_indexã§æ—¢å­˜åˆ—ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«
df_indexed = df.set_index('å•†å“å')
print("å•†å“åã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¨­å®š:")
print(df_indexed)
print(f"æ–°ã—ã„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {df_indexed.index}")

# æ–¹æ³•2: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ç›´æ¥æŒ‡å®š
df_custom = df.copy()
df_custom.index = ['æœç‰©1', 'æœç‰©2', 'æœç‰©3', 'æœç‰©4']
print("\nã‚«ã‚¹ã‚¿ãƒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹:")
print(df_custom)

# æ–¹æ³•3: ä½œæˆæ™‚ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æŒ‡å®š
df_with_index = pd.DataFrame({
    'ä¾¡æ ¼': [100, 80, 120, 200],
    'åœ¨åº«': [50, 30, 25, 15]
}, index=['ã‚Šã‚“ã”', 'ãƒãƒŠãƒŠ', 'ã‚ªãƒ¬ãƒ³ã‚¸', 'ã¶ã©ã†'])
print("\nä½œæˆæ™‚ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æŒ‡å®š:")
print(df_with_index)

print("\nğŸ”„ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æ“ä½œ:")

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒªã‚»ãƒƒãƒˆ
df_reset = df_indexed.reset_index()
print("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆ:")
print(df_reset)

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åã®è¨­å®š
df_named = df_indexed.copy()
df_named.index.name = 'å•†å“'
print("\nã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«åå‰ã‚’è¨­å®š:")
print(df_named)

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å¤‰æ›´
df_renamed = df_indexed.rename(index={'ã‚Šã‚“ã”': 'ã‚¢ãƒƒãƒ—ãƒ«', 'ãƒãƒŠãƒŠ': 'ãƒãƒŠãƒ¼ãƒŠ'})
print("\nã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®åå‰å¤‰æ›´:")
print(df_renamed)

print("\n3. éšå±¤ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆMultiIndexï¼‰")
print("=" * 30)

# éšå±¤ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆ
print("ğŸ—ï¸ éšå±¤ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆ:")

# å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã§éšå±¤ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
sales_data = {
    'å£²ä¸Š': [100, 120, 80, 90, 150, 130, 110, 140],
    'åˆ©ç›Š': [30, 36, 24, 27, 45, 39, 33, 42]
}

# 2éšå±¤ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
multi_index = pd.MultiIndex.from_tuples([
    ('æ±äº¬', 'æ–°å®¿åº—'),
    ('æ±äº¬', 'æ¸‹è°·åº—'),
    ('æ±äº¬', 'æ± è¢‹åº—'),
    ('å¤§é˜ª', 'æ¢…ç”°åº—'),
    ('å¤§é˜ª', 'é›£æ³¢åº—'),
    ('å¤§é˜ª', 'å¤©ç‹å¯ºåº—'),
    ('åå¤å±‹', 'æ „åº—'),
    ('åå¤å±‹', 'åé§…åº—')
], names=['åœ°åŸŸ', 'åº—èˆ—'])

df_multi = pd.DataFrame(sales_data, index=multi_index)
print("éšå±¤ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿:")
print(df_multi)

print(f"\nã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ¬ãƒ™ãƒ«æ•°: {df_multi.index.nlevels}")
print(f"ãƒ¬ãƒ™ãƒ«å: {df_multi.index.names}")
print(f"ãƒ¬ãƒ™ãƒ«0ã®å€¤: {df_multi.index.get_level_values(0).unique()}")
print(f"ãƒ¬ãƒ™ãƒ«1ã®å€¤: {df_multi.index.get_level_values(1).unique()}")

print("\nğŸ¯ éšå±¤ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§ã®é¸æŠ:")

# ãƒ¬ãƒ™ãƒ«0ã§ã®é¸æŠ
print("æ±äº¬ã®åº—èˆ—:")
print(df_multi.loc['æ±äº¬'])

# ç‰¹å®šã®åº—èˆ—
print("\næ–°å®¿åº—ã®ãƒ‡ãƒ¼ã‚¿:")
print(df_multi.loc[('æ±äº¬', 'æ–°å®¿åº—')])

# ãƒ¬ãƒ™ãƒ«ã”ã¨ã®é›†è¨ˆ
print("\nåœ°åŸŸåˆ¥åˆè¨ˆ:")
print(df_multi.groupby(level=0).sum())

# éšå±¤ã®å…¥ã‚Œæ›¿ãˆ
df_swapped = df_multi.swaplevel()
print("\néšå±¤ã‚’å…¥ã‚Œæ›¿ãˆ:")
print(df_swapped.head())

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä¸¦ã³æ›¿ãˆ
df_sorted = df_multi.sort_index()
print("\nã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§ä¸¦ã³æ›¿ãˆ:")
print(df_sorted)

print("\n4. é«˜é€Ÿãªãƒ‡ãƒ¼ã‚¿é¸æŠãƒ†ã‚¯ãƒ‹ãƒƒã‚¯")
print("=" * 30)

# å¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§æ€§èƒ½æ¯”è¼ƒ
print("âš¡ æ€§èƒ½æ¯”è¼ƒï¼ˆå¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼‰:")

# å¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
np.random.seed(42)
large_df = pd.DataFrame({
    'A': np.random.randint(1, 1000, 100000),
    'B': np.random.normal(0, 1, 100000),
    'C': np.random.choice(['X', 'Y', 'Z'], 100000)
})

import time

# æ–¹æ³•1: boolean indexing
start = time.time()
result1 = large_df[large_df['A'] > 500]
time1 = time.time() - start

# æ–¹æ³•2: query method
start = time.time()
result2 = large_df.query('A > 500')
time2 = time.time() - start

print(f"Boolean indexing: {time1:.4f}ç§’")
print(f"Query method: {time2:.4f}ç§’")

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨­å®šå¾Œã®é«˜é€Ÿé¸æŠ
large_df_indexed = large_df.set_index('A').sort_index()

start = time.time()
result3 = large_df_indexed.loc[500:1000]  # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¯„å›²é¸æŠ
time3 = time.time() - start

print(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¯„å›²é¸æŠ: {time3:.4f}ç§’")

print("\nğŸ” é«˜åº¦ãªé¸æŠæ–¹æ³•:")

# isin ã‚’ä½¿ã£ãŸåŠ¹ç‡çš„ãªé¸æŠ
categories = ['X', 'Y']
start = time.time()
result_isin = large_df[large_df['C'].isin(categories)]
time_isin = time.time() - start

# è¤‡æ•°æ¡ä»¶ã§ã®é¸æŠï¼ˆåŠ¹ç‡çš„ï¼‰
start = time.time()
result_multiple = large_df.query('A > 500 and C in @categories')
time_multiple = time.time() - start

print(f"isiné¸æŠ: {time_isin:.4f}ç§’")
print(f"è¤‡æ•°æ¡ä»¶query: {time_multiple:.4f}ç§’")

print("\n5. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹")
print("=" * 30)

print("ğŸ’¡ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ´»ç”¨ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹:")

# 1. æ¤œç´¢ãŒå¤šã„åˆ—ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«
print("1. ã‚ˆãæ¤œç´¢ã™ã‚‹åˆ—ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¨­å®š")
customer_df = pd.DataFrame({
    'customer_id': range(1000, 2000),
    'name': [f'Customer{i}' for i in range(1000)],
    'purchase_amount': np.random.randint(100, 10000, 1000)
})

# customer_idã§ã‚ˆãæ¤œç´¢ã™ã‚‹å ´åˆ
customer_indexed = customer_df.set_index('customer_id')
print("é¡§å®¢ID1500ã®æƒ…å ±:", customer_indexed.loc[1500, 'name'])

# 2. æ—¥ä»˜ãƒ‡ãƒ¼ã‚¿ã¯å¿…ãšDatetimeIndexã«
print("\n2. æ—¥ä»˜ãƒ‡ãƒ¼ã‚¿ã¯DatetimeIndexã‚’ä½¿ç”¨")
date_df = pd.DataFrame({
    'date': pd.date_range('2024-01-01', periods=365, freq='D'),
    'sales': np.random.randint(10000, 50000, 365)
})
date_df = date_df.set_index('date')

# æ—¥ä»˜ç¯„å›²ã§ã®é«˜é€Ÿé¸æŠ
q1_sales = date_df.loc['2024-01':'2024-03']
print(f"Q1ã®å£²ä¸Šãƒ‡ãƒ¼ã‚¿: {len(q1_sales)}æ—¥åˆ†")

# 3. ã‚«ãƒ†ã‚´ãƒªãƒ‡ãƒ¼ã‚¿ã§ã®åŠ¹ç‡çš„ãªå‡¦ç†
print("\n3. ã‚«ãƒ†ã‚´ãƒªãƒ‡ãƒ¼ã‚¿ã®åŠ¹ç‡çš„å‡¦ç†")
category_df = pd.DataFrame({
    'product_category': pd.Categorical(['Electronics', 'Clothing', 'Books'] * 1000),
    'sales': np.random.randint(100, 1000, 3000)
})

# ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®é›†è¨ˆï¼ˆé«˜é€Ÿï¼‰
category_summary = category_df.groupby('product_category').agg({
    'sales': ['sum', 'mean', 'count']
})
print("ã‚«ãƒ†ã‚´ãƒªåˆ¥å£²ä¸Šã‚µãƒãƒªãƒ¼:")
print(category_summary)

print("\nâœ… Week 2 å®Œäº†ï¼šã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ“ä½œã‚’ãƒã‚¹ã‚¿ãƒ¼ã—ã¾ã—ãŸï¼")
```

### **Day 4-7: é«˜åº¦ãªãƒ‡ãƒ¼ã‚¿é¸æŠã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°**

```python
print("\n6. é«˜åº¦ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯")
print("=" * 30)

# è¤‡é›‘ãªæ¡ä»¶ã§ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
sales_df = pd.DataFrame({
    'date': pd.date_range('2024-01-01', periods=1000, freq='D'),
    'product': np.random.choice(['A', 'B', 'C', 'D'], 1000),
    'region': np.random.choice(['North', 'South', 'East', 'West'], 1000),
    'sales': np.random.normal(1000, 300, 1000),
    'profit': np.random.normal(200, 100, 1000)
})

print("ğŸ¯ è¤‡é›‘ãªæ¡ä»¶ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°:")

# 1. çµ±è¨ˆçš„æ¡ä»¶
print("1. çµ±è¨ˆçš„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°:")
high_sales = sales_df[sales_df['sales'] > sales_df['sales'].quantile(0.9)]
print(f"ä¸Šä½10%ã®å£²ä¸Š: {len(high_sales)}ä»¶")

# 2. è¤‡æ•°åˆ—ã®æ¡ä»¶çµ„ã¿åˆã‚ã›
print("\n2. è¤‡æ•°åˆ—æ¡ä»¶çµ„ã¿åˆã‚ã›:")
complex_filter = sales_df[
    (sales_df['sales'] > 1200) &
    (sales_df['profit'] > 150) &
    (sales_df['region'].isin(['North', 'East']))
]
print(f"è¤‡åˆæ¡ä»¶ã«åˆè‡´: {len(complex_filter)}ä»¶")

# 3. æ–‡å­—åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
print("\n3. é«˜åº¦ãªæ–‡å­—åˆ—ãƒãƒƒãƒãƒ³ã‚°:")
product_df = pd.DataFrame({
    'product_name': ['iPhone_14', 'Samsung_Galaxy', 'iPad_Pro', 'MacBook_Air', 'Surface_Pro'],
    'brand': ['Apple', 'Samsung', 'Apple', 'Apple', 'Microsoft'],
    'price': [80000, 70000, 90000, 120000, 110000]
})

# æ­£è¦è¡¨ç¾ã‚’ä½¿ã£ãŸé¸æŠ
apple_products = product_df[product_df['product_name'].str.contains(r'iPhone|iPad|MacBook')]
print("Appleè£½å“:")
print(apple_products)

# ä¾¡æ ¼ç¯„å›²ã§ã®é¸æŠ
mid_range = product_df[product_df['price'].between(70000, 100000)]
print("\nä¾¡æ ¼70,000-100,000å††ã®å•†å“:")
print(mid_range)

print("\n7. å‹•çš„ãªãƒ‡ãƒ¼ã‚¿é¸æŠ")
print("=" * 30)

# å‹•çš„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é–¢æ•°
def dynamic_filter(df, conditions):
    """
    å‹•çš„ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¡ä»¶ã‚’é©ç”¨

    Parameters:
    df: DataFrame
    conditions: dict of conditions
    """
    result = df.copy()

    for column, condition in conditions.items():
        if isinstance(condition, dict):
            if 'min' in condition:
                result = result[result[column] >= condition['min']]
            if 'max' in condition:
                result = result[result[column] <= condition['max']]
            if 'values' in condition:
                result = result[result[column].isin(condition['values'])]
        else:
            result = result[result[column] == condition]

    return result

# ä½¿ç”¨ä¾‹
filter_conditions = {
    'sales': {'min': 800, 'max': 1500},
    'region': {'values': ['North', 'South']},
    'product': 'A'
}

filtered_data = dynamic_filter(sales_df, filter_conditions)
print(f"å‹•çš„ãƒ•ã‚£ãƒ«ã‚¿çµæœ: {len(filtered_data)}ä»¶")

print("\n8. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯")
print("=" * 30)

# ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–ã«ã‚ˆã‚‹é«˜é€ŸåŒ–
print("âš¡ ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–:")

# æœ€é©åŒ–å‰
large_sales = pd.DataFrame({
    'product_id': np.random.randint(1, 1000, 100000),
    'quantity': np.random.randint(1, 100, 100000),
    'price': np.random.uniform(10, 1000, 100000),
    'category': np.random.choice(['A', 'B', 'C', 'D'], 100000)
})

print(f"æœ€é©åŒ–å‰ãƒ¡ãƒ¢ãƒª: {large_sales.memory_usage(deep=True).sum() / 1024**2:.1f}MB")

# æœ€é©åŒ–å¾Œ
large_sales_opt = large_sales.copy()
large_sales_opt['product_id'] = large_sales_opt['product_id'].astype('int16')
large_sales_opt['quantity'] = large_sales_opt['quantity'].astype('int8')
large_sales_opt['price'] = large_sales_opt['price'].astype('float32')
large_sales_opt['category'] = large_sales_opt['category'].astype('category')

print(f"æœ€é©åŒ–å¾Œãƒ¡ãƒ¢ãƒª: {large_sales_opt.memory_usage(deep=True).sum() / 1024**2:.1f}MB")

# ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é€Ÿåº¦æ¯”è¼ƒ
start = time.time()
result_orig = large_sales[large_sales['category'] == 'A']
time_orig = time.time() - start

start = time.time()
result_opt = large_sales_opt[large_sales_opt['category'] == 'A']
time_opt = time.time() - start

print(f"ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ™‚é–“ - æœ€é©åŒ–å‰: {time_orig:.4f}ç§’")
print(f"ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ™‚é–“ - æœ€é©åŒ–å¾Œ: {time_opt:.4f}ç§’")

print("\nâœ… Week 2 å®Œäº†ï¼šé«˜åº¦ãªãƒ‡ãƒ¼ã‚¿é¸æŠã‚’ãƒã‚¹ã‚¿ãƒ¼ã—ã¾ã—ãŸï¼")
```

---

## Week 3: ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã¨å‰å‡¦ç†ã®å®Ÿè·µ

### ğŸ¯ **ã“ã®é€±ã®ã‚´ãƒ¼ãƒ«**

- å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ãƒã‚¹ã‚¿ãƒ¼ã™ã‚‹
- æ§˜ã€…ãªãƒ‡ãƒ¼ã‚¿å“è³ªå•é¡Œã«å¯¾å‡¦ã§ãã‚‹
- è‡ªå‹•åŒ–ã•ã‚ŒãŸã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹

---

### **Day 1-3: æ¬ æå€¤å‡¦ç†ã®å®Œå…¨æ”»ç•¥**

```python
print("\nğŸ§¹ Week 3: ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã¨å‰å‡¦ç†ã®å®Ÿè·µ")
print("=" * 50)

print("1. å®Ÿå‹™ã«ãŠã‘ã‚‹æ¬ æå€¤ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç†è§£")
print("=" * 40)

# å®Ÿéš›ã®ãƒ“ã‚¸ãƒã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’æ¨¡æ“¬
np.random.seed(42)

# è¤‡é›‘ãªæ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŒã¤ãƒ‡ãƒ¼ã‚¿ä½œæˆ
customer_data = []
for i in range(1000):
    # åŸºæœ¬æƒ…å ±
    customer_id = f"C{i+1:04d}"
    age = np.random.randint(18, 80)

    # å¹´åï¼ˆé«˜é½¢è€…ã¯é€€è·ã§æœªå›ç­”å¤šã„ï¼‰
    if age > 65:
        income = np.nan if np.random.random() > 0.3 else np.random.randint(0, 300)
    else:
        income = np.random.randint(200, 1000) if np.random.random() > 0.1 else np.nan

    # é›»è©±ç•ªå·ï¼ˆè‹¥ã„ä¸–ä»£ã¯å›ºå®šé›»è©±ãªã—ï¼‰
    if age < 30:
        phone = "" if np.random.random() > 0.2 else f"03-{np.random.randint(1000,9999)}-{np.random.randint(1000,9999)}"
    else:
        phone = f"03-{np.random.randint(1000,9999)}-{np.random.randint(1000,9999)}" if np.random.random() > 0.05 else ""

    # è³¼å…¥å±¥æ­´ï¼ˆæ–°è¦é¡§å®¢ã¯è³¼å…¥å±¥æ­´ãªã—ï¼‰
    days_since_reg = np.random.randint(1, 1000)
    if days_since_reg < 30:
        last_purchase = np.nan
        total_purchases = 0
    else:
        last_purchase = pd.Timestamp('2024-01-01') - pd.Timedelta(days=np.random.randint(1, days_since_reg))
        total_purchases = np.random.randint(1, 50)

    customer_data.append({
        'customer_id': customer_id,
        'age': age,
        'income': income,
        'phone': phone,
        'registration_date': pd.Timestamp('2024-01-01') - pd.Timedelta(days=days_since_reg),
        'last_purchase_date': last_purchase,
        'total_purchases': total_purchases,
        'preferred_category': np.random.choice(['Fashion', 'Electronics', 'Books', '', 'Sports'],
                                            p=[0.25, 0.25, 0.15, 0.1, 0.25])
    })

df_customers = pd.DataFrame(customer_data)

print("ğŸ“Š å®Ÿå‹™ãƒ‡ãƒ¼ã‚¿ã®æ¬ æå€¤ãƒ‘ã‚¿ãƒ¼ãƒ³:")
print(df_customers.head())

print("\nğŸ” æ¬ æå€¤ã®è©³ç´°åˆ†æ:")
missing_analysis = pd.DataFrame({
    'æ¬ ææ•°': df_customers.isnull().sum(),
    'æ¬ æç‡(%)': (df_customers.isnull().sum() / len(df_customers) * 100).round(2),
    'ãƒ‡ãƒ¼ã‚¿å‹': df_customers.dtypes
})
print(missing_analysis)

# æ¬ æå€¤ã®ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
print("\nğŸ“ˆ æ¬ æå€¤ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¯è¦–åŒ–:")
import matplotlib.pyplot as plt

# æ¬ æå€¤ã®çµ„ã¿åˆã‚ã›ãƒ‘ã‚¿ãƒ¼ãƒ³
missing_patterns = df_customers.isnull()
pattern_counts = missing_patterns.groupby(list(missing_patterns.columns)).size().sort_values(ascending=False)
print("ä¸»è¦ãªæ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆä¸Šä½5ï¼‰:")
print(pattern_counts.head())

print("\n2. æ¬ æå€¤ã®ç¨®é¡ã¨å¯¾å‡¦æ³•")
print("=" * 40)

print("ğŸ’¡ æ¬ æå€¤ã®åˆ†é¡ã¨æˆ¦ç•¥:")

# MCAR (Missing Completely At Random) - å®Œå…¨ã«ãƒ©ãƒ³ãƒ€ãƒ 
print("1. MCARï¼ˆå®Œå…¨ãƒ©ãƒ³ãƒ€ãƒ æ¬ æï¼‰:")
# ä¾‹ï¼šã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ã§ä¸€éƒ¨ãƒ‡ãƒ¼ã‚¿ãŒæ¬ æ
mcar_example = df_customers.copy()
random_missing = np.random.choice(df_customers.index, 50, replace=False)
mcar_example.loc[random_missing, 'total_purchases'] = np.nan
print(f"ãƒ©ãƒ³ãƒ€ãƒ æ¬ æã‚’è¿½åŠ : {mcar_example['total_purchases'].isnull().sum()}ä»¶")

# MAR (Missing At Random) - ä»–ã®å¤‰æ•°ã«ä¾å­˜ã—ãŸæ¬ æ
print("\n2. MARï¼ˆä»–å¤‰æ•°ä¾å­˜æ¬ æï¼‰:")
# ä¾‹ï¼šé«˜é½¢è€…ã®å¹´åæœªå›ç­”ï¼ˆå¹´é½¢ã«ä¾å­˜ï¼‰
mar_analysis = df_customers.groupby(pd.cut(df_customers['age'], bins=[0, 30, 50, 65, 100]))['income'].apply(
    lambda x: x.isnull().sum() / len(x) * 100
)
print("å¹´é½¢å±¤åˆ¥å¹´åæ¬ æç‡:")
print(mar_analysis.round(2))

# MNAR (Missing Not At Random) - æ¬ æå€¤è‡ªä½“ã«æ„å‘³ãŒã‚ã‚‹
print("\n3. MNARï¼ˆéãƒ©ãƒ³ãƒ€ãƒ æ¬ æï¼‰:")
# ä¾‹ï¼šé«˜åå…¥è€…ã®å¹´åæœªå›ç­”ï¼ˆãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼é‡è¦–ï¼‰
high_income_mask = df_customers['income'] > 800
print(f"é«˜åå…¥è€…ã®å¹´åå›ç­”ç‡: {((~df_customers.loc[high_income_mask, 'income'].isnull()).sum() / high_income_mask.sum() * 100):.1f}%")

print("\n3. é«˜åº¦ãªæ¬ æå€¤è£œå®Œæ‰‹æ³•")
print("=" * 40)

# æº–å‚™ï¼šä½œæ¥­ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
df_impute = df_customers.copy()

print("ğŸ”§ è£œå®Œæ‰‹æ³•ã®å®Ÿè£…:")

# 1. åŸºæœ¬çµ±è¨ˆã«ã‚ˆã‚‹è£œå®Œ
print("1. åŸºæœ¬çµ±è¨ˆè£œå®Œ:")
# å¹´é½¢å±¤åˆ¥ã®å¹´åä¸­å¤®å€¤ã§è£œå®Œ
age_groups = pd.cut(df_impute['age'], bins=[0, 30, 50, 65, 100], labels=['20ä»£', '30-40ä»£', '50-60ä»£', '60ä»£ä»¥ä¸Š'])
income_by_age = df_impute.groupby(age_groups)['income'].median()

def fill_income_by_age(row):
    if pd.isna(row['income']):
        age_group = pd.cut([row['age']], bins=[0, 30, 50, 65, 100], labels=['20ä»£', '30-40ä»£', '50-60ä»£', '60ä»£ä»¥ä¸Š'])[0]
        return income_by_age[age_group]
    return row['income']

df_impute['income_filled'] = df_impute.apply(fill_income_by_age, axis=1)
print("å¹´é½¢å±¤åˆ¥å¹´åä¸­å¤®å€¤ã§è£œå®Œå®Œäº†")

# 2. å‰æ–¹ãƒ»å¾Œæ–¹è£œå®Œï¼ˆæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ï¼‰
print("\n2. æ™‚ç³»åˆ—è£œå®Œ:")
df_sorted = df_impute.sort_values('registration_date')
df_sorted['income_ffill'] = df_sorted['income'].fillna(method='ffill')
df_sorted['income_bfill'] = df_sorted['income'].fillna(method='bfill')
print("å‰æ–¹ãƒ»å¾Œæ–¹è£œå®Œå®Œäº†")

# 3. è£œé–“æ³•
print("\n3. è£œé–“æ³•:")
df_sorted['income_interpolate'] = df_sorted['income'].interpolate(method='linear')
print("ç·šå½¢è£œé–“å®Œäº†")

# 4. æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹è£œå®Œï¼ˆç°¡æ˜“ç‰ˆï¼‰
print("\n4. äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«è£œå®Œ:")
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

# å¹´åäºˆæ¸¬ã®ãŸã‚ã®ç‰¹å¾´é‡æº–å‚™
features_df = df_impute[['age', 'total_purchases']].copy()
features_df['days_since_reg'] = (pd.Timestamp('2024-01-01') - df_impute['registration_date']).dt.days

# å¹´åãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹è¡Œã§å­¦ç¿’
complete_mask = ~df_impute['income'].isnull()
X_train = features_df[complete_mask]
y_train = df_impute.loc[complete_mask, 'income']

# ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# æ¬ æå€¤äºˆæ¸¬
missing_mask = df_impute['income'].isnull()
X_missing = features_df[missing_mask]
predicted_income = rf_model.predict(X_missing)

df_impute.loc[missing_mask, 'income_predicted'] = predicted_income
print("ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã«ã‚ˆã‚‹äºˆæ¸¬è£œå®Œå®Œäº†")

# è£œå®Œçµæœã®æ¯”è¼ƒ
print("\nğŸ“Š è£œå®Œæ‰‹æ³•ã®æ¯”è¼ƒ:")
comparison_df = pd.DataFrame({
    'å…ƒãƒ‡ãƒ¼ã‚¿': df_impute['income'],
    'å¹´é½¢å±¤åˆ¥ä¸­å¤®å€¤': df_impute['income_filled'],
    'RFäºˆæ¸¬': df_impute.get('income_predicted', np.nan)
}).fillna('æ¬ æ')

print(comparison_df.head(10))

print("\n4. ãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡")
print("=" * 40)

def data_quality_report(df, target_col):
    """ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒãƒ¼ãƒˆä½œæˆ"""
    report = {}

    # åŸºæœ¬çµ±è¨ˆ
    report['total_records'] = len(df)
    report['missing_count'] = df[target_col].isnull().sum()
    report['missing_rate'] = report['missing_count'] / report['total_records'] * 100

    # æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆ
    valid_data = df[target_col].dropna()
    if len(valid_data) > 0:
        report['mean'] = valid_data.mean()
        report['median'] = valid_data.median()
        report['std'] = valid_data.std()
        report['min'] = valid_data.min()
        report['max'] = valid_data.max()

    return report

# å„è£œå®Œæ‰‹æ³•ã®å“è³ªè©•ä¾¡
original_quality = data_quality_report(df_impute, 'income')
filled_quality = data_quality_report(df_impute, 'income_filled')

print("ğŸ“‹ å“è³ªãƒ¬ãƒãƒ¼ãƒˆ:")
print(f"å…ƒãƒ‡ãƒ¼ã‚¿æ¬ æç‡: {original_quality['missing_rate']:.2f}%")
print(f"è£œå®Œå¾Œæ¬ æç‡: {filled_quality['missing_rate']:.2f}%")
print(f"å…ƒãƒ‡ãƒ¼ã‚¿å¹³å‡: {original_quality.get('mean', 'N/A')}")
print(f"è£œå®Œå¾Œå¹³å‡: {filled_quality.get('mean', 'N/A')}")

print("\nâœ… Week 3 Day 1-3 å®Œäº†ï¼šæ¬ æå€¤å‡¦ç†ã‚’ãƒã‚¹ã‚¿ãƒ¼ã—ã¾ã—ãŸï¼")
```

### **Day 4-7: ãƒ‡ãƒ¼ã‚¿ç•°å¸¸å€¤æ¤œå‡ºã¨æ–‡å­—åˆ—ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°**

```python
print("\n5. ç•°å¸¸å€¤æ¤œå‡ºã¨å‡¦ç†")
print("=" * 40)

# ç•°å¸¸å€¤ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
np.random.seed(42)
sales_data = []

for i in range(1000):
    # é€šå¸¸ã®å£²ä¸Šãƒ‡ãƒ¼ã‚¿
    if np.random.random() > 0.05:  # 95%ã¯æ­£å¸¸ãƒ‡ãƒ¼ã‚¿
        sales = np.random.normal(10000, 2000)
        if sales < 0:
            sales = abs(sales)  # è² ã®å£²ä¸Šã¯çµ¶å¯¾å€¤ã«
    else:  # 5%ã¯ç•°å¸¸å€¤
        if np.random.random() > 0.5:
            sales = np.random.normal(50000, 5000)  # ç•°å¸¸ã«é«˜ã„å£²ä¸Š
        else:
            sales = np.random.uniform(0, 100)  # ç•°å¸¸ã«ä½ã„å£²ä¸Š

    sales_data.append({
        'date': pd.Timestamp('2024-01-01') + pd.Timedelta(days=np.random.randint(0, 365)),
        'store_id': np.random.choice(['S001', 'S002', 'S003', 'S004'], p=[0.4, 0.3, 0.2, 0.1]),
        'sales': sales,
        'customer_count': max(1, int(sales / 500 + np.random.normal(0, 5)))
    })

sales_df = pd.DataFrame(sales_data)

print("ğŸ“Š å£²ä¸Šãƒ‡ãƒ¼ã‚¿ï¼ˆç•°å¸¸å€¤å«ã‚€ï¼‰:")
print(sales_df.describe())

print("\nğŸ” ç•°å¸¸å€¤æ¤œå‡ºæ‰‹æ³•:")

# 1. çµ±è¨ˆçš„æ‰‹æ³•ï¼ˆIQRæ³•ï¼‰
def detect_outliers_iqr(df, column):
    """IQRæ³•ã«ã‚ˆã‚‹ç•°å¸¸å€¤æ¤œå‡º"""
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
    return outliers, (lower_bound, upper_bound)

outliers_iqr, bounds_iqr = detect_outliers_iqr(sales_df, 'sales')
print(f"1. IQRæ³•æ¤œå‡ºç•°å¸¸å€¤: {len(outliers_iqr)}ä»¶")
print(f"   æ­£å¸¸ç¯„å›²: {bounds_iqr[0]:.0f} - {bounds_iqr[1]:.0f}")

# 2. Z-scoreæ³•
def detect_outliers_zscore(df, column, threshold=3):
    """Z-scoreæ³•ã«ã‚ˆã‚‹ç•°å¸¸å€¤æ¤œå‡º"""
    z_scores = np.abs((df[column] - df[column].mean()) / df[column].std())
    outliers = df[z_scores > threshold]
    return outliers

outliers_zscore = detect_outliers_zscore(sales_df, 'sales')
print(f"2. Z-scoreæ³•æ¤œå‡ºç•°å¸¸å€¤: {len(outliers_zscore)}ä»¶")

# 3. ä¿®æ­£Z-scoreæ³•ï¼ˆMADä½¿ç”¨ï¼‰
def detect_outliers_modified_zscore(df, column, threshold=3.5):
    """ä¿®æ­£Z-scoreæ³•ï¼ˆMADä½¿ç”¨ï¼‰"""
    median = df[column].median()
    mad = np.median(np.abs(df[column] - median))
    modified_z_scores = 0.6745 * (df[column] - median) / mad
    outliers = df[np.abs(modified_z_scores) > threshold]
    return outliers

outliers_mad = detect_outliers_modified_zscore(sales_df, 'sales')
print(f"3. ä¿®æ­£Z-scoreæ³•æ¤œå‡ºç•°å¸¸å€¤: {len(outliers_mad)}ä»¶")

# 4. æ©Ÿæ¢°å­¦ç¿’æ‰‹æ³•ï¼ˆIsolation Forestï¼‰
from sklearn.ensemble import IsolationForest

isolation_forest = IsolationForest(contamination=0.1, random_state=42)
outlier_labels = isolation_forest.fit_predict(sales_df[['sales', 'customer_count']])
outliers_if = sales_df[outlier_labels == -1]
print(f"4. Isolation Forestæ¤œå‡ºç•°å¸¸å€¤: {len(outliers_if)}ä»¶")

print("\nğŸ› ï¸ ç•°å¸¸å€¤ã®å‡¦ç†æ–¹æ³•:")

# å‡¦ç†æ–¹æ³•1: å‰Šé™¤
sales_cleaned_drop = sales_df[~sales_df.index.isin(outliers_iqr.index)]
print(f"å‰Šé™¤å‡¦ç†å¾Œ: {len(sales_cleaned_drop)}ä»¶ï¼ˆ{len(sales_df) - len(sales_cleaned_drop)}ä»¶å‰Šé™¤ï¼‰")

# å‡¦ç†æ–¹æ³•2: ã‚­ãƒ£ãƒƒãƒ”ãƒ³ã‚°ï¼ˆä¸Šä¸‹é™å€¤ã§ç½®æ›ï¼‰
sales_cleaned_cap = sales_df.copy()
sales_cleaned_cap['sales'] = sales_cleaned_cap['sales'].clip(lower=bounds_iqr[0], upper=bounds_iqr[1])
print(f"ã‚­ãƒ£ãƒƒãƒ”ãƒ³ã‚°å‡¦ç†: ä¸‹é™{bounds_iqr[0]:.0f}, ä¸Šé™{bounds_iqr[1]:.0f}")

# å‡¦ç†æ–¹æ³•3: å¤‰æ›ï¼ˆå¯¾æ•°å¤‰æ›ï¼‰
sales_cleaned_log = sales_df.copy()
sales_cleaned_log['sales_log'] = np.log1p(sales_cleaned_log['sales'])  # log1p = log(1+x)
print("å¯¾æ•°å¤‰æ›å‡¦ç†å®Œäº†")

print("\n6. é«˜åº¦ãªæ–‡å­—åˆ—ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°")
print("=" * 40)

# å®Ÿå‹™çš„ãªæ±šã‚ŒãŸæ–‡å­—åˆ—ãƒ‡ãƒ¼ã‚¿
messy_data = {
    'customer_name': [
        'ç”°ä¸­ã€€å¤ªéƒ', '  ä½è—¤èŠ±å­  ', 'SUZUKI Jiro', 'ãŸã‹ã¯ã—ã€€ã¿ã•ã',
        'å±±ç”°ä¸€éƒã€€ã€€', 'ä¸­æ‘ã€€å¥å¤ª', 'Tanaka Hanako', 'éˆ´æœ¨ã€€æ¬¡éƒ',
        '', None, 'ç”°ä¸­ï¼ˆå¤ªéƒï¼‰', 'ä½è—¤-èŠ±å­'
    ],
    'email': [
        'tanaka@email.com', 'SATO@EMAIL.COM', 'suzuki@gmail', 'takahashi@',
        '', 'yamada@@email.com', 'nakamura@email.com.', 'tanaka@email',
        'hanako.tanaka@company.co.jp', None, 'invalid-email', 'test@test.test'
    ],
    'phone': [
        '090-1234-5678', '03(1234)5678', '0312345678', '090 1234 5678',
        '090-1234-567', '090-1234-56789', '', '03-1234-5678',
        '81-90-1234-5678', '+81-90-1234-5678', 'invalid', None
    ],
    'address': [
        'æ±äº¬éƒ½æ–°å®¿åŒº1-2-3', 'å¤§é˜ªåºœå¤§é˜ªå¸‚åŒ—åŒºï¼”ï¼ï¼•ï¼ï¼–', 'æ„›çŸ¥çœŒåå¤å±‹å¸‚',
        'ã€’100-0001 æ±äº¬éƒ½åƒä»£ç”°åŒº', 'ç¥å¥ˆå·çœŒæ¨ªæµœå¸‚æ¸¯åŒ—åŒºæ–°æ¨ªæµœï¼’ï¼ï¼•ï¼ï¼˜',
        '', 'ç¦å²¡çœŒç¦å²¡å¸‚åšå¤šåŒº', 'åŒ—æµ·é“æœ­å¹Œå¸‚ä¸­å¤®åŒº',
        'æ²–ç¸„çœŒé‚£è¦‡å¸‚', 'æ±äº¬éƒ½ã€€æ–°å®¿åŒºã€€ã€€ï¼‘ï¼ï¼’ï¼ï¼“', None, 'ç„¡åŠ¹ãªä½æ‰€'
    ]
}

messy_df = pd.DataFrame(messy_data)
print("ğŸ“‹ æ±šã‚ŒãŸæ–‡å­—åˆ—ãƒ‡ãƒ¼ã‚¿:")
print(messy_df)

print("\nğŸ§½ æ–‡å­—åˆ—ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å‡¦ç†:")

# 1. åŸºæœ¬çš„ãªã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
def clean_name(name):
    """åå‰ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
    if pd.isna(name) or name == '':
        return None

    # å‰å¾Œã®ç©ºç™½å‰Šé™¤
    name = str(name).strip()

    # å…¨è§’ç©ºç™½ã‚’åŠè§’ã«
    name = name.replace('ã€€', ' ')

    # è¤‡æ•°ã®ç©ºç™½ã‚’1ã¤ã«
    import re
    name = re.sub(r'\s+', ' ', name)

    # æ‹¬å¼§ãªã©ã®é™¤å»
    name = re.sub(r'[ï¼ˆï¼‰()ã€ã€‘\[\]ï¼-]', '', name)

    return name if name else None

messy_df['customer_name_clean'] = messy_df['customer_name'].apply(clean_name)

# 2. ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã®æ¤œè¨¼ã¨ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
def clean_email(email):
    """ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¤œè¨¼"""
    if pd.isna(email) or email == '':
        return None

    email = str(email).strip().lower()

    # åŸºæœ¬çš„ãªãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹å½¢å¼ãƒã‚§ãƒƒã‚¯
    import re
    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'

    # ä¸€èˆ¬çš„ãªä¿®æ­£
    email = email.replace('@@', '@')  # é‡è¤‡@ã®ä¿®æ­£
    email = re.sub(r'\.+$', '', email)  # æœ«å°¾ã®ãƒ”ãƒªã‚ªãƒ‰å‰Šé™¤

    if re.match(pattern, email):
        return email
    else:
        return None

messy_df['email_clean'] = messy_df['email'].apply(clean_email)

# 3. é›»è©±ç•ªå·ã®æ­£è¦åŒ–
def clean_phone(phone):
    """é›»è©±ç•ªå·ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ­£è¦åŒ–"""
    if pd.isna(phone) or phone == '':
        return None

    phone = str(phone).strip()

    # æ•°å­—ã®ã¿æŠ½å‡º
    import re
    digits = re.sub(r'[^\d]', '', phone)

    # å›½éš›ç•ªå·ã®å‡¦ç†
    if digits.startswith('81'):
        digits = digits[2:]

    # é•·ã•ãƒã‚§ãƒƒã‚¯
    if len(digits) == 10:  # å›ºå®šé›»è©±
        return f"{digits[:2]}-{digits[2:6]}-{digits[6:]}"
    elif len(digits) == 11:  # æºå¸¯é›»è©±
        return f"{digits[:3]}-{digits[3:7]}-{digits[7:]}"
    else:
        return None

messy_df['phone_clean'] = messy_df['phone'].apply(clean_phone)

# 4. ä½æ‰€ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
def clean_address(address):
    """ä½æ‰€ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
    if pd.isna(address) or address == '':
        return None

    address = str(address).strip()

    # éƒµä¾¿ç•ªå·ã®é™¤å»
    import re
    address = re.sub(r'ã€’?\d{3}-?\d{4}\s*', '', address)

    # å…¨è§’æ•°å­—ã‚’åŠè§’ã«
    address = address.translate(str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789'))

    # ç©ºç™½ã®æ­£è¦åŒ–
    address = re.sub(r'\s+', '', address)

    # åŸºæœ¬çš„ãªä½æ‰€å½¢å¼ãƒã‚§ãƒƒã‚¯
    if any(pref in address for pref in ['éƒ½', 'åºœ', 'çœŒ']) and any(city in address for city in ['å¸‚', 'åŒº', 'ç”º', 'æ‘']):
        return address
    else:
        return None

messy_df['address_clean'] = messy_df['address'].apply(clean_address)

print("ğŸ” ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°çµæœ:")
comparison_cols = ['customer_name', 'customer_name_clean', 'email', 'email_clean']
print(messy_df[comparison_cols].head())

print("\nğŸ“Š ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹æœ:")
for col in ['customer_name', 'email', 'phone', 'address']:
    original_valid = messy_df[col].notna().sum()
    clean_valid = messy_df[f'{col}_clean'].notna().sum()
    print(f"{col}: {original_valid} â†’ {clean_valid} (æœ‰åŠ¹ç‡: {clean_valid/len(messy_df)*100:.1f}%)")

print("\n7. è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³")
print("=" * 40)

class DataCleaner:
    """è‡ªå‹•ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""

    def __init__(self):
        self.cleaning_log = []

    def clean_dataframe(self, df):
        """DataFrameã®è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        df_clean = df.copy()

        for column in df_clean.columns:
            if df_clean[column].dtype == 'object':
                df_clean[column] = self._clean_text_column(df_clean[column], column)
            elif df_clean[column].dtype in ['int64', 'float64']:
                df_clean[column] = self._clean_numeric_column(df_clean[column], column)

        return df_clean

    def _clean_text_column(self, series, column_name):
        """ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        original_nulls = series.isnull().sum()

        # åŸºæœ¬çš„ãªã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        cleaned = series.str.strip()
        cleaned = cleaned.replace('', np.nan)

        # ç©ºç™½ã®æ­£è¦åŒ–
        cleaned = cleaned.str.replace(r'\s+', ' ', regex=True)

        new_nulls = cleaned.isnull().sum()
        self.cleaning_log.append(f"{column_name}: {original_nulls} â†’ {new_nulls} nullå€¤")

        return cleaned

    def _clean_numeric_column(self, series, column_name):
        """æ•°å€¤åˆ—ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        original_nulls = series.isnull().sum()

        # ç•°å¸¸å€¤æ¤œå‡ºï¼ˆIQRæ³•ï¼‰
        Q1 = series.quantile(0.25)
        Q3 = series.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # ç•°å¸¸å€¤ã‚’NaNã«
        cleaned = series.copy()
        outlier_mask = (series < lower_bound) | (series > upper_bound)
        cleaned[outlier_mask] = np.nan

        outlier_count = outlier_mask.sum()
        new_nulls = cleaned.isnull().sum()

        self.cleaning_log.append(f"{column_name}: {outlier_count}å€‹ã®ç•°å¸¸å€¤ã‚’æ¤œå‡º, {original_nulls} â†’ {new_nulls} nullå€¤")

        return cleaned

    def get_cleaning_report(self):
        """ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¬ãƒãƒ¼ãƒˆã‚’å–å¾—"""
        return "\n".join(self.cleaning_log)

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½¿ç”¨ä¾‹
cleaner = DataCleaner()
sales_cleaned = cleaner.clean_dataframe(sales_df)

print("ğŸ¤– è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°çµæœ:")
print(cleaner.get_cleaning_report())

print("\nâœ… Week 3 å®Œäº†ï¼šãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ãƒã‚¹ã‚¿ãƒ¼ã—ã¾ã—ãŸï¼")
```

---

## Week 4: ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆapply/lambda æ´»ç”¨ï¼‰

### ğŸ¯ **ã“ã®é€±ã®ã‚´ãƒ¼ãƒ«**

- apply()ã¨ lambda å¼ã‚’å®Œå…¨ç†è§£ã™ã‚‹
- å®Ÿå‹™çš„ãªç‰¹å¾´é‡ä½œæˆãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã‚’ãƒã‚¹ã‚¿ãƒ¼ã™ã‚‹
- é«˜åº¦ãªå¤‰æ›å‡¦ç†ã‚’åŠ¹ç‡çš„ã«å®Ÿè£…ã™ã‚‹

---

### **Day 1-4: apply()ã¨ lambda å¼ã®å®Œå…¨æ”»ç•¥**

```python
print("\nğŸ”§ Week 4: ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆapply/lambdaæ´»ç”¨ï¼‰")
print("=" * 50)

print("1. apply()ã¨lambdaå¼ã®åŸºæœ¬æ¦‚å¿µ")
print("=" * 40)

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
customer_data = pd.DataFrame({
    'customer_id': range(1, 1001),
    'age': np.random.randint(18, 80, 1000),
    'income': np.random.normal(50000, 20000, 1000),
    'purchase_amount': np.random.exponential(1000, 1000),
    'purchase_frequency': np.random.poisson(5, 1000),
    'registration_date': pd.date_range('2020-01-01', periods=1000, freq='D'),
    'last_login': pd.date_range('2024-01-01', periods=1000, freq='H'),
    'product_category': np.random.choice(['Electronics', 'Fashion', 'Books', 'Sports'], 1000)
})

print("ğŸ“Š é¡§å®¢ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«:")
print(customer_data.head())

print("\nğŸ¯ apply()ã®åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³:")

# 1. å˜ä¸€åˆ—ã«å¯¾ã™ã‚‹applyï¼ˆSeries.applyï¼‰
print("1. Series.apply() - å˜ä¸€åˆ—å¤‰æ›:")

# ã‚·ãƒ³ãƒ—ãƒ«ãªlambdaå¼
customer_data['age_category'] = customer_data['age'].apply(lambda x: 'Young' if x < 30 else 'Middle' if x < 50 else 'Senior')
print("å¹´é½¢ã‚«ãƒ†ã‚´ãƒªä½œæˆ:")
print(customer_data[['age', 'age_category']].head())

# ã‚ˆã‚Šè¤‡é›‘ãªé–¢æ•°
def income_level(income):
    """åå…¥ãƒ¬ãƒ™ãƒ«ã®åˆ†é¡"""
    if income < 30000:
        return 'Low'
    elif income < 70000:
        return 'Medium'
    else:
        return 'High'

customer_data['income_level'] = customer_data['income'].apply(income_level)
print("\nåå…¥ãƒ¬ãƒ™ãƒ«åˆ†é¡:")
print(customer_data[['income', 'income_level']].head())

# 2. è¤‡æ•°åˆ—ã«å¯¾ã™ã‚‹applyï¼ˆDataFrame.applyï¼‰
print("\n2. DataFrame.apply() - è¤‡æ•°åˆ—ä½¿ç”¨:")

# axis=1ã§è¡Œæ–¹å‘ã«é©ç”¨
def customer_score(row):
    """é¡§å®¢ã‚¹ã‚³ã‚¢è¨ˆç®—"""
    age_score = 100 - row['age']  # è‹¥ã„ã»ã©é«˜ã‚¹ã‚³ã‚¢
    income_score = row['income'] / 1000  # åå…¥ã‚’1000ã§å‰²ã‚‹
    frequency_score = row['purchase_frequency'] * 10
    return age_score + income_score + frequency_score

customer_data['customer_score'] = customer_data.apply(customer_score, axis=1)
print("é¡§å®¢ã‚¹ã‚³ã‚¢è¨ˆç®—:")
print(customer_data[['age', 'income', 'purchase_frequency', 'customer_score']].head())

# lambdaå¼ã§ã®è¤‡æ•°åˆ—ä½¿ç”¨
customer_data['purchase_per_freq'] = customer_data.apply(
    lambda row: row['purchase_amount'] / max(row['purchase_frequency'], 1), axis=1
)
print("\nè³¼å…¥é »åº¦ã‚ãŸã‚Šã®é‡‘é¡:")
print(customer_data[['purchase_amount', 'purchase_frequency', 'purchase_per_freq']].head())

print("\n3. é«˜åº¦ãªapply()æ´»ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³")
print("=" * 40)

# 3. æ¡ä»¶åˆ†å²ã‚’å«ã‚€è¤‡é›‘ãªå‡¦ç†
def customer_segment(row):
    """é¡§å®¢ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†é¡"""
    age = row['age']
    income = row['income']
    frequency = row['purchase_frequency']

    if age < 30:
        if income > 50000:
            return 'Young_HighIncome'
        else:
            return 'Young_LowIncome'
    elif age < 50:
        if frequency > 5:
            return 'Middle_HighFreq'
        else:
            return 'Middle_LowFreq'
    else:
        if income > 60000:
            return 'Senior_Wealthy'
        else:
            return 'Senior_Standard'

customer_data['segment'] = customer_data.apply(customer_segment, axis=1)
print("é¡§å®¢ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†æ:")
segment_summary = customer_data['segment'].value_counts()
print(segment_summary)

# 4. æ—¥ä»˜è¨ˆç®—ã‚’å«ã‚€å‡¦ç†
print("\nğŸ“… æ—¥ä»˜é–¢é€£ã®ç‰¹å¾´é‡:")

# ç™»éŒ²ã‹ã‚‰ã®çµŒéæ—¥æ•°
customer_data['days_since_registration'] = customer_data.apply(
    lambda row: (pd.Timestamp('2024-01-01') - row['registration_date']).days, axis=1
)

# æœ€çµ‚ãƒ­ã‚°ã‚¤ãƒ³ã‹ã‚‰ã®çµŒéæ™‚é–“
customer_data['hours_since_last_login'] = customer_data.apply(
    lambda row: (pd.Timestamp('2024-01-01 12:00:00') - row['last_login']).total_seconds() / 3600, axis=1
)

print("æ—¥ä»˜é–¢é€£ç‰¹å¾´é‡:")
print(customer_data[['registration_date', 'days_since_registration', 'last_login', 'hours_since_last_login']].head())

# 5. æ–‡å­—åˆ—å‡¦ç†ã‚’å«ã‚€ç‰¹å¾´é‡
print("\nğŸ“ æ–‡å­—åˆ—å‡¦ç†ç‰¹å¾´é‡:")

# ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰ç‰¹å¾´é‡ä½œæˆ
def category_features(category):
    """ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰è¤‡æ•°ç‰¹å¾´é‡ã‚’ä½œæˆ"""
    features = {}
    features['is_tech'] = 1 if category == 'Electronics' else 0
    features['is_fashion'] = 1 if category == 'Fashion' else 0
    features['category_length'] = len(category)
    return pd.Series(features)

category_features_df = customer_data['product_category'].apply(category_features)
customer_data = pd.concat([customer_data, category_features_df], axis=1)

print("ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡:")
print(customer_data[['product_category', 'is_tech', 'is_fashion', 'category_length']].head())

print("\n4. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–")
print("=" * 40)

# apply vs ãƒ™ã‚¯ãƒˆãƒ«åŒ–æ“ä½œã®æ¯”è¼ƒ
print("âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ:")

large_df = pd.DataFrame({
    'value1': np.random.random(100000),
    'value2': np.random.random(100000)
})

import time

# applyä½¿ç”¨
start = time.time()
result_apply = large_df.apply(lambda row: row['value1'] * row['value2'], axis=1)
time_apply = time.time() - start

# ãƒ™ã‚¯ãƒˆãƒ«åŒ–æ“ä½œ
start = time.time()
result_vectorized = large_df['value1'] * large_df['value2']
time_vectorized = time.time() - start

print(f"applyä½¿ç”¨: {time_apply:.4f}ç§’")
print(f"ãƒ™ã‚¯ãƒˆãƒ«åŒ–: {time_vectorized:.4f}ç§’")
print(f"é€Ÿåº¦æ”¹å–„: {time_apply/time_vectorized:.1f}å€")

# NumPyé–¢æ•°ã®æ´»ç”¨
print("\nğŸš€ NumPyé–¢æ•°æ´»ç”¨:")

def optimize_conditional(df):
    """æ¡ä»¶åˆ†å²ã®æœ€é©åŒ–"""
    # éåŠ¹ç‡ãªapply
    start = time.time()
    result_slow = df['value1'].apply(lambda x: 'High' if x > 0.5 else 'Low')
    time_slow = time.time() - start

    # åŠ¹ç‡çš„ãªnp.where
    start = time.time()
    result_fast = np.where(df['value1'] > 0.5, 'High', 'Low')
    time_fast = time.time() - start

    print(f"applyæ¡ä»¶åˆ†å²: {time_slow:.4f}ç§’")
    print(f"np.where: {time_fast:.4f}ç§’")
    print(f"é€Ÿåº¦æ”¹å–„: {time_slow/time_fast:.1f}å€")

optimize_conditional(large_df.head(10000))

print("\n5. å®Ÿå‹™ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ä¾‹")
print("=" * 40)

# ECã‚µã‚¤ãƒˆã®å®Ÿå‹™çš„ãªç‰¹å¾´é‡ä½œæˆ
ecommerce_data = pd.DataFrame({
    'user_id': range(1, 5001),
    'signup_date': pd.date_range('2023-01-01', periods=5000, freq='D'),
    'last_purchase_date': pd.date_range('2024-01-01', periods=5000, freq='H'),
    'total_orders': np.random.poisson(10, 5000),
    'total_amount': np.random.exponential(5000, 5000),
    'avg_order_value': np.random.normal(500, 200, 5000),
    'favorite_category': np.random.choice(['Electronics', 'Fashion', 'Books', 'Home'], 5000),
    'payment_method': np.random.choice(['Credit', 'Debit', 'Digital'], 5000),
    'location_tier': np.random.choice(['Tier1', 'Tier2', 'Tier3'], 5000)
})

print("ğŸ›’ ECã‚µã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿:")
print(ecommerce_data.head())

print("\nğŸ¯ å®Ÿå‹™ç‰¹å¾´é‡ã®ä½œæˆ:")

# 1. RFMåˆ†æã®åŸºç¤ç‰¹å¾´é‡
def calculate_rfm_features(row):
    """RFMåˆ†æç”¨ç‰¹å¾´é‡"""
    current_date = pd.Timestamp('2024-01-01')

    # Recencyï¼ˆæœ€çµ‚è³¼å…¥ã‹ã‚‰ã®æ—¥æ•°ï¼‰
    recency = (current_date - row['last_purchase_date']).days

    # Frequencyï¼ˆæ³¨æ–‡é »åº¦ï¼‰
    frequency = row['total_orders']

    # Monetaryï¼ˆç·è³¼å…¥é‡‘é¡ï¼‰
    monetary = row['total_amount']

    return pd.Series({
        'recency': recency,
        'frequency': frequency,
        'monetary': monetary,
        'rfm_score': (1/max(recency, 1)) * frequency * (monetary/1000)
    })

rfm_features = ecommerce_data.apply(calculate_rfm_features, axis=1)
ecommerce_data = pd.concat([ecommerce_data, rfm_features], axis=1)

# 2. é¡§å®¢ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ç‰¹å¾´é‡
def customer_lifecycle(row):
    """é¡§å®¢ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«æ®µéš"""
    days_since_signup = (pd.Timestamp('2024-01-01') - row['signup_date']).days
    orders_per_day = row['total_orders'] / max(days_since_signup, 1)

    if days_since_signup < 30:
        if row['total_orders'] > 2:
            return 'New_Active'
        else:
            return 'New_Inactive'
    elif days_since_signup < 365:
        if orders_per_day > 0.1:
            return 'Growing'
        else:
            return 'Declining'
    else:
        if orders_per_day > 0.05:
            return 'Loyal'
        else:
            return 'AtRisk'

ecommerce_data['lifecycle_stage'] = ecommerce_data.apply(customer_lifecycle, axis=1)

# 3. è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹å¾´é‡
ecommerce_data['orders_per_month'] = ecommerce_data.apply(
    lambda row: row['total_orders'] / max((pd.Timestamp('2024-01-01') - row['signup_date']).days / 30, 1), axis=1
)

ecommerce_data['is_high_value'] = ecommerce_data.apply(
    lambda row: 1 if row['avg_order_value'] > row['avg_order_value'].quantile(0.8) else 0, axis=1
)

# 4. ã‚«ãƒ†ã‚´ãƒªåˆ¥ç‰¹å¾´é‡ï¼ˆãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼‰
category_dummies = pd.get_dummies(ecommerce_data['favorite_category'], prefix='cat')
payment_dummies = pd.get_dummies(ecommerce_data['payment_method'], prefix='pay')
tier_dummies = pd.get_dummies(ecommerce_data['location_tier'], prefix='tier')

ecommerce_data = pd.concat([ecommerce_data, category_dummies, payment_dummies, tier_dummies], axis=1)

print("ğŸ“Š ä½œæˆã•ã‚ŒãŸç‰¹å¾´é‡:")
feature_columns = ['recency', 'frequency', 'monetary', 'rfm_score', 'lifecycle_stage',
                  'orders_per_month', 'is_high_value']
print(ecommerce_data[feature_columns].head())

print("\nğŸ“ˆ ç‰¹å¾´é‡ã®çµ±è¨ˆ:")
print(ecommerce_data[['rfm_score', 'orders_per_month']].describe())

print("\n6. é«˜åº¦ãªå¤‰æ›ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯")
print("=" * 40)

# ã‚«ã‚¹ã‚¿ãƒ å¤‰æ›å™¨ã‚¯ãƒ©ã‚¹
class FeatureTransformer:
    """ã‚«ã‚¹ã‚¿ãƒ ç‰¹å¾´é‡å¤‰æ›å™¨"""

    def __init__(self):
        self.transformations = []

    def add_transformation(self, name, func):
        """å¤‰æ›å‡¦ç†ã‚’è¿½åŠ """
        self.transformations.append((name, func))

    def fit_transform(self, df):
        """ã™ã¹ã¦ã®å¤‰æ›ã‚’é©ç”¨"""
        result_df = df.copy()

        for name, func in self.transformations:
            try:
                if hasattr(func, '__call__'):
                    if 'axis' in func.__code__.co_varnames:
                        result_df[name] = df.apply(func, axis=1)
                    else:
                        result_df[name] = df.apply(func)
                else:
                    result_df[name] = func
                print(f"âœ… {name} å¤‰æ›å®Œäº†")
            except Exception as e:
                print(f"âŒ {name} å¤‰æ›å¤±æ•—: {e}")

        return result_df

# å¤‰æ›å™¨ã®ä½¿ç”¨ä¾‹
transformer = FeatureTransformer()

# è¤‡æ•°ã®å¤‰æ›ã‚’ç™»éŒ²
transformer.add_transformation(
    'customer_tenure_months',
    lambda row: (pd.Timestamp('2024-01-01') - row['signup_date']).days / 30
)

transformer.add_transformation(
    'purchase_intensity',
    lambda row: row['total_orders'] / max((pd.Timestamp('2024-01-01') - row['signup_date']).days, 1) * 365
)

transformer.add_transformation(
    'value_segment',
    lambda row: 'Premium' if row['total_amount'] > 10000 else 'Standard' if row['total_amount'] > 2000 else 'Basic'
)

# å¤‰æ›å®Ÿè¡Œ
sample_df = ecommerce_data.head(100)
transformed_df = transformer.fit_transform(sample_df)

print("\nğŸ”„ å¤‰æ›å™¨ã«ã‚ˆã‚‹ç‰¹å¾´é‡ä½œæˆ:")
new_features = ['customer_tenure_months', 'purchase_intensity', 'value_segment']
print(transformed_df[new_features].head())

print("\nâœ… Week 4 å®Œäº†ï¼šç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’ãƒã‚¹ã‚¿ãƒ¼ã—ã¾ã—ãŸï¼")
```

### **Day 5-7: é«˜åº¦ãªç‰¹å¾´é‡ä½œæˆãƒ†ã‚¯ãƒ‹ãƒƒã‚¯**

```python
print("\n7. æ™‚ç³»åˆ—ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°")
print("=" * 40)

# æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
dates = pd.date_range('2023-01-01', '2024-12-31', freq='D')
time_series_data = pd.DataFrame({
    'date': dates,
    'sales': np.random.normal(1000, 200, len(dates)) + 500 * np.sin(2 * np.pi * np.arange(len(dates)) / 365),
    'customers': np.random.poisson(50, len(dates)),
    'marketing_spend': np.random.exponential(100, len(dates))
})

# åŸºæœ¬çš„ãªæ™‚ç³»åˆ—ç‰¹å¾´é‡
time_series_data['year'] = time_series_data['date'].dt.year
time_series_data['month'] = time_series_data['date'].dt.month
time_series_data['day_of_week'] = time_series_data['date'].dt.dayofweek
time_series_data['is_weekend'] = time_series_data['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)

print("ğŸ“… åŸºæœ¬æ™‚ç³»åˆ—ç‰¹å¾´é‡:")
print(time_series_data.head())

# ãƒ©ã‚°ç‰¹å¾´é‡ï¼ˆéå»ã®å€¤ï¼‰
for lag in [1, 7, 30]:
    time_series_data[f'sales_lag_{lag}'] = time_series_data['sales'].shift(lag)

# ç§»å‹•å¹³å‡ç‰¹å¾´é‡
for window in [7, 30]:
    time_series_data[f'sales_ma_{window}'] = time_series_data['sales'].rolling(window=window).mean()

# å¤‰åŒ–ç‡ç‰¹å¾´é‡
time_series_data['sales_pct_change'] = time_series_data['sales'].pct_change()
time_series_data['sales_diff'] = time_series_data['sales'].diff()

print("\nğŸ“Š æ™‚ç³»åˆ—ç‰¹å¾´é‡ã‚µãƒ³ãƒ—ãƒ«:")
ts_features = ['sales', 'sales_lag_1', 'sales_lag_7', 'sales_ma_7', 'sales_pct_change']
print(time_series_data[ts_features].head(10))

print("\n8. çµ±è¨ˆçš„ç‰¹å¾´é‡ã®ä½œæˆ")
print("=" * 40)

# é›†ç´„çµ±è¨ˆé‡ã®ç‰¹å¾´é‡
def create_statistical_features(df, target_col, group_col):
    """çµ±è¨ˆçš„ç‰¹å¾´é‡ã®ä½œæˆ"""
    grouped = df.groupby(group_col)[target_col]

    stats_df = grouped.agg([
        'mean', 'median', 'std', 'min', 'max', 'count'
    ]).reset_index()

    # åˆ—åã®å¤‰æ›´
    stats_df.columns = [group_col] + [f'{target_col}_{stat}' for stat in ['mean', 'median', 'std', 'min', 'max', 'count']]

    return stats_df

# æœˆåˆ¥çµ±è¨ˆç‰¹å¾´é‡
monthly_stats = create_statistical_features(time_series_data, 'sales', 'month')
print("æœˆåˆ¥å£²ä¸Šçµ±è¨ˆ:")
print(monthly_stats)

# é¡§å®¢ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥çµ±è¨ˆ
customer_stats = ecommerce_data.groupby('lifecycle_stage').agg({
    'total_amount': ['mean', 'std'],
    'total_orders': ['mean', 'median'],
    'avg_order_value': ['mean', 'std']
}).round(2)

print("\né¡§å®¢ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«åˆ¥çµ±è¨ˆ:")
print(customer_stats)

print("\n9. ç›¸äº’ä½œç”¨ç‰¹å¾´é‡")
print("=" * 40)

# æ•°å€¤å¤‰æ•°åŒå£«ã®ç›¸äº’ä½œç”¨
interaction_df = ecommerce_data.copy()

# ä¹—ç®—ã«ã‚ˆã‚‹ç›¸äº’ä½œç”¨
interaction_df['amount_frequency_interaction'] = interaction_df['total_amount'] * interaction_df['total_orders']

# æ¯”ç‡ã«ã‚ˆã‚‹ç›¸äº’ä½œç”¨
interaction_df['amount_per_order'] = interaction_df['total_amount'] / (interaction_df['total_orders'] + 1)

# æ¡ä»¶ä»˜ãç›¸äº’ä½œç”¨
def conditional_interaction(row):
    """æ¡ä»¶ä»˜ãç›¸äº’ä½œç”¨ç‰¹å¾´é‡"""
    if row['favorite_category'] == 'Electronics':
        return row['total_amount'] * 1.2  # Electronicsé¡§å®¢ã¯é«˜ä¾¡å€¤é‡ã¿
    elif row['favorite_category'] == 'Fashion':
        return row['total_orders'] * 1.5  # Fashioné¡§å®¢ã¯é »åº¦é‡ã¿
    else:
        return row['avg_order_value']

interaction_df['category_weighted_value'] = interaction_df.apply(conditional_interaction, axis=1)

print("ğŸ”„ ç›¸äº’ä½œç”¨ç‰¹å¾´é‡:")
interaction_features = ['total_amount', 'total_orders', 'amount_frequency_interaction', 'amount_per_order']
print(interaction_df[interaction_features].head())

print("\n10. æ¬¡å…ƒå‰Šæ¸›ã¨ç‰¹å¾´é‡é¸æŠ")
print("=" * 40)

# ä¸»æˆåˆ†åˆ†æã«ã‚ˆã‚‹ç‰¹å¾´é‡ä½œæˆ
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# æ•°å€¤ç‰¹å¾´é‡ã®ã¿é¸æŠ
numeric_features = ['total_orders', 'total_amount', 'avg_order_value', 'recency', 'frequency', 'monetary']
X_numeric = ecommerce_data[numeric_features].fillna(0)

# æ¨™æº–åŒ–
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_numeric)

# PCAé©ç”¨
pca = PCA(n_components=3)
X_pca = pca.fit_transform(X_scaled)

# PCAçµæœã‚’DataFrameã«è¿½åŠ 
pca_df = pd.DataFrame(X_pca, columns=['PCA1', 'PCA2', 'PCA3'])
ecommerce_data = pd.concat([ecommerce_data.reset_index(drop=True), pca_df], axis=1)

print("ğŸ“‰ PCAç‰¹å¾´é‡:")
print(f"ç¬¬1ä¸»æˆåˆ†ã®å¯„ä¸ç‡: {pca.explained_variance_ratio_[0]:.3f}")
print(f"ç¬¬2ä¸»æˆåˆ†ã®å¯„ä¸ç‡: {pca.explained_variance_ratio_[1]:.3f}")
print(f"ç¬¬3ä¸»æˆåˆ†ã®å¯„ä¸ç‡: {pca.explained_variance_ratio_[2]:.3f}")
print(f"ç´¯ç©å¯„ä¸ç‡: {pca.explained_variance_ratio_.cumsum()}")

print("\n11. ç‰¹å¾´é‡è©•ä¾¡ã¨é¸æŠ")
print("=" * 40)

# ç‰¹å¾´é‡ã®é‡è¦åº¦è©•ä¾¡
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import SelectKBest, f_regression

# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®ä½œæˆï¼ˆä¾‹ï¼šç·è³¼å…¥é‡‘é¡ã‚’äºˆæ¸¬ï¼‰
target = ecommerce_data['total_amount'].fillna(0)

# ç‰¹å¾´é‡æº–å‚™
feature_cols = ['total_orders', 'avg_order_value', 'recency', 'frequency', 'monetary',
               'orders_per_month', 'PCA1', 'PCA2', 'PCA3']
X_features = ecommerce_data[feature_cols].fillna(0)

# Random Forestã«ã‚ˆã‚‹ç‰¹å¾´é‡é‡è¦åº¦
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_features, target)

feature_importance = pd.DataFrame({
    'feature': feature_cols,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)

print("ğŸŒ³ Random Forestç‰¹å¾´é‡é‡è¦åº¦:")
print(feature_importance)

# çµ±è¨ˆçš„ç‰¹å¾´é‡é¸æŠ
selector = SelectKBest(score_func=f_regression, k=5)
X_selected = selector.fit_transform(X_features, target)
selected_features = np.array(feature_cols)[selector.get_support()]

print(f"\nğŸ“Š çµ±è¨ˆçš„é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡: {selected_features}")

print("\n12. ç‰¹å¾´é‡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆ")
print("=" * 40)

class ComprehensiveFeatureEngine:
    """åŒ…æ‹¬çš„ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚¨ãƒ³ã‚¸ãƒ³"""

    def __init__(self):
        self.feature_log = []
        self.transformers = {}

    def create_basic_features(self, df):
        """åŸºæœ¬ç‰¹å¾´é‡ã®ä½œæˆ"""
        df = df.copy()

        # æ—¥ä»˜ç‰¹å¾´é‡
        if 'signup_date' in df.columns:
            df['days_since_signup'] = (pd.Timestamp('2024-01-01') - df['signup_date']).dt.days
            df['signup_year'] = df['signup_date'].dt.year
            df['signup_month'] = df['signup_date'].dt.month
            self.feature_log.append("æ—¥ä»˜ç‰¹å¾´é‡ä½œæˆå®Œäº†")

        return df

    def create_ratio_features(self, df):
        """æ¯”ç‡ç‰¹å¾´é‡ã®ä½œæˆ"""
        df = df.copy()

        if 'total_amount' in df.columns and 'total_orders' in df.columns:
            df['amount_per_order'] = df['total_amount'] / (df['total_orders'] + 1)
            self.feature_log.append("æ¯”ç‡ç‰¹å¾´é‡ä½œæˆå®Œäº†")

        return df

    def create_categorical_features(self, df):
        """ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã®ä½œæˆ"""
        df = df.copy()

        categorical_cols = df.select_dtypes(include=['object']).columns
        for col in categorical_cols:
            if df[col].nunique() < 10:  # ã‚«ãƒ†ã‚´ãƒªæ•°ãŒå°‘ãªã„å ´åˆã®ã¿ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆ
                dummies = pd.get_dummies(df[col], prefix=col)
                df = pd.concat([df, dummies], axis=1)

        self.feature_log.append(f"ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ä½œæˆå®Œäº†: {categorical_cols.tolist()}")
        return df

    def create_statistical_features(self, df, group_cols, target_cols):
        """çµ±è¨ˆç‰¹å¾´é‡ã®ä½œæˆ"""
        df = df.copy()

        for group_col in group_cols:
            for target_col in target_cols:
                if group_col in df.columns and target_col in df.columns:
                    grouped = df.groupby(group_col)[target_col]
                    df[f'{target_col}_mean_by_{group_col}'] = grouped.transform('mean')
                    df[f'{target_col}_std_by_{group_col}'] = grouped.transform('std')

        self.feature_log.append("çµ±è¨ˆç‰¹å¾´é‡ä½œæˆå®Œäº†")
        return df

    def fit_transform(self, df, group_cols=None, target_cols=None):
        """å…¨ã¦ã®ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œ"""
        df = self.create_basic_features(df)
        df = self.create_ratio_features(df)
        df = self.create_categorical_features(df)

        if group_cols and target_cols:
            df = self.create_statistical_features(df, group_cols, target_cols)

        return df

    def get_log(self):
        """å‡¦ç†ãƒ­ã‚°ã‚’å–å¾—"""
        return "\n".join(self.feature_log)

# ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ³ä½¿ç”¨ä¾‹
engine = ComprehensiveFeatureEngine()
sample_data = ecommerce_data.head(100).copy()

enhanced_data = engine.fit_transform(
    sample_data,
    group_cols=['favorite_category', 'payment_method'],
    target_cols=['total_amount', 'total_orders']
)

print("ğŸš€ åŒ…æ‹¬çš„ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°:")
print(engine.get_log())
print(f"\nå…ƒã®ç‰¹å¾´é‡æ•°: {sample_data.shape[1]}")
print(f"æ‹¡å¼µå¾Œç‰¹å¾´é‡æ•°: {enhanced_data.shape[1]}")

print("\nâœ… Week 4 å®Œäº†ï¼šé«˜åº¦ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’ãƒã‚¹ã‚¿ãƒ¼ã—ã¾ã—ãŸï¼")
```

---

## Week 5: ãƒ‡ãƒ¼ã‚¿çµåˆãƒã‚¹ã‚¿ãƒ¼ï¼ˆconcat/merge å®Œå…¨æ”»ç•¥ï¼‰

### ğŸ¯ **ã“ã®é€±ã®ã‚´ãƒ¼ãƒ«**

- merge()ã¨ concat()ã‚’å®Œå…¨ç†è§£ã™ã‚‹
- è¤‡é›‘ãªçµåˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è‡ªåœ¨ã«æ“ã‚‹
- å®Ÿå‹™ã§ã‚ˆãã‚ã‚‹çµåˆã‚¨ãƒ©ãƒ¼ã‚’å›é¿ã™ã‚‹
- å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®åŠ¹ç‡çš„ãªçµåˆã‚’ãƒã‚¹ã‚¿ãƒ¼ã™ã‚‹

---

### **Day 1-3: merge()ã®å®Œå…¨æ”»ç•¥**

```python
print("ğŸ”— Week 5: ãƒ‡ãƒ¼ã‚¿çµåˆãƒã‚¹ã‚¿ãƒ¼ï¼ˆconcat/mergeå®Œå…¨æ”»ç•¥ï¼‰")
print("=" * 50)

import pandas as pd
import numpy as np
import time
from functools import reduce

print("1. merge()ã®åŸºæœ¬æ¦‚å¿µã¨å†…éƒ¨å‹•ä½œ")
print("=" * 40)

# å®Ÿå‹™çš„ãªã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆ
# é¡§å®¢ãƒã‚¹ã‚¿
customers = pd.DataFrame({
    'customer_id': ['C001', 'C002', 'C003', 'C004', 'C005'],
    'customer_name': ['ç”°ä¸­å•†äº‹', 'ä½è—¤å»ºè¨­', 'éˆ´æœ¨è£½ä½œæ‰€', 'é«˜æ©‹é›»æ©Ÿ', 'å±±ç”°é‹è¼¸'],
    'industry': ['å•†ç¤¾', 'å»ºè¨­', 'è£½é€ ', 'é›»æ©Ÿ', 'é‹è¼¸'],
    'credit_score': [750, 680, 820, 720, 690],
    'registration_date': pd.to_datetime(['2020-01-01', '2020-03-15', '2019-06-20', '2021-02-10', '2020-11-05'])
})

# å£²ä¸Šãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³
transactions = pd.DataFrame({
    'transaction_id': ['T001', 'T002', 'T003', 'T004', 'T005', 'T006', 'T007'],
    'customer_id': ['C001', 'C002', 'C001', 'C003', 'C006', 'C002', 'C004'],  # C006ã¯å­˜åœ¨ã—ãªã„é¡§å®¢
    'amount': [100000, 250000, 150000, 300000, 50000, 200000, 180000],
    'transaction_date': pd.to_datetime(['2024-01-15', '2024-01-20', '2024-02-01', '2024-02-05', '2024-02-10', '2024-02-15', '2024-02-20']),
    'product_category': ['A', 'B', 'A', 'C', 'A', 'B', 'C']
})

# å•†å“ãƒã‚¹ã‚¿
products = pd.DataFrame({
    'product_category': ['A', 'B', 'C', 'D'],  # Dã¯å–å¼•ãŒãªã„ã‚«ãƒ†ã‚´ãƒª
    'category_name': ['é›»å­éƒ¨å“', 'æ©Ÿæ¢°éƒ¨å“', 'ææ–™', 'å·¥å…·'],
    'unit_price': [1000, 5000, 2000, 3000],
    'supplier': ['ã‚µãƒ—ãƒ©ã‚¤ãƒ¤ãƒ¼1', 'ã‚µãƒ—ãƒ©ã‚¤ãƒ¤ãƒ¼2', 'ã‚µãƒ—ãƒ©ã‚¤ãƒ¤ãƒ¼3', 'ã‚µãƒ—ãƒ©ã‚¤ãƒ¤ãƒ¼4']
})

print("ğŸ“Š åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ:")
print("é¡§å®¢ãƒã‚¹ã‚¿:")
print(customers)
print("\nå£²ä¸Šãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³:")
print(transactions)
print("\nå•†å“ãƒã‚¹ã‚¿:")
print(products)

print("\nğŸ” merge()ã®åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³:")

# 1. å†…éƒ¨çµåˆï¼ˆinner joinï¼‰
print("1. å†…éƒ¨çµåˆï¼ˆINNER JOINï¼‰:")
inner_result = pd.merge(transactions, customers, on='customer_id', how='inner')
print(f"ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³{len(transactions)}ä»¶ Ã— é¡§å®¢{len(customers)}ä»¶ â†’ çµæœ{len(inner_result)}ä»¶")
print(inner_result[['transaction_id', 'customer_name', 'amount']])

# 2. å·¦çµåˆï¼ˆleft joinï¼‰
print("\n2. å·¦çµåˆï¼ˆLEFT JOINï¼‰:")
left_result = pd.merge(transactions, customers, on='customer_id', how='left')
print(f"çµæœ{len(left_result)}ä»¶ï¼ˆãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ä»¶æ•°ã‚’ç¶­æŒï¼‰")
print("é¡§å®¢ãƒã‚¹ã‚¿ã«ãªã„å–å¼•:")
print(left_result[left_result['customer_name'].isnull()][['transaction_id', 'customer_id', 'amount']])

# 3. å³çµåˆï¼ˆright joinï¼‰
print("\n3. å³çµåˆï¼ˆRIGHT JOINï¼‰:")
right_result = pd.merge(transactions, customers, on='customer_id', how='right')
print(f"çµæœ{len(right_result)}ä»¶ï¼ˆé¡§å®¢æ•°ã‚’ç¶­æŒï¼‰")
print("å–å¼•ãŒãªã„é¡§å®¢:")
print(right_result[right_result['transaction_id'].isnull()][['customer_id', 'customer_name']])

# 4. å¤–éƒ¨çµåˆï¼ˆouter joinï¼‰
print("\n4. å¤–éƒ¨çµåˆï¼ˆOUTER JOINï¼‰:")
outer_result = pd.merge(transactions, customers, on='customer_id', how='outer')
print(f"çµæœ{len(outer_result)}ä»¶ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ã‚’ç¶­æŒï¼‰")
print("ãƒ‡ãƒ¼ã‚¿ã®å®Œå…¨æ€§:")
print(f"- æœ‰åŠ¹ãªå–å¼•: {outer_result['transaction_id'].notna().sum()}ä»¶")
print(f"- æœ‰åŠ¹ãªé¡§å®¢: {outer_result['customer_name'].notna().sum()}ä»¶")

print("\n2. è¤‡é›‘ãªçµåˆæ¡ä»¶")
print("=" * 40)

# è¤‡æ•°åˆ—ã«ã‚ˆã‚‹çµåˆ
detailed_transactions = pd.DataFrame({
    'customer_id': ['C001', 'C001', 'C002', 'C002'],
    'product_category': ['A', 'B', 'A', 'C'],
    'quantity': [10, 5, 20, 15],
    'transaction_date': pd.to_datetime(['2024-01-15', '2024-01-16', '2024-01-20', '2024-01-25'])
})

print("ğŸ“‹ è¤‡æ•°åˆ—çµåˆã®ä¾‹:")
print("è©³ç´°ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³:")
print(detailed_transactions)

# è¤‡æ•°åˆ—ã§ã®çµåˆ
multi_column_merge = pd.merge(
    detailed_transactions,
    products,
    on='product_category',
    how='left'
)
print("\nå•†å“ãƒã‚¹ã‚¿ã¨ã®çµåˆ:")
print(multi_column_merge)

# ç•°ãªã‚‹åˆ—åã§ã®çµåˆ
customer_info = pd.DataFrame({
    'cust_id': ['C001', 'C002', 'C003'],  # åˆ—åãŒç•°ãªã‚‹
    'region': ['é–¢æ±', 'é–¢è¥¿', 'ä¸­éƒ¨'],
    'sales_rep': ['å–¶æ¥­A', 'å–¶æ¥­B', 'å–¶æ¥­C']
})

different_name_merge = pd.merge(
    transactions,
    customer_info,
    left_on='customer_id',
    right_on='cust_id',
    how='left'
)
print("\nç•°ãªã‚‹åˆ—åã§ã®çµåˆ:")
print(different_name_merge[['customer_id', 'cust_id', 'region', 'sales_rep']].head())

print("\n3. çµåˆã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–")
print("=" * 40)

# å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
print("âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ:")

# å¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
np.random.seed(42)
large_left = pd.DataFrame({
    'key': np.random.choice(range(10000), 100000),
    'value1': np.random.random(100000)
})

large_right = pd.DataFrame({
    'key': range(10000),
    'value2': np.random.random(10000)
})

print(f"å·¦ãƒ†ãƒ¼ãƒ–ãƒ«: {len(large_left):,}è¡Œ")
print(f"å³ãƒ†ãƒ¼ãƒ–ãƒ«: {len(large_right):,}è¡Œ")

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š
start_time = time.time()
result_normal = pd.merge(large_left, large_right, on='key', how='left')
normal_time = time.time() - start_time

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨­å®šã«ã‚ˆã‚‹é«˜é€ŸåŒ–
large_right_indexed = large_right.set_index('key')
start_time = time.time()
result_indexed = large_left.set_index('key').join(large_right_indexed, how='left')
indexed_time = time.time() - start_time

print(f"é€šå¸¸ã®merge: {normal_time:.3f}ç§’")
print(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åˆ©ç”¨: {indexed_time:.3f}ç§’")
print(f"é€Ÿåº¦æ”¹å–„: {normal_time/indexed_time:.1f}å€")

# ã‚½ãƒ¼ãƒˆæ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã§ã®æœ€é©åŒ–
large_left_sorted = large_left.sort_values('key')
large_right_sorted = large_right.sort_values('key')

start_time = time.time()
result_sorted = pd.merge(large_left_sorted, large_right_sorted, on='key', how='left')
sorted_time = time.time() - start_time

print(f"ã‚½ãƒ¼ãƒˆæ¸ˆã¿merge: {sorted_time:.3f}ç§’")

print("\n4. çµåˆæ™‚ã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°")
print("=" * 40)

print("ğŸš¨ ã‚ˆãã‚ã‚‹çµåˆã‚¨ãƒ©ãƒ¼ã¨å¯¾å‡¦æ³•:")

# 1. ã‚­ãƒ¼åˆ—ã®ãƒ‡ãƒ¼ã‚¿å‹ä¸ä¸€è‡´
problematic_df1 = pd.DataFrame({
    'id': [1, 2, 3],  # æ•°å€¤å‹
    'name': ['A', 'B', 'C']
})

problematic_df2 = pd.DataFrame({
    'id': ['1', '2', '4'],  # æ–‡å­—åˆ—å‹
    'value': [100, 200, 300]
})

print("1. ãƒ‡ãƒ¼ã‚¿å‹ä¸ä¸€è‡´ã®å•é¡Œ:")
print(f"df1ã®idå‹: {problematic_df1['id'].dtype}")
print(f"df2ã®idå‹: {problematic_df2['id'].dtype}")

# ä¿®æ­£æ–¹æ³•
problematic_df2['id'] = problematic_df2['id'].astype(int)
fixed_merge = pd.merge(problematic_df1, problematic_df2, on='id', how='left')
print("ä¿®æ­£å¾Œã®çµåˆ:")
print(fixed_merge)

# 2. é‡è¤‡ã‚­ãƒ¼ã®å•é¡Œ
duplicate_customers = pd.DataFrame({
    'customer_id': ['C001', 'C001', 'C002'],  # C001ãŒé‡è¤‡
    'customer_name': ['ç”°ä¸­å•†äº‹', 'ç”°ä¸­å•†äº‹(æ–°)', 'ä½è—¤å»ºè¨­'],
    'status': ['active', 'inactive', 'active']
})

print("\n2. é‡è¤‡ã‚­ãƒ¼ã®å•é¡Œ:")
duplicate_merge = pd.merge(transactions.head(3), duplicate_customers, on='customer_id', how='left')
print(f"å…ƒãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³: {len(transactions.head(3))}ä»¶")
print(f"çµåˆå¾Œ: {len(duplicate_merge)}ä»¶ï¼ˆé‡è¤‡ã«ã‚ˆã‚Šå¢—åŠ ï¼‰")

# é‡è¤‡é™¤å»ã®æ–¹æ³•
cleaned_customers = duplicate_customers.drop_duplicates(subset=['customer_id'], keep='first')
clean_merge = pd.merge(transactions.head(3), cleaned_customers, on='customer_id', how='left')
print(f"é‡è¤‡é™¤å»å¾Œ: {len(clean_merge)}ä»¶")

print("\n5. é«˜åº¦ãªçµåˆãƒ†ã‚¯ãƒ‹ãƒƒã‚¯")
print("=" * 40)

# æ¡ä»¶ä»˜ãçµåˆï¼ˆç¯„å›²çµåˆï¼‰
print("ğŸ“… ç¯„å›²çµåˆã®ä¾‹:")

# ä¾¡æ ¼å¸¯ãƒã‚¹ã‚¿
price_tiers = pd.DataFrame({
    'tier_name': ['ä½ä¾¡æ ¼', 'ä¸­ä¾¡æ ¼', 'é«˜ä¾¡æ ¼'],
    'min_amount': [0, 100000, 200000],
    'max_amount': [99999, 199999, float('inf'))]
})

print("ä¾¡æ ¼å¸¯ãƒã‚¹ã‚¿:")
print(price_tiers)

# æ¡ä»¶ä»˜ãçµåˆã‚’å®Ÿç¾ã™ã‚‹é–¢æ•°
def conditional_merge(transactions_df, price_tiers_df):
    """æ¡ä»¶ä»˜ãçµåˆï¼šå–å¼•é‡‘é¡ã«åŸºã¥ãä¾¡æ ¼å¸¯åˆ†é¡"""
    result_list = []

    for _, trans in transactions_df.iterrows():
        amount = trans['amount']
        matched_tier = price_tiers_df[
            (price_tiers_df['min_amount'] <= amount) &
            (price_tiers_df['max_amount'] >= amount)
        ]

        if not matched_tier.empty:
            trans_with_tier = trans.to_dict()
            trans_with_tier.update(matched_tier.iloc[0].to_dict())
            result_list.append(trans_with_tier)

    return pd.DataFrame(result_list)

conditional_result = conditional_merge(transactions, price_tiers)
print("\næ¡ä»¶ä»˜ãçµåˆçµæœ:")
print(conditional_result[['transaction_id', 'amount', 'tier_name']])

# ã‚ˆã‚ŠåŠ¹ç‡çš„ãªç¯„å›²çµåˆï¼ˆpd.cutä½¿ç”¨ï¼‰
transactions['price_tier'] = pd.cut(
    transactions['amount'],
    bins=[0, 100000, 200000, float('inf')],
    labels=['ä½ä¾¡æ ¼', 'ä¸­ä¾¡æ ¼', 'é«˜ä¾¡æ ¼'],
    include_lowest=True
)
print("\npd.cutã‚’ä½¿ç”¨ã—ãŸä¾¡æ ¼å¸¯åˆ†é¡:")
print(transactions[['transaction_id', 'amount', 'price_tier']])

print("\n6. è¤‡æ•°ãƒ†ãƒ¼ãƒ–ãƒ«ã®é€£é–çµåˆ")
print("=" * 40)

# å–¶æ¥­æ‹…å½“ãƒã‚¹ã‚¿
sales_reps = pd.DataFrame({
    'rep_id': ['R001', 'R002', 'R003'],
    'rep_name': ['ç”°ä¸­å–¶æ¥­', 'ä½è—¤å–¶æ¥­', 'éˆ´æœ¨å–¶æ¥­'],
    'region': ['é–¢æ±', 'é–¢è¥¿', 'ä¸­éƒ¨']
})

# é¡§å®¢-å–¶æ¥­æ‹…å½“ãƒãƒƒãƒ”ãƒ³ã‚°
customer_rep_mapping = pd.DataFrame({
    'customer_id': ['C001', 'C002', 'C003', 'C004', 'C005'],
    'rep_id': ['R001', 'R002', 'R001', 'R003', 'R002']
})

print("ğŸ“Š å¤šæ®µéšçµåˆ:")
print("å–¶æ¥­æ‹…å½“ãƒã‚¹ã‚¿:")
print(sales_reps)
print("\né¡§å®¢-å–¶æ¥­æ‹…å½“ãƒãƒƒãƒ”ãƒ³ã‚°:")
print(customer_rep_mapping)

# æ–¹æ³•1: æ®µéšçš„çµåˆ
step1 = pd.merge(transactions, customers, on='customer_id', how='left')
step2 = pd.merge(step1, customer_rep_mapping, on='customer_id', how='left')
step3 = pd.merge(step2, sales_reps, on='rep_id', how='left')

print("\næ®µéšçš„çµåˆçµæœ:")
print(step3[['transaction_id', 'customer_name', 'rep_name', 'amount']])

# æ–¹æ³•2: é–¢æ•°å‹çµåˆï¼ˆreduceä½¿ç”¨ï¼‰
def chain_merge(dfs_and_keys):
    """è¤‡æ•°DataFrameã®é€£é–çµåˆ"""
    return reduce(
        lambda left, right_info: pd.merge(left, right_info[0], on=right_info[1], how='left'),
        dfs_and_keys[1:],
        dfs_and_keys[0][0]
    )

merge_chain = [
    (transactions, 'customer_id'),
    (customers, 'customer_id'),
    (customer_rep_mapping, 'customer_id'),
    (sales_reps, 'rep_id')
]

chained_result = chain_merge(merge_chain)
print("\né–¢æ•°å‹é€£é–çµåˆçµæœ:")
print(chained_result[['transaction_id', 'customer_name', 'rep_name', 'amount']])

print("\nâœ… Week 5 Day 1-3 å®Œäº†ï¼šmerge()ã‚’å®Œå…¨ãƒã‚¹ã‚¿ãƒ¼ã—ã¾ã—ãŸï¼")
```

### **Day 4-7: concat()å®Œå…¨æ”»ç•¥ã¨é«˜åº¦ãªçµåˆãƒ‘ã‚¿ãƒ¼ãƒ³**

```python
print("\n7. concat()ã®å®Œå…¨ç†è§£")
print("=" * 40)

print("ğŸ“š concat()ã®åŸºæœ¬æ¦‚å¿µ:")

# åŸºæœ¬çš„ãªconcatæ“ä½œ
df1 = pd.DataFrame({
    'A': [1, 2, 3],
    'B': [4, 5, 6],
    'C': [7, 8, 9]
})

df2 = pd.DataFrame({
    'A': [10, 11, 12],
    'B': [13, 14, 15],
    'C': [16, 17, 18]
})

df3 = pd.DataFrame({
    'D': [19, 20, 21],
    'E': [22, 23, 24]
})

print("åŸºæœ¬DataFrame:")
print("df1:")
print(df1)
print("\ndf2:")
print(df2)
print("\ndf3:")
print(df3)

# ç¸¦æ–¹å‘ã®é€£çµ
vertical_concat = pd.concat([df1, df2], axis=0, ignore_index=True)
print("\nğŸ“ˆ ç¸¦æ–¹å‘é€£çµï¼ˆaxis=0ï¼‰:")
print(vertical_concat)

# æ¨ªæ–¹å‘ã®é€£çµ
horizontal_concat = pd.concat([df1, df3], axis=1)
print("\nğŸ“Š æ¨ªæ–¹å‘é€£çµï¼ˆaxis=1ï¼‰:")
print(horizontal_concat)

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿æŒã—ãŸé€£çµ
indexed_concat = pd.concat([df1, df2], axis=0, keys=['ãƒ‡ãƒ¼ã‚¿1', 'ãƒ‡ãƒ¼ã‚¿2'])
print("\nğŸ·ï¸ éšå±¤ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä»˜ãé€£çµ:")
print(indexed_concat)

print("\n8. å®Ÿå‹™çš„ãªconcatæ´»ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³")
print("=" * 40)

# æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®çµåˆ
print("ğŸ“… æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®çµåˆ:")

# æœˆåˆ¥å£²ä¸Šãƒ‡ãƒ¼ã‚¿
jan_sales = pd.DataFrame({
    'date': pd.date_range('2024-01-01', '2024-01-31', freq='D'),
    'sales': np.random.normal(1000, 100, 31),
    'month': '2024-01'
})

feb_sales = pd.DataFrame({
    'date': pd.date_range('2024-02-01', '2024-02-29', freq='D'),
    'sales': np.random.normal(1100, 120, 29),
    'month': '2024-02'
})

mar_sales = pd.DataFrame({
    'date': pd.date_range('2024-03-01', '2024-03-31', freq='D'),
    'sales': np.random.normal(1200, 150, 31),
    'month': '2024-03'
})

# å››åŠæœŸãƒ‡ãƒ¼ã‚¿ã®çµåˆ
q1_sales = pd.concat([jan_sales, feb_sales, mar_sales], ignore_index=True)
print(f"Q1å£²ä¸Šãƒ‡ãƒ¼ã‚¿: {len(q1_sales)}æ—¥åˆ†")
print("æœˆåˆ¥å£²ä¸Šã‚µãƒãƒªãƒ¼:")
print(q1_sales.groupby('month')['sales'].agg(['sum', 'mean']).round(2))

# è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã¨çµåˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
print("\nğŸ“ è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«çµåˆãƒ‘ã‚¿ãƒ¼ãƒ³:")

def simulate_file_concat():
    """è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã¨çµåˆã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"""
    file_data = []

    for i in range(1, 6):  # 5ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        df = pd.DataFrame({
            'customer_id': [f'C{j:03d}' for j in range(i*100, (i+1)*100)],
            'value': np.random.normal(1000, 200, 100),
            'source_file': f'file_{i}.csv'
        })
        file_data.append(df)

    # å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµåˆ
    combined_data = pd.concat(file_data, ignore_index=True)
    return combined_data

combined_files = simulate_file_concat()
print(f"çµåˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿: {len(combined_files)}è¡Œ")
print("ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ãƒ‡ãƒ¼ã‚¿æ•°:")
print(combined_files['source_file'].value_counts())

print("\n9. ç•°ãªã‚‹æ§‹é€ ã®DataFrameçµåˆ")
print("=" * 40)

# åˆ—ãŒç•°ãªã‚‹DataFrameã®çµåˆ
print("ğŸ”€ åˆ—æ§‹é€ ãŒç•°ãªã‚‹DataFrame:")

sales_2023 = pd.DataFrame({
    'date': pd.date_range('2023-01-01', periods=5, freq='D'),
    'product_a': [100, 110, 105, 120, 115],
    'product_b': [200, 210, 205, 220, 215]
})

sales_2024 = pd.DataFrame({
    'date': pd.date_range('2024-01-01', periods=5, freq='D'),
    'product_a': [130, 140, 135, 150, 145],
    'product_c': [300, 310, 305, 320, 315]  # product_bãŒãªãã€product_cãŒæ–°è¦
})

print("2023å¹´ãƒ‡ãƒ¼ã‚¿:")
print(sales_2023)
print("\n2024å¹´ãƒ‡ãƒ¼ã‚¿:")
print(sales_2024)

# å¤–éƒ¨çµåˆã§å…¨åˆ—ã‚’ä¿æŒ
mixed_columns_concat = pd.concat([sales_2023, sales_2024], sort=False)
print("\næ··åˆåˆ—çµåˆçµæœ:")
print(mixed_columns_concat)

# å…±é€šåˆ—ã®ã¿çµåˆ
common_columns = list(set(sales_2023.columns) & set(sales_2024.columns))
print(f"\nå…±é€šåˆ—: {common_columns}")
common_only_concat = pd.concat([
    sales_2023[common_columns],
    sales_2024[common_columns]
])
print("å…±é€šåˆ—ã®ã¿çµåˆ:")
print(common_only_concat)

print("\n10. å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®åŠ¹ç‡çš„çµåˆ")
print("=" * 40)

print("âš¡ å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿çµåˆã®æœ€é©åŒ–:")

# ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªçµåˆæ–¹æ³•
def efficient_large_concat(dataframes, chunk_size=1000):
    """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿çµåˆ"""

    # ãƒãƒ£ãƒ³ã‚¯å˜ä½ã§å‡¦ç†
    result_chunks = []
    current_chunk = []
    current_size = 0

    for df in dataframes:
        current_chunk.append(df)
        current_size += len(df)

        if current_size >= chunk_size:
            # ãƒãƒ£ãƒ³ã‚¯ã‚’çµåˆ
            chunk_result = pd.concat(current_chunk, ignore_index=True)
            result_chunks.append(chunk_result)

            # ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒªã‚»ãƒƒãƒˆ
            current_chunk = []
            current_size = 0

    # æ®‹ã‚Šã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†
    if current_chunk:
        chunk_result = pd.concat(current_chunk, ignore_index=True)
        result_chunks.append(chunk_result)

    # æœ€çµ‚çµåˆ
    return pd.concat(result_chunks, ignore_index=True)

# ãƒ†ã‚¹ãƒˆç”¨ã®å¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
large_dataframes = []
for i in range(10):
    df = pd.DataFrame({
        'id': range(i*1000, (i+1)*1000),
        'value': np.random.random(1000),
        'category': np.random.choice(['A', 'B', 'C'], 1000)
    })
    large_dataframes.append(df)

# åŠ¹ç‡çš„çµåˆã®ãƒ†ã‚¹ãƒˆ
start_time = time.time()
efficient_result = efficient_large_concat(large_dataframes, chunk_size=2000)
efficient_time = time.time() - start_time

# é€šå¸¸çµåˆã¨ã®æ¯”è¼ƒ
start_time = time.time()
normal_result = pd.concat(large_dataframes, ignore_index=True)
normal_time = time.time() - start_time

print(f"åŠ¹ç‡çš„çµåˆ: {efficient_time:.3f}ç§’")
print(f"é€šå¸¸çµåˆ: {normal_time:.3f}ç§’")
print(f"çµæœãƒ‡ãƒ¼ã‚¿: {len(efficient_result)}è¡Œ")

print("\n11. ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ä»˜ãçµåˆ")
print("=" * 40)

class QualityCheckedMerge:
    """ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½ä»˜ããƒãƒ¼ã‚¸"""

    def __init__(self):
        self.merge_log = []
        self.quality_issues = []

    def safe_merge(self, left, right, on=None, how='inner', **kwargs):
        """å®‰å…¨ãªãƒãƒ¼ã‚¸å®Ÿè¡Œ"""

        # äº‹å‰ãƒã‚§ãƒƒã‚¯
        self._pre_merge_checks(left, right, on)

        # ãƒãƒ¼ã‚¸å®Ÿè¡Œ
        try:
            result = pd.merge(left, right, on=on, how=how, **kwargs)
            self._post_merge_checks(left, right, result, how)

            self.merge_log.append(f"âœ… ãƒãƒ¼ã‚¸æˆåŠŸ: {len(left)} Ã— {len(right)} â†’ {len(result)}")
            return result

        except Exception as e:
            self.quality_issues.append(f"âŒ ãƒãƒ¼ã‚¸ã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise

    def _pre_merge_checks(self, left, right, on):
        """ãƒãƒ¼ã‚¸å‰å“è³ªãƒã‚§ãƒƒã‚¯"""

        if on:
            # ã‚­ãƒ¼åˆ—ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
            if on not in left.columns:
                self.quality_issues.append(f"âš ï¸ å·¦ãƒ†ãƒ¼ãƒ–ãƒ«ã«ã‚­ãƒ¼åˆ—'{on}'ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            if on not in right.columns:
                self.quality_issues.append(f"âš ï¸ å³ãƒ†ãƒ¼ãƒ–ãƒ«ã«ã‚­ãƒ¼åˆ—'{on}'ãŒå­˜åœ¨ã—ã¾ã›ã‚“")

            # ãƒ‡ãƒ¼ã‚¿å‹ãƒã‚§ãƒƒã‚¯
            if on in left.columns and on in right.columns:
                if left[on].dtype != right[on].dtype:
                    self.quality_issues.append(f"âš ï¸ ã‚­ãƒ¼åˆ—'{on}'ã®ãƒ‡ãƒ¼ã‚¿å‹ãŒç•°ãªã‚Šã¾ã™: {left[on].dtype} vs {right[on].dtype}")

            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            left_duplicates = left[on].duplicated().sum()
            right_duplicates = right[on].duplicated().sum()

            if left_duplicates > 0:
                self.quality_issues.append(f"âš ï¸ å·¦ãƒ†ãƒ¼ãƒ–ãƒ«ã«ã‚­ãƒ¼é‡è¤‡: {left_duplicates}ä»¶")
            if right_duplicates > 0:
                self.quality_issues.append(f"âš ï¸ å³ãƒ†ãƒ¼ãƒ–ãƒ«ã«ã‚­ãƒ¼é‡è¤‡: {right_duplicates}ä»¶")

    def _post_merge_checks(self, left, right, result, how):
        """ãƒãƒ¼ã‚¸å¾Œå“è³ªãƒã‚§ãƒƒã‚¯"""

        # è¡Œæ•°ãƒã‚§ãƒƒã‚¯
        if how == 'inner':
            if len(result) > len(left) or len(result) > len(right):
                self.quality_issues.append("âš ï¸ å†…éƒ¨çµåˆã§è¡Œæ•°ãŒå¢—åŠ ï¼ˆé‡è¤‡ã‚­ãƒ¼ã®å¯èƒ½æ€§ï¼‰")

        elif how == 'left':
            if len(result) != len(left):
                self.quality_issues.append(f"âš ï¸ å·¦çµåˆã§è¡Œæ•°ãŒå¤‰åŒ–: {len(left)} â†’ {len(result)}")

        # nullå€¤ãƒã‚§ãƒƒã‚¯
        new_nulls = result.isnull().sum().sum() - left.isnull().sum().sum()
        if new_nulls > 0:
            self.quality_issues.append(f"âš ï¸ ãƒãƒ¼ã‚¸ã«ã‚ˆã‚Š{new_nulls}å€‹ã®nullå€¤ãŒç™ºç”Ÿ")

    def get_quality_report(self):
        """å“è³ªãƒ¬ãƒãƒ¼ãƒˆã‚’å–å¾—"""
        report = "=== ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒãƒ¼ãƒˆ ===\n"
        report += "\n".join(self.merge_log)
        if self.quality_issues:
            report += "\n\n=== å“è³ªå•é¡Œ ===\n"
            report += "\n".join(self.quality_issues)
        return report

# å“è³ªãƒã‚§ãƒƒã‚¯ä»˜ããƒãƒ¼ã‚¸ã®ä½¿ç”¨ä¾‹
quality_merger = QualityCheckedMerge()

# å•é¡Œã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆ
problematic_left = pd.DataFrame({
    'id': [1, 1, 2, 3],  # é‡è¤‡ã‚ã‚Š
    'value': ['A', 'B', 'C', 'D']
})

problematic_right = pd.DataFrame({
    'id': ['1', '2', '4'],  # æ–‡å­—åˆ—å‹
    'info': ['X', 'Y', 'Z']
})

print("ğŸ” å“è³ªãƒã‚§ãƒƒã‚¯ä»˜ããƒãƒ¼ã‚¸ãƒ†ã‚¹ãƒˆ:")
try:
    # ãƒ‡ãƒ¼ã‚¿å‹ã‚’ä¿®æ­£ã—ã¦ã‹ã‚‰ãƒãƒ¼ã‚¸
    problematic_right['id'] = problematic_right['id'].astype(int)
    safe_result = quality_merger.safe_merge(problematic_left, problematic_right, on='id', how='left')
    print("ãƒãƒ¼ã‚¸çµæœ:")
    print(safe_result)
except Exception as e:
    print(f"ãƒãƒ¼ã‚¸ã‚¨ãƒ©ãƒ¼: {e}")

print("\nå“è³ªãƒ¬ãƒãƒ¼ãƒˆ:")
print(quality_merger.get_quality_report())

print("\n12. å®Ÿå‹™çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³")
print("=" * 40)

class DataIntegrationPipeline:
    """ãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""

    def __init__(self):
        self.steps = []
        self.results = {}
        self.performance_log = {}

    def add_merge_step(self, name, left_key, right_key, join_type='left', validation=True):
        """ãƒãƒ¼ã‚¸ã‚¹ãƒ†ãƒƒãƒ—ã‚’è¿½åŠ """
        self.steps.append({
            'type': 'merge',
            'name': name,
            'left_key': left_key,
            'right_key': right_key,
            'join_type': join_type,
            'validation': validation
        })

    def add_concat_step(self, name, dataframes, axis=0):
        """çµåˆã‚¹ãƒ†ãƒƒãƒ—ã‚’è¿½åŠ """
        self.steps.append({
            'type': 'concat',
            'name': name,
            'dataframes': dataframes,
            'axis': axis
        })

    def execute_pipeline(self, initial_data):
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        current_data = initial_data.copy()

        for step in self.steps:
            start_time = time.time()

            if step['type'] == 'merge':
                current_data = self._execute_merge_step(current_data, step)
            elif step['type'] == 'concat':
                current_data = self._execute_concat_step(current_data, step)

            execution_time = time.time() - start_time
            self.performance_log[step['name']] = execution_time
            self.results[step['name']] = current_data.copy()

            print(f"âœ… {step['name']} å®Œäº†: {execution_time:.3f}ç§’, {len(current_data)}è¡Œ")

        return current_data

    def _execute_merge_step(self, data, step):
        """ãƒãƒ¼ã‚¸ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œ"""
        # å®Ÿè£…ã¯ç°¡ç•¥åŒ–
        return data

    def _execute_concat_step(self, data, step):
        """çµåˆã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œ"""
        return pd.concat([data] + step['dataframes'], axis=step['axis'], ignore_index=True)

    def get_performance_report(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ"""
        total_time = sum(self.performance_log.values())
        report = f"ç·å®Ÿè¡Œæ™‚é–“: {total_time:.3f}ç§’\n"
        for step, time_taken in self.performance_log.items():
            percentage = (time_taken / total_time) * 100
            report += f"- {step}: {time_taken:.3f}ç§’ ({percentage:.1f}%)\n"
        return report

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½¿ç”¨ä¾‹
pipeline = DataIntegrationPipeline()

# è¿½åŠ ãƒ‡ãƒ¼ã‚¿ã®çµåˆã‚¹ãƒ†ãƒƒãƒ—
additional_transactions = pd.DataFrame({
    'transaction_id': ['T008', 'T009'],
    'customer_id': ['C001', 'C003'],
    'amount': [75000, 125000],
    'transaction_date': pd.to_datetime(['2024-03-01', '2024-03-05']),
    'product_category': ['B', 'A']
})

pipeline.add_concat_step('è¿½åŠ å–å¼•çµåˆ', [additional_transactions], axis=0)

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
print("ğŸš€ ãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ:")
final_result = pipeline.execute_pipeline(transactions)

print(f"\næœ€çµ‚çµæœ: {len(final_result)}è¡Œ")
print("\nãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ:")
print(pipeline.get_performance_report())

print("\nâœ… Week 5 å®Œäº†ï¼šãƒ‡ãƒ¼ã‚¿çµåˆã‚’å®Œå…¨ãƒã‚¹ã‚¿ãƒ¼ã—ã¾ã—ãŸï¼")
```

---

## Week 6: é›†è¨ˆãƒ»ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ãƒ»æ™‚ç³»åˆ—åˆ†æ

### ğŸ¯ **ã“ã®é€±ã®ã‚´ãƒ¼ãƒ«**

- groupby æ“ä½œã®é«˜åº¦ãªãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã‚’ãƒã‚¹ã‚¿ãƒ¼ã™ã‚‹
- è¤‡é›‘ãªé›†è¨ˆå‡¦ç†ã‚’åŠ¹ç‡çš„ã«å®Ÿè£…ã™ã‚‹
- æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®åˆ†ææ‰‹æ³•ã‚’ç¿’å¾—ã™ã‚‹
- ãƒ“ã‚¸ãƒã‚¹æŒ‡æ¨™ã®è¨ˆç®—æ–¹æ³•ã‚’ç†è§£ã™ã‚‹

---

### **Day 1-4: é«˜åº¦ãª groupby æ“ä½œ**

```python
print("\nğŸ“Š Week 6: é›†è¨ˆãƒ»ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ãƒ»æ™‚ç³»åˆ—åˆ†æ")
print("=" * 50)

print("1. groupbyã®å†…éƒ¨å‹•ä½œã¨æœ€é©åŒ–")
print("=" * 40)

# å¤§è¦æ¨¡ãªå£²ä¸Šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ
np.random.seed(42)
sales_data = []

# 2å¹´åˆ†ã®æ—¥æ¬¡å£²ä¸Šãƒ‡ãƒ¼ã‚¿
for i in range(730):  # 2å¹´ = 730æ—¥
    date = pd.Timestamp('2023-01-01') + pd.Timedelta(days=i)

    # è¤‡æ•°åº—èˆ—ã®å£²ä¸Š
    for store in ['æ±äº¬åº—', 'å¤§é˜ªåº—', 'åå¤å±‹åº—', 'ç¦å²¡åº—', 'æœ­å¹Œåº—']:
        # è¤‡æ•°å•†å“ã‚«ãƒ†ã‚´ãƒª
        for category in ['Electronics', 'Fashion', 'Food', 'Books']:
            # å­£ç¯€æ€§ã‚’åŠ å‘³ã—ãŸå£²ä¸Š
            base_sales = 10000
            seasonal_factor = 1 + 0.3 * np.sin(2 * np.pi * i / 365)
            weekend_factor = 1.2 if date.weekday() >= 5 else 1.0

            sales = base_sales * seasonal_factor * weekend_factor * np.random.uniform(0.7, 1.3)

            sales_data.append({
                'date': date,
                'store': store,
                'category': category,
                'sales': sales,
                'units_sold': int(sales / np.random.uniform(50, 200)),
                'year': date.year,
                'month': date.month,
                'quarter': date.quarter,
                'weekday': date.day_name()
            })

sales_df = pd.DataFrame(sales_data)
print(f"ğŸ“ˆ å£²ä¸Šãƒ‡ãƒ¼ã‚¿: {len(sales_df):,}è¡Œ")
print("ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«:")
print(sales_df.head())

print("\nâš¡ groupby ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–:")

# 1. ã‚«ãƒ†ã‚´ãƒªå‹ã«ã‚ˆã‚‹æœ€é©åŒ–
start_time = time.time()
result1 = sales_df.groupby('store')['sales'].sum()
time1 = time.time() - start_time

# ã‚«ãƒ†ã‚´ãƒªå‹ã«å¤‰æ›
sales_df_opt = sales_df.copy()
sales_df_opt['store'] = sales_df_opt['store'].astype('category')
sales_df_opt['category'] = sales_df_opt['category'].astype('category')

start_time = time.time()
result2 = sales_df_opt.groupby('store')['sales'].sum()
time2 = time.time() - start_time

print(f"é€šå¸¸å‹: {time1:.4f}ç§’")
print(f"ã‚«ãƒ†ã‚´ãƒªå‹: {time2:.4f}ç§’")
print(f"é€Ÿåº¦æ”¹å–„: {time1/time2:.1f}å€")

# 2. ã‚½ãƒ¼ãƒˆæ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã§ã®æœ€é©åŒ–
sales_df_sorted = sales_df.sort_values(['store', 'category'])

start_time = time.time()
result3 = sales_df_sorted.groupby(['store', 'category'])['sales'].sum()
time3 = time.time() - start_time

print(f"ã‚½ãƒ¼ãƒˆæ¸ˆã¿è¤‡æ•°ã‚­ãƒ¼: {time3:.4f}ç§’")

print("\n2. é«˜åº¦ãªé›†è¨ˆé–¢æ•°")
print("=" * 40)

# ã‚«ã‚¹ã‚¿ãƒ é›†è¨ˆé–¢æ•°
def sales_metrics(group):
    """ã‚«ã‚¹ã‚¿ãƒ å£²ä¸ŠæŒ‡æ¨™è¨ˆç®—"""
    return pd.Series({
        'å£²ä¸Šåˆè¨ˆ': group['sales'].sum(),
        'å£²ä¸Šå¹³å‡': group['sales'].mean(),
        'å£²ä¸Šä¸­å¤®å€¤': group['sales'].median(),
        'å£²ä¸Šæ¨™æº–åå·®': group['sales'].std(),
        'æœ€å¤§å£²ä¸Š': group['sales'].max(),
        'æœ€å°å£²ä¸Š': group['sales'].min(),
        'å£²ä¸Šç¯„å›²': group['sales'].max() - group['sales'].min(),
        'å¤‰å‹•ä¿‚æ•°': group['sales'].std() / group['sales'].mean(),
        'å£²ä¸Šæˆé•·ç‡': group['sales'].pct_change().mean() * 100,
        'å–å¼•æ—¥æ•°': len(group),
        'ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ç‡': (group['sales'] > 0).sum() / len(group)
    })

# åº—èˆ—åˆ¥è©³ç´°åˆ†æ
store_analysis = sales_df.groupby('store').apply(sales_metrics)
print("ğŸª åº—èˆ—åˆ¥è©³ç´°åˆ†æ:")
print(store_analysis.round(2))

# ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«è¨ˆç®—
def percentile_analysis(group):
    """ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«åˆ†æ"""
    return pd.Series({
        'P10': group['sales'].quantile(0.1),
        'P25': group['sales'].quantile(0.25),
        'P50': group['sales'].quantile(0.5),
        'P75': group['sales'].quantile(0.75),
        'P90': group['sales'].quantile(0.9),
        'P95': group['sales'].quantile(0.95),
        'P99': group['sales'].quantile(0.99)
    })

category_percentiles = sales_df.groupby('category').apply(percentile_analysis)
print("\nğŸ“Š ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«åˆ†æ:")
print(category_percentiles.round(0))

print("\n3. è¤‡é›‘ãªé›†è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³")
print("=" * 40)

# è¤‡æ•°ãƒ¬ãƒ™ãƒ«ã®é›†è¨ˆ
print("ğŸ”„ å¤šæ®µéšé›†è¨ˆ:")

# ãƒ¬ãƒ™ãƒ«1: åº—èˆ—Ã—ã‚«ãƒ†ã‚´ãƒªåˆ¥æ—¥æ¬¡å¹³å‡
daily_avg = sales_df.groupby(['store', 'category', 'date'])['sales'].mean().reset_index()

# ãƒ¬ãƒ™ãƒ«2: åº—èˆ—Ã—ã‚«ãƒ†ã‚´ãƒªåˆ¥æœˆæ¬¡çµ±è¨ˆ
monthly_stats = daily_avg.groupby([
    daily_avg['store'],
    daily_avg['category'],
    daily_avg['date'].dt.to_period('M')
]).agg({
    'sales': ['mean', 'std', 'count']
})

monthly_stats.columns = ['æœˆå¹³å‡å£²ä¸Š', 'æœˆå£²ä¸Šæ¨™æº–åå·®', 'å–¶æ¥­æ—¥æ•°']
print("æœˆæ¬¡çµ±è¨ˆã‚µãƒ³ãƒ—ãƒ«:")
print(monthly_stats.head(10))

# æ¡ä»¶ä»˜ãé›†è¨ˆ
print("\nğŸ¯ æ¡ä»¶ä»˜ãé›†è¨ˆ:")

def conditional_aggregation(group):
    """æ¡ä»¶ã«åŸºã¥ãé›†è¨ˆ"""
    high_sales = group[group['sales'] > group['sales'].quantile(0.8)]
    low_sales = group[group['sales'] < group['sales'].quantile(0.2)]

    return pd.Series({
        'é«˜å£²ä¸Šæ—¥æ•°': len(high_sales),
        'é«˜å£²ä¸Šå¹³å‡': high_sales['sales'].mean() if len(high_sales) > 0 else 0,
        'ä½å£²ä¸Šæ—¥æ•°': len(low_sales),
        'ä½å£²ä¸Šå¹³å‡': low_sales['sales'].mean() if len(low_sales) > 0 else 0,
        'å®‰å®šåº¦': 1 - (group['sales'].std() / group['sales'].mean())
    })

store_performance = sales_df.groupby('store').apply(conditional_aggregation)
print("åº—èˆ—ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ:")
print(store_performance.round(3))

print("\n4. ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é–¢æ•°ã¨ç§»å‹•è¨ˆç®—")
print("=" * 40)

# å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã‚’æ—¥ä»˜é †ã«ã‚½ãƒ¼ãƒˆ
sales_daily = sales_df.groupby(['store', 'date'])['sales'].sum().reset_index()
sales_daily = sales_daily.sort_values(['store', 'date'])

print("ğŸ“ˆ ç§»å‹•å¹³å‡ãƒ»ç´¯ç©è¨ˆç®—:")

# åº—èˆ—åˆ¥ã®ç§»å‹•å¹³å‡
sales_daily['MA_7'] = sales_daily.groupby('store')['sales'].rolling(window=7).mean().reset_index(0, drop=True)
sales_daily['MA_30'] = sales_daily.groupby('store')['sales'].rolling(window=30).mean().reset_index(0, drop=True)

# ç´¯ç©å£²ä¸Š
sales_daily['cumulative_sales'] = sales_daily.groupby('store')['sales'].cumsum()

# å‰å¹´åŒæœˆæ¯”
sales_daily['year'] = sales_daily['date'].dt.year
sales_daily['month'] = sales_daily['date'].dt.month
sales_daily['yoy_change'] = sales_daily.groupby(['store', 'month'])['sales'].pct_change(periods=365) * 100

print("ç§»å‹•å¹³å‡ã‚µãƒ³ãƒ—ãƒ«:")
print(sales_daily[['store', 'date', 'sales', 'MA_7', 'MA_30']].head(10))

# ãƒ©ãƒ³ã‚­ãƒ³ã‚°è¨ˆç®—
sales_daily['daily_rank'] = sales_daily.groupby('date')['sales'].rank(ascending=False)
sales_daily['monthly_rank'] = sales_daily.groupby([sales_daily['date'].dt.to_period('M')])['sales'].rank(ascending=False)

print("\nğŸ† ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†æ:")
top_performers = sales_daily[sales_daily['daily_rank'] <= 2].groupby('store').size().sort_values(ascending=False)
print("ãƒˆãƒƒãƒ—2å…¥ã‚Šå›æ•°:")
print(top_performers)

print("\n5. æ™‚ç³»åˆ—ç‰¹å¾´é‡ã®ç”Ÿæˆ")
print("=" * 40)

print("ğŸ“… æ™‚ç³»åˆ—ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°:")

# ãƒ©ã‚°ç‰¹å¾´é‡
for lag in [1, 7, 30]:
    sales_daily[f'sales_lag_{lag}'] = sales_daily.groupby('store')['sales'].shift(lag)

# å¤‰åŒ–ç‡ç‰¹å¾´é‡
sales_daily['sales_pct_change_1d'] = sales_daily.groupby('store')['sales'].pct_change()
sales_daily['sales_pct_change_7d'] = sales_daily.groupby('store')['sales'].pct_change(periods=7)

# å­£ç¯€æ€§ç‰¹å¾´é‡
sales_daily['day_of_year'] = sales_daily['date'].dt.dayofyear
sales_daily['week_of_year'] = sales_daily['date'].dt.isocalendar().week
sales_daily['is_weekend'] = sales_daily['date'].dt.weekday >= 5

# å‘¨æœŸæ€§æ¤œå‡º
def seasonal_decompose_simple(group):
    """ç°¡æ˜“å­£ç¯€æ€§åˆ†è§£"""
    if len(group) < 365:  # 1å¹´æœªæº€ã®ãƒ‡ãƒ¼ã‚¿ã¯å­£ç¯€æ€§åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—
        return group

    # ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆ365æ—¥ç§»å‹•å¹³å‡ï¼‰
    group['trend'] = group['sales'].rolling(window=365, center=True).mean()

    # å­£ç¯€æ€§ï¼ˆé€±æ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    group['seasonal'] = group.groupby(group['date'].dt.dayofweek)['sales'].transform('mean')

    # æ®‹å·®
    group['residual'] = group['sales'] - group['trend'].fillna(group['sales'].mean()) - group['seasonal']

    return group

# åº—èˆ—åˆ¥å­£ç¯€æ€§åˆ†è§£
sales_with_decomp = sales_daily.groupby('store').apply(seasonal_decompose_simple).reset_index(drop=True)

print("å­£ç¯€æ€§åˆ†è§£ã‚µãƒ³ãƒ—ãƒ«:")
sample_store = sales_with_decomp[sales_with_decomp['store'] == 'æ±äº¬åº—'].head(10)
print(sample_store[['date', 'sales', 'trend', 'seasonal', 'residual']])

print("\n6. ãƒ“ã‚¸ãƒã‚¹æŒ‡æ¨™ã®è¨ˆç®—")
print("=" * 40)

print("ğŸ’¼ KPIè¨ˆç®—:")

# å£²ä¸Šæˆé•·ç‡
def calculate_growth_metrics(df):
    """æˆé•·æŒ‡æ¨™ã®è¨ˆç®—"""
    df = df.sort_values('date')

    # å‰æ—¥æ¯”
    df['sales_growth_1d'] = df['sales'].pct_change() * 100

    # å‰é€±æ¯”
    df['sales_growth_7d'] = df['sales'].pct_change(periods=7) * 100

    # å‰æœˆæ¯”ï¼ˆç´„30æ—¥ï¼‰
    df['sales_growth_30d'] = df['sales'].pct_change(periods=30) * 100

    # å¹´åˆæ¥æˆé•·ç‡
    df['ytd_growth'] = (df['sales'] / df.groupby(df['date'].dt.year)['sales'].cumsum().shift().fillna(df['sales'])) - 1

    return df

# åº—èˆ—åˆ¥æˆé•·æŒ‡æ¨™
growth_metrics = sales_daily.groupby('store').apply(calculate_growth_metrics).reset_index(drop=True)

# æˆé•·ç‡ã‚µãƒãƒªãƒ¼
growth_summary = growth_metrics.groupby('store').agg({
    'sales_growth_1d': ['mean', 'std'],
    'sales_growth_7d': ['mean', 'std'],
    'sales_growth_30d': ['mean', 'std']
}).round(2)

print("æˆé•·ç‡ã‚µãƒãƒªãƒ¼:")
print(growth_summary)

# å£²ä¸Šã®å®‰å®šæ€§æŒ‡æ¨™
stability_metrics = sales_daily.groupby('store').agg({
    'sales': [
        lambda x: x.std() / x.mean(),  # å¤‰å‹•ä¿‚æ•°
        lambda x: (x.max() - x.min()) / x.mean(),  # æ­£è¦åŒ–ç¯„å›²
        lambda x: (x > x.quantile(0.9)).sum() / len(x),  # é«˜å£²ä¸Šæ—¥ã®æ¯”ç‡
        lambda x: (x < x.quantile(0.1)).sum() / len(x)   # ä½å£²ä¸Šæ—¥ã®æ¯”ç‡
    ]
})

stability_metrics.columns = ['å¤‰å‹•ä¿‚æ•°', 'æ­£è¦åŒ–ç¯„å›²', 'é«˜å£²ä¸Šæ—¥æ¯”ç‡', 'ä½å£²ä¸Šæ—¥æ¯”ç‡']
print("\nğŸ“Š å£²ä¸Šå®‰å®šæ€§æŒ‡æ¨™:")
print(stability_metrics.round(3))

print("\nâœ… Week 6 Day 1-4 å®Œäº†ï¼šé«˜åº¦ãªé›†è¨ˆã‚’ãƒã‚¹ã‚¿ãƒ¼ã—ã¾ã—ãŸï¼")
```

### **Day 5-7: æ™‚ç³»åˆ—åˆ†æã®å®Ÿè·µ**

```python
print("\n7. é«˜åº¦ãªæ™‚ç³»åˆ—åˆ†æ")
print("=" * 40)

# è©³ç´°ãªæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ
print("ğŸ“Š æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™:")

# ã‚ˆã‚Šè¤‡é›‘ãªæ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³
def generate_complex_timeseries(start_date, periods, freq='D'):
    """è¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŒã¤æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
    dates = pd.date_range(start_date, periods=periods, freq=freq)

    data = []
    for i, date in enumerate(dates):
        # åŸºæœ¬ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆæˆé•·ï¼‰
        trend = 1000 + i * 2

        # å¹´æ¬¡å­£ç¯€æ€§
        yearly_seasonal = 200 * np.sin(2 * np.pi * i / 365)

        # é€±æ¬¡å­£ç¯€æ€§
        weekly_seasonal = 100 * np.sin(2 * np.pi * i / 7)

        # æœˆæœ«åŠ¹æœ
        month_end_effect = 150 if date.day >= 25 else 0

        # ç¥æ—¥åŠ¹æœï¼ˆç°¡ç•¥åŒ–ï¼‰
        holiday_effect = -200 if date.month == 1 and date.day == 1 else 0

        # ãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¤ã‚º
        noise = np.random.normal(0, 100)

        # å¤–éƒ¨ã‚¤ãƒ™ãƒ³ãƒˆï¼ˆä¾‹ï¼šã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ï¼‰
        campaign_effect = 500 if (i % 90 == 0 and i > 0) else 0

        value = trend + yearly_seasonal + weekly_seasonal + month_end_effect + holiday_effect + noise + campaign_effect

        data.append({
            'date': date,
            'value': max(0, value),  # è² ã®å€¤ã¯0ã«
            'trend': trend,
            'yearly_seasonal': yearly_seasonal,
            'weekly_seasonal': weekly_seasonal,
            'month_end_effect': month_end_effect,
            'campaign_effect': campaign_effect,
            'noise': noise
        })

    return pd.DataFrame(data)

# 3å¹´åˆ†ã®ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
ts_data = generate_complex_timeseries('2021-01-01', 1095)  # 3å¹´ = 1095æ—¥
ts_data.set_index('date', inplace=True)

print(f"æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿: {len(ts_data)}æ—¥åˆ†")
print("ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«:")
print(ts_data.head())

print("\nğŸ“ˆ ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ:")

# ç§»å‹•å¹³å‡ã«ã‚ˆã‚‹ãƒˆãƒ¬ãƒ³ãƒ‰æŠ½å‡º
ts_data['MA_30'] = ts_data['value'].rolling(window=30).mean()
ts_data['MA_90'] = ts_data['value'].rolling(window=90).mean()
ts_data['MA_365'] = ts_data['value'].rolling(window=365).mean()

# ãƒˆãƒ¬ãƒ³ãƒ‰ã®æ–¹å‘æ€§
ts_data['trend_direction'] = np.sign(ts_data['MA_30'].diff())

# ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦
ts_data['trend_strength'] = abs(ts_data['MA_30'].pct_change())

print("ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æçµæœ:")
trend_summary = ts_data.groupby('trend_direction')['trend_strength'].agg(['count', 'mean'])
trend_summary.index = ['ä¸‹é™', 'å¤‰åŒ–ãªã—', 'ä¸Šæ˜‡']
print(trend_summary)

print("\nğŸ”„ å­£ç¯€æ€§åˆ†æ:")

# å­£ç¯€æ€§ã®å®šé‡åŒ–
seasonal_patterns = {}

# æœˆåˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³
monthly_pattern = ts_data.groupby(ts_data.index.month)['value'].agg(['mean', 'std'])
monthly_pattern.index = [f'{i}æœˆ' for i in monthly_pattern.index]
seasonal_patterns['æœˆåˆ¥'] = monthly_pattern

# æ›œæ—¥åˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³
weekday_pattern = ts_data.groupby(ts_data.index.dayofweek)['value'].agg(['mean', 'std'])
weekday_pattern.index = ['æœˆ', 'ç«', 'æ°´', 'æœ¨', 'é‡‘', 'åœŸ', 'æ—¥']
seasonal_patterns['æ›œæ—¥åˆ¥'] = weekday_pattern

# å››åŠæœŸåˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³
quarterly_pattern = ts_data.groupby(ts_data.index.quarter)['value'].agg(['mean', 'std'])
quarterly_pattern.index = [f'Q{i}' for i in quarterly_pattern.index]
seasonal_patterns['å››åŠæœŸåˆ¥'] = quarterly_pattern

print("å­£ç¯€æ€§ãƒ‘ã‚¿ãƒ¼ãƒ³:")
for pattern_name, pattern_data in seasonal_patterns.items():
    print(f"\n{pattern_name}:")
    print(pattern_data.round(2))

# å­£ç¯€æ€§æŒ‡æ•°ã®è¨ˆç®—
def calculate_seasonal_index(series, period):
    """å­£ç¯€æ€§æŒ‡æ•°ã®è¨ˆç®—"""
    # ç§»å‹•å¹³å‡
    ma = series.rolling(window=period, center=True).mean()

    # å­£ç¯€æ€§æˆåˆ†
    seasonal_component = series / ma

    # æœŸé–“åˆ¥å¹³å‡å­£ç¯€æ€§æŒ‡æ•°
    if period == 12:  # æœˆæ¬¡
        seasonal_index = seasonal_component.groupby(seasonal_component.index.month).mean()
    elif period == 7:  # é€±æ¬¡
        seasonal_index = seasonal_component.groupby(seasonal_component.index.dayofweek).mean()
    else:
        seasonal_index = seasonal_component.groupby(seasonal_component.index.dayofyear % period).mean()

    return seasonal_index

# æœˆåˆ¥å­£ç¯€æ€§æŒ‡æ•°
monthly_seasonal_index = calculate_seasonal_index(ts_data['value'], 12)
print(f"\nğŸ“Š æœˆåˆ¥å­£ç¯€æ€§æŒ‡æ•°:")
for month, index in monthly_seasonal_index.items():
    print(f"{month}æœˆ: {index:.3f}")

print("\n8. ç•°å¸¸å€¤æ¤œå‡ºã¨å¤–ã‚Œå€¤åˆ†æ")
print("=" * 40)

# æ™‚ç³»åˆ—ç•°å¸¸å€¤æ¤œå‡º
def detect_timeseries_outliers(series, method='iqr', window=30):
    """æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®ç•°å¸¸å€¤æ¤œå‡º"""

    if method == 'iqr':
        # IQRæ³•
        Q1 = series.quantile(0.25)
        Q3 = series.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers = (series < lower_bound) | (series > upper_bound)

    elif method == 'rolling_iqr':
        # ç§»å‹•çª“IQRæ³•
        rolling_q1 = series.rolling(window=window).quantile(0.25)
        rolling_q3 = series.rolling(window=window).quantile(0.75)
        rolling_iqr = rolling_q3 - rolling_q1
        lower_bound = rolling_q1 - 1.5 * rolling_iqr
        upper_bound = rolling_q3 + 1.5 * rolling_iqr
        outliers = (series < lower_bound) | (series > upper_bound)

    elif method == 'std':
        # æ¨™æº–åå·®æ³•
        mean = series.mean()
        std = series.std()
        outliers = abs(series - mean) > 3 * std

    elif method == 'rolling_std':
        # ç§»å‹•çª“æ¨™æº–åå·®æ³•
        rolling_mean = series.rolling(window=window).mean()
        rolling_std = series.rolling(window=window).std()
        outliers = abs(series - rolling_mean) > 3 * rolling_std

    return outliers

# å„ç¨®ç•°å¸¸å€¤æ¤œå‡ºã®å®Ÿè¡Œ
outlier_methods = ['iqr', 'rolling_iqr', 'std', 'rolling_std']
outlier_results = {}

for method in outlier_methods:
    outliers = detect_timeseries_outliers(ts_data['value'], method=method)
    outlier_results[method] = outliers
    print(f"{method}æ³•: {outliers.sum()}å€‹ã®ç•°å¸¸å€¤ã‚’æ¤œå‡º")

# ç•°å¸¸å€¤ã®è©³ç´°åˆ†æ
print("\nğŸ” ç•°å¸¸å€¤ã®è©³ç´°åˆ†æ:")

# rolling_iqræ³•ã®çµæœã‚’ä½¿ç”¨
outliers = outlier_results['rolling_iqr']
outlier_data = ts_data[outliers].copy()

if len(outlier_data) > 0:
    print(f"ç•°å¸¸å€¤ç™ºç”Ÿæ—¥æ•°: {len(outlier_data)}æ—¥")
    print(f"ç•°å¸¸å€¤ã®å¹³å‡å€¤: {outlier_data['value'].mean():.0f}")
    print(f"æ­£å¸¸å€¤ã®å¹³å‡å€¤: {ts_data[~outliers]['value'].mean():.0f}")

    # ç•°å¸¸å€¤ã®æœˆåˆ¥åˆ†å¸ƒ
    outlier_monthly = outlier_data.groupby(outlier_data.index.month).size()
    print("\næœˆåˆ¥ç•°å¸¸å€¤ç™ºç”Ÿæ•°:")
    for month, count in outlier_monthly.items():
        print(f"{month}æœˆ: {count}å›")

print("\n9. äºˆæ¸¬ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®åŸºç¤")
print("=" * 40)

print("ğŸ”® ç°¡æ˜“äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«:")

# å˜ç´”ç§»å‹•å¹³å‡äºˆæ¸¬
def simple_moving_average_forecast(series, window=30, periods=7):
    """å˜ç´”ç§»å‹•å¹³å‡ã«ã‚ˆã‚‹äºˆæ¸¬"""
    last_values = series.tail(window)
    forecast = [last_values.mean()] * periods
    return forecast

# æŒ‡æ•°å¹³æ»‘åŒ–äºˆæ¸¬
def exponential_smoothing_forecast(series, alpha=0.3, periods=7):
    """æŒ‡æ•°å¹³æ»‘åŒ–ã«ã‚ˆã‚‹äºˆæ¸¬"""
    # åˆæœŸå€¤
    s = [series.iloc[0]]

    # æŒ‡æ•°å¹³æ»‘åŒ–è¨ˆç®—
    for i in range(1, len(series)):
        s.append(alpha * series.iloc[i] + (1 - alpha) * s[i-1])

    # äºˆæ¸¬
    forecast = [s[-1]] * periods
    return forecast

# ç·šå½¢ãƒˆãƒ¬ãƒ³ãƒ‰äºˆæ¸¬
def linear_trend_forecast(series, periods=7):
    """ç·šå½¢ãƒˆãƒ¬ãƒ³ãƒ‰äºˆæ¸¬"""
    from sklearn.linear_model import LinearRegression

    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    X = np.arange(len(series)).reshape(-1, 1)
    y = series.values

    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    model = LinearRegression()
    model.fit(X, y)

    # äºˆæ¸¬
    future_X = np.arange(len(series), len(series) + periods).reshape(-1, 1)
    forecast = model.predict(future_X)

    return forecast

# å„äºˆæ¸¬æ‰‹æ³•ã®å®Ÿè¡Œ
forecast_methods = {
    'ç§»å‹•å¹³å‡': simple_moving_average_forecast,
    'æŒ‡æ•°å¹³æ»‘åŒ–': exponential_smoothing_forecast,
    'ç·šå½¢ãƒˆãƒ¬ãƒ³ãƒ‰': linear_trend_forecast
}

forecast_results = {}
train_data = ts_data['value'].iloc[:-7]  # æœ€å¾Œã®7æ—¥ã‚’é™¤ã„ã¦å­¦ç¿’
actual_values = ts_data['value'].iloc[-7:].values  # å®Ÿéš›ã®å€¤

print("ğŸ“Š äºˆæ¸¬çµæœ:")
for method_name, method_func in forecast_methods.items():
    try:
        if method_name == 'æŒ‡æ•°å¹³æ»‘åŒ–':
            forecast = method_func(train_data, alpha=0.3, periods=7)
        else:
            forecast = method_func(train_data, periods=7)

        forecast_results[method_name] = forecast

        # äºˆæ¸¬ç²¾åº¦ï¼ˆMAPEï¼‰
        mape = np.mean(np.abs((actual_values - forecast) / actual_values)) * 100
        print(f"{method_name}: MAPE = {mape:.2f}%")

    except Exception as e:
        print(f"{method_name}: ã‚¨ãƒ©ãƒ¼ - {e}")

print("\n10. æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–åˆ†æ")
print("=" * 40)

# çµ±è¨ˆçš„ã‚µãƒãƒªãƒ¼
print("ğŸ“ˆ æ™‚ç³»åˆ—çµ±è¨ˆã‚µãƒãƒªãƒ¼:")

def timeseries_summary(series):
    """æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆã‚µãƒãƒªãƒ¼"""
    summary = {
        'è¦³æ¸¬æ•°': len(series),
        'å¹³å‡': series.mean(),
        'ä¸­å¤®å€¤': series.median(),
        'æ¨™æº–åå·®': series.std(),
        'æœ€å°å€¤': series.min(),
        'æœ€å¤§å€¤': series.max(),
        'ç¯„å›²': series.max() - series.min(),
        'å¤‰å‹•ä¿‚æ•°': series.std() / series.mean(),
        'æ­ªåº¦': series.skew(),
        'å°–åº¦': series.kurtosis()
    }
    return pd.Series(summary)

ts_summary = timeseries_summary(ts_data['value'])
print(ts_summary.round(3))

# è‡ªå·±ç›¸é–¢åˆ†æï¼ˆç°¡æ˜“ç‰ˆï¼‰
def calculate_autocorrelation(series, max_lags=30):
    """è‡ªå·±ç›¸é–¢ã®è¨ˆç®—"""
    autocorr = {}
    for lag in range(1, max_lags + 1):
        lagged_series = series.shift(lag)
        correlation = series.corr(lagged_series)
        autocorr[lag] = correlation
    return pd.Series(autocorr)

autocorr_result = calculate_autocorrelation(ts_data['value'])
print(f"\nğŸ”„ è‡ªå·±ç›¸é–¢åˆ†æï¼ˆä¸Šä½5ãƒ©ã‚°ï¼‰:")
top_autocorr = autocorr_result.abs().nlargest(5)
for lag, corr in top_autocorr.items():
    print(f"ãƒ©ã‚°{lag}: {corr:.3f}")

print("\n11. å®Ÿå‹™æ™‚ç³»åˆ—åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³")
print("=" * 40)

class TimeSeriesAnalyzer:
    """æ™‚ç³»åˆ—åˆ†æç·åˆã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.data = None
        self.analysis_results = {}

    def load_data(self, data, date_column=None, value_column=None):
        """ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
        if date_column and value_column:
            self.data = data.set_index(date_column)[value_column]
        else:
            self.data = data

        print(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(self.data)}è¦³æ¸¬å€¤")

    def basic_analysis(self):
        """åŸºæœ¬åˆ†æ"""
        if self.data is None:
            raise ValueError("ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")

        # åŸºæœ¬çµ±è¨ˆ
        self.analysis_results['basic_stats'] = {
            'è¦³æ¸¬æ•°': len(self.data),
            'å¹³å‡': self.data.mean(),
            'æ¨™æº–åå·®': self.data.std(),
            'æœ€å°å€¤': self.data.min(),
            'æœ€å¤§å€¤': self.data.max()
        }

        # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        ma_short = self.data.rolling(window=7).mean()
        ma_long = self.data.rolling(window=30).mean()

        trend_up = (ma_short > ma_long).sum()
        trend_down = (ma_short < ma_long).sum()

        self.analysis_results['trend'] = {
            'ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰æ—¥æ•°': trend_up,
            'ä¸‹é™ãƒˆãƒ¬ãƒ³ãƒ‰æ—¥æ•°': trend_down,
            'ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦': abs(ma_short.diff()).mean()
        }

        return self.analysis_results

    def seasonal_analysis(self):
        """å­£ç¯€æ€§åˆ†æ"""
        if self.data is None:
            raise ValueError("ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")

        # æœˆåˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³
        monthly = self.data.groupby(self.data.index.month).agg(['mean', 'std'])

        # æ›œæ—¥åˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³
        weekly = self.data.groupby(self.data.index.dayofweek).agg(['mean', 'std'])

        self.analysis_results['seasonality'] = {
            'monthly_pattern': monthly,
            'weekly_pattern': weekly,
            'monthly_volatility': monthly['std'].mean(),
            'weekly_volatility': weekly['std'].mean()
        }

        return self.analysis_results['seasonality']

    def anomaly_detection(self, method='rolling_iqr', window=30):
        """ç•°å¸¸å€¤æ¤œå‡º"""
        if self.data is None:
            raise ValueError("ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")

        outliers = detect_timeseries_outliers(self.data, method=method, window=window)

        self.analysis_results['anomalies'] = {
            'outlier_count': outliers.sum(),
            'outlier_ratio': outliers.sum() / len(self.data),
            'outlier_dates': self.data[outliers].index.tolist(),
            'outlier_values': self.data[outliers].values.tolist()
        }

        return self.analysis_results['anomalies']

    def generate_report(self):
        """ç·åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report = "=== æ™‚ç³»åˆ—åˆ†æãƒ¬ãƒãƒ¼ãƒˆ ===\n\n"

        if 'basic_stats' in self.analysis_results:
            report += "ã€åŸºæœ¬çµ±è¨ˆã€‘\n"
            for key, value in self.analysis_results['basic_stats'].items():
                report += f"- {key}: {value:.2f}\n"
            report += "\n"

        if 'trend' in self.analysis_results:
            report += "ã€ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã€‘\n"
            for key, value in self.analysis_results['trend'].items():
                report += f"- {key}: {value:.2f}\n"
            report += "\n"

        if 'anomalies' in self.analysis_results:
            report += "ã€ç•°å¸¸å€¤åˆ†æã€‘\n"
            anomalies = self.analysis_results['anomalies']
            report += f"- ç•°å¸¸å€¤æ•°: {anomalies['outlier_count']}\n"
            report += f"- ç•°å¸¸å€¤ç‡: {anomalies['outlier_ratio']:.2%}\n"
            report += "\n"

        return report

# åˆ†æå™¨ã®ä½¿ç”¨ä¾‹
analyzer = TimeSeriesAnalyzer()
analyzer.load_data(ts_data['value'])

# åˆ†æå®Ÿè¡Œ
basic_results = analyzer.basic_analysis()
seasonal_results = analyzer.seasonal_analysis()
anomaly_results = analyzer.anomaly_detection()

# ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
print("ğŸ“‹ æ™‚ç³»åˆ—åˆ†æãƒ¬ãƒãƒ¼ãƒˆ:")
print(analyzer.generate_report())

print("\nâœ… Week 6 å®Œäº†ï¼šé›†è¨ˆãƒ»æ™‚ç³»åˆ—åˆ†æã‚’ãƒã‚¹ã‚¿ãƒ¼ã—ã¾ã—ãŸï¼")
```

---

## Week 7: é«˜åº¦ãªå¯è¦–åŒ–ã¨çµ±è¨ˆåˆ†æ

### ğŸ¯ **ã“ã®é€±ã®ã‚´ãƒ¼ãƒ«**

- pandas ã¨å¯è¦–åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®é€£æºã‚’ãƒã‚¹ã‚¿ãƒ¼ã™ã‚‹
- çµ±è¨ˆçš„åˆ†ææ‰‹æ³•ã‚’ç¿’å¾—ã™ã‚‹
- ãƒ“ã‚¸ãƒã‚¹ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’æ§‹ç¯‰ã™ã‚‹
- ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãƒ†ãƒªãƒ³ã‚°ã®ã‚¹ã‚­ãƒ«ã‚’èº«ã«ã¤ã‘ã‚‹

---

### **Day 1-4: é«˜åº¦ãªå¯è¦–åŒ–ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯**

```python
print("\nğŸ“Š Week 7: é«˜åº¦ãªå¯è¦–åŒ–ã¨çµ±è¨ˆåˆ†æ")
print("=" * 50)

import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.dates import DateFormatter
import matplotlib.patches as mpatches
from scipy import stats
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = 'DejaVu Sans'
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 8)

print("1. pandasã¨å¯è¦–åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®çµ±åˆ")
print("=" * 40)

# å¯è¦–åŒ–ç”¨ã®ãƒªãƒƒãƒãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
np.random.seed(42)
visualization_data = []

for i in range(1000):
    date = pd.Timestamp('2023-01-01') + pd.Timedelta(days=np.random.randint(0, 365))
    customer_type = np.random.choice(['æ–°è¦', 'ãƒªãƒ”ãƒ¼ãƒˆ', 'VIP'], p=[0.3, 0.5, 0.2])

    # é¡§å®¢ã‚¿ã‚¤ãƒ—ã«ã‚ˆã‚‹è³¼å…¥è¡Œå‹•ã®é•ã„
    if customer_type == 'æ–°è¦':
        purchase_amount = np.random.exponential(500)
        satisfaction = np.random.normal(3.5, 0.8)
    elif customer_type == 'ãƒªãƒ”ãƒ¼ãƒˆ':
        purchase_amount = np.random.exponential(800)
        satisfaction = np.random.normal(4.0, 0.6)
    else:  # VIP
        purchase_amount = np.random.exponential(1500)
        satisfaction = np.random.normal(4.5, 0.4)

    visualization_data.append({
        'date': date,
        'customer_type': customer_type,
        'purchase_amount': purchase_amount,
        'satisfaction': max(1, min(5, satisfaction)),
        'age': np.random.randint(18, 70),
        'region': np.random.choice(['é–¢æ±', 'é–¢è¥¿', 'ä¸­éƒ¨', 'ä¹å·']),
        'product_category': np.random.choice(['Electronics', 'Fashion', 'Books', 'Sports']),
        'marketing_channel': np.random.choice(['Web', 'åº—èˆ—', 'SNS', 'ãƒ¡ãƒ¼ãƒ«'])
    })

viz_df = pd.DataFrame(visualization_data)

print("ğŸ“Š å¯è¦–åŒ–ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ:")
print(viz_df.head())
print(f"\nãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {viz_df.shape}")

print("\nğŸ¨ é«˜åº¦ãª matplotlib å¯è¦–åŒ–:")

# 1. è¤‡åˆã‚°ãƒ©ãƒ•ï¼ˆå£²ä¸Šãƒˆãƒ¬ãƒ³ãƒ‰ + æº€è¶³åº¦ï¼‰
fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 12))

# æ—¥æ¬¡å£²ä¸Šãƒˆãƒ¬ãƒ³ãƒ‰
daily_sales = viz_df.groupby('date').agg({
    'purchase_amount': 'sum',
    'satisfaction': 'mean'
}).reset_index()

ax1.plot(daily_sales['date'], daily_sales['purchase_amount'],
         color='blue', linewidth=2, alpha=0.7, label='æ—¥æ¬¡å£²ä¸Š')
ax1.fill_between(daily_sales['date'], daily_sales['purchase_amount'],
                alpha=0.3, color='blue')
ax1.set_title('æ—¥æ¬¡å£²ä¸Šæ¨ç§»', fontsize=14, fontweight='bold')
ax1.set_ylabel('å£²ä¸Šé‡‘é¡')
ax1.legend()
ax1.grid(True, alpha=0.3)

# é¡§å®¢ã‚¿ã‚¤ãƒ—åˆ¥è³¼å…¥é‡‘é¡åˆ†å¸ƒ
viz_df.boxplot(column='purchase_amount', by='customer_type', ax=ax2)
ax2.set_title('é¡§å®¢ã‚¿ã‚¤ãƒ—åˆ¥è³¼å…¥é‡‘é¡åˆ†å¸ƒ', fontsize=14, fontweight='bold')
ax2.set_xlabel('é¡§å®¢ã‚¿ã‚¤ãƒ—')
ax2.set_ylabel('è³¼å…¥é‡‘é¡')

# åœ°åŸŸåˆ¥å£²ä¸Šæ§‹æˆ
region_sales = viz_df.groupby('region')['purchase_amount'].sum()
colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']
wedges, texts, autotexts = ax3.pie(region_sales.values, labels=region_sales.index,
                                  autopct='%1.1f%%', colors=colors, startangle=90)
ax3.set_title('åœ°åŸŸåˆ¥å£²ä¸Šæ§‹æˆæ¯”', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()

print("\nğŸ”¥ seaborn ã«ã‚ˆã‚‹é«˜åº¦ãªçµ±è¨ˆå¯è¦–åŒ–:")

# 2. ç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
plt.figure(figsize=(10, 8))
numeric_cols = ['purchase_amount', 'satisfaction', 'age']
correlation_matrix = viz_df[numeric_cols].corr()

sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,
            square=True, fmt='.3f', cbar_kws={'label': 'ç›¸é–¢ä¿‚æ•°'})
plt.title('å¤‰æ•°é–“ç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()

# 3. ãƒšã‚¢ãƒ—ãƒ­ãƒƒãƒˆï¼ˆæ•£å¸ƒå›³ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ï¼‰
g = sns.pairplot(viz_df[numeric_cols + ['customer_type']],
                hue='customer_type', diag_kind='kde',
                plot_kws={'alpha': 0.6})
g.fig.suptitle('å¤‰æ•°é–“é–¢ä¿‚æ€§åˆ†æ', y=1.02, fontsize=16, fontweight='bold')
plt.show()

# 4. ãƒ•ã‚¡ã‚»ãƒƒãƒˆã‚°ãƒªãƒƒãƒ‰
g = sns.FacetGrid(viz_df, col='region', row='customer_type',
                 height=4, aspect=1.2)
g.map(plt.scatter, 'age', 'purchase_amount', alpha=0.6)
g.add_legend()
g.fig.suptitle('åœ°åŸŸÃ—é¡§å®¢ã‚¿ã‚¤ãƒ—åˆ¥ å¹´é½¢vsè³¼å…¥é‡‘é¡', y=1.02, fontsize=16)
plt.show()

print("\nâš¡ Plotly ã«ã‚ˆã‚‹ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–å¯è¦–åŒ–:")

# 5. ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–æ•£å¸ƒå›³
fig = px.scatter(viz_df, x='age', y='purchase_amount',
                color='customer_type', size='satisfaction',
                hover_data=['region', 'product_category'],
                title='å¹´é½¢ vs è³¼å…¥é‡‘é¡ï¼ˆã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ï¼‰')
fig.show()

# 6. æ™‚ç³»åˆ—ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã‚°ãƒ©ãƒ•
monthly_trend = viz_df.groupby([viz_df['date'].dt.to_period('M'), 'customer_type']).agg({
    'purchase_amount': 'sum'
}).reset_index()
monthly_trend['date'] = monthly_trend['date'].astype(str)

fig = px.line(monthly_trend, x='date', y='purchase_amount',
             color='customer_type', title='æœˆæ¬¡å£²ä¸Šæ¨ç§»ï¼ˆé¡§å®¢ã‚¿ã‚¤ãƒ—åˆ¥ï¼‰')
fig.update_layout(xaxis_title='æœˆ', yaxis_title='å£²ä¸Šé‡‘é¡')
fig.show()

print("\n2. çµ±è¨ˆçš„åˆ†æã®å®Ÿè£…")
print("=" * 40)

print("ğŸ“Š è¨˜è¿°çµ±è¨ˆå­¦:")

# åŸºæœ¬çµ±è¨ˆé‡ã®æ‹¡å¼µè¨ˆç®—
def enhanced_describe(df, column):
    """æ‹¡å¼µè¨˜è¿°çµ±è¨ˆ"""
    series = df[column]

    basic_stats = series.describe()

    # è¿½åŠ çµ±è¨ˆé‡
    additional_stats = pd.Series({
        'ç¯„å›²': series.max() - series.min(),
        'å››åˆ†ä½ç¯„å›²': series.quantile(0.75) - series.quantile(0.25),
        'å¤‰å‹•ä¿‚æ•°': series.std() / series.mean(),
        'æ­ªåº¦': stats.skew(series),
        'å°–åº¦': stats.kurtosis(series),
        '5%ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«': series.quantile(0.05),
        '95%ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«': series.quantile(0.95)
    })

    return pd.concat([basic_stats, additional_stats])

purchase_stats = enhanced_describe(viz_df, 'purchase_amount')
print("è³¼å…¥é‡‘é¡ã®æ‹¡å¼µçµ±è¨ˆ:")
print(purchase_stats.round(3))

print("\nğŸ”¬ æ¨å®šçµ±è¨ˆå­¦:")

# ä¿¡é ¼åŒºé–“ã®è¨ˆç®—
def confidence_interval(data, confidence=0.95):
    """ä¿¡é ¼åŒºé–“ã®è¨ˆç®—"""
    n = len(data)
    mean = np.mean(data)
    std_err = stats.sem(data)
    margin_error = std_err * stats.t.ppf((1 + confidence) / 2, n - 1)

    return mean - margin_error, mean + margin_error

# é¡§å®¢ã‚¿ã‚¤ãƒ—åˆ¥ã®ä¿¡é ¼åŒºé–“
print("é¡§å®¢ã‚¿ã‚¤ãƒ—åˆ¥è³¼å…¥é‡‘é¡ã®95%ä¿¡é ¼åŒºé–“:")
for customer_type in viz_df['customer_type'].unique():
    data = viz_df[viz_df['customer_type'] == customer_type]['purchase_amount']
    ci_lower, ci_upper = confidence_interval(data)
    print(f"{customer_type}: [{ci_lower:.2f}, {ci_upper:.2f}]")

print("\nğŸ§ª ä»®èª¬æ¤œå®š:")

# 1. é¡§å®¢ã‚¿ã‚¤ãƒ—é–“ã®å¹³å‡è³¼å…¥é‡‘é¡ã®å·®ã®æ¤œå®šï¼ˆANOVAï¼‰
groups = [group['purchase_amount'].values for name, group in viz_df.groupby('customer_type')]
f_stat, p_value = stats.f_oneway(*groups)

print(f"ANOVAæ¤œå®šçµæœ:")
print(f"Fçµ±è¨ˆé‡: {f_stat:.4f}")
print(f"på€¤: {p_value:.4f}")
print(f"çµæœ: {'æœ‰æ„å·®ã‚ã‚Š' if p_value < 0.05 else 'æœ‰æ„å·®ãªã—'}")

# 2. äºŒç¾¤é–“ã®å¹³å‡ã®å·®ã®æ¤œå®šï¼ˆtæ¤œå®šï¼‰
new_customers = viz_df[viz_df['customer_type'] == 'æ–°è¦']['purchase_amount']
vip_customers = viz_df[viz_df['customer_type'] == 'VIP']['purchase_amount']

t_stat, p_value = stats.ttest_ind(new_customers, vip_customers)
print(f"\næ–°è¦ vs VIPé¡§å®¢ã®tæ¤œå®š:")
print(f"tçµ±è¨ˆé‡: {t_stat:.4f}")
print(f"på€¤: {p_value:.4f}")

# 3. ç›¸é–¢ä¿‚æ•°ã®æœ‰æ„æ€§æ¤œå®š
corr_coef, p_value = stats.pearsonr(viz_df['age'], viz_df['purchase_amount'])
print(f"\nå¹´é½¢ã¨è³¼å…¥é‡‘é¡ã®ç›¸é–¢:")
print(f"ç›¸é–¢ä¿‚æ•°: {corr_coef:.4f}")
print(f"på€¤: {p_value:.4f}")

print("\nğŸ“ˆ å›å¸°åˆ†æ:")

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error

# ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
viz_df_encoded = viz_df.copy()

# ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
label_encoders = {}
categorical_cols = ['customer_type', 'region', 'product_category', 'marketing_channel']

for col in categorical_cols:
    le = LabelEncoder()
    viz_df_encoded[col + '_encoded'] = le.fit_transform(viz_df_encoded[col])
    label_encoders[col] = le

# ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®æº–å‚™
feature_cols = ['age', 'satisfaction'] + [col + '_encoded' for col in categorical_cols]
X = viz_df_encoded[feature_cols]
y = viz_df_encoded['purchase_amount']

# è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

# äºˆæ¸¬ã¨è©•ä¾¡
y_pred = lr_model.predict(X_test)
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print(f"ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«æ€§èƒ½:")
print(f"RÂ²ã‚¹ã‚³ã‚¢: {r2:.4f}")
print(f"RMSE: {rmse:.2f}")

# ç‰¹å¾´é‡é‡è¦åº¦
feature_importance = pd.DataFrame({
    'feature': feature_cols,
    'coefficient': lr_model.coef_,
    'abs_coefficient': np.abs(lr_model.coef_)
}).sort_values('abs_coefficient', ascending=False)

print(f"\nç‰¹å¾´é‡é‡è¦åº¦ï¼ˆä¸Šä½5ï¼‰:")
print(feature_importance.head())

print("\n3. ãƒ“ã‚¸ãƒã‚¹ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ§‹ç¯‰")
print("=" * 40)

class BusinessDashboard:
    """ãƒ“ã‚¸ãƒã‚¹ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¹"""

    def __init__(self, data):
        self.data = data
        self.kpis = {}
        self.charts = {}

    def calculate_kpis(self):
        """ä¸»è¦æŒ‡æ¨™ã®è¨ˆç®—"""
        self.kpis = {
            'ç·å£²ä¸Š': self.data['purchase_amount'].sum(),
            'ç·é¡§å®¢æ•°': self.data.shape[0],
            'å¹³å‡è³¼å…¥é‡‘é¡': self.data['purchase_amount'].mean(),
            'å¹³å‡æº€è¶³åº¦': self.data['satisfaction'].mean(),
            'VIPé¡§å®¢æ¯”ç‡': (self.data['customer_type'] == 'VIP').mean() * 100,
            'æ–°è¦é¡§å®¢æ¯”ç‡': (self.data['customer_type'] == 'æ–°è¦').mean() * 100
        }
        return self.kpis

    def create_summary_dashboard(self):
        """ã‚µãƒãƒªãƒ¼ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆ"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))

        # 1. å£²ä¸Šãƒˆãƒ¬ãƒ³ãƒ‰
        daily_sales = self.data.groupby('date')['purchase_amount'].sum().reset_index()
        ax1.plot(daily_sales['date'], daily_sales['purchase_amount'],
                color='#2E86AB', linewidth=2)
        ax1.fill_between(daily_sales['date'], daily_sales['purchase_amount'],
                        alpha=0.3, color='#2E86AB')
        ax1.set_title('æ—¥æ¬¡å£²ä¸Šæ¨ç§»', fontsize=14, fontweight='bold')
        ax1.set_ylabel('å£²ä¸Šé‡‘é¡')
        ax1.grid(True, alpha=0.3)

        # 2. é¡§å®¢ã‚¿ã‚¤ãƒ—åˆ¥æ§‹æˆæ¯”
        customer_counts = self.data['customer_type'].value_counts()
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
        ax2.pie(customer_counts.values, labels=customer_counts.index,
               autopct='%1.1f%%', colors=colors)
        ax2.set_title('é¡§å®¢ã‚¿ã‚¤ãƒ—åˆ¥æ§‹æˆæ¯”', fontsize=14, fontweight='bold')

        # 3. åœ°åŸŸåˆ¥å£²ä¸Š
        region_sales = self.data.groupby('region')['purchase_amount'].sum().sort_values(ascending=True)
        bars = ax3.barh(region_sales.index, region_sales.values, color='#96CEB4')
        ax3.set_title('åœ°åŸŸåˆ¥å£²ä¸Š', fontsize=14, fontweight='bold')
        ax3.set_xlabel('å£²ä¸Šé‡‘é¡')

        # ãƒãƒ¼ã«å€¤ã‚’è¡¨ç¤º
        for i, (bar, value) in enumerate(zip(bars, region_sales.values)):
            ax3.text(value + max(region_sales.values) * 0.01, i,
                    f'{value:,.0f}', va='center', fontsize=10)

        # 4. æº€è¶³åº¦åˆ†å¸ƒ
        ax4.hist(self.data['satisfaction'], bins=20, color='#FFEAA7',
                alpha=0.7, edgecolor='black')
        ax4.axvline(self.data['satisfaction'].mean(), color='red',
                   linestyle='--', linewidth=2, label=f'å¹³å‡: {self.data["satisfaction"].mean():.2f}')
        ax4.set_title('é¡§å®¢æº€è¶³åº¦åˆ†å¸ƒ', fontsize=14, fontweight='bold')
        ax4.set_xlabel('æº€è¶³åº¦')
        ax4.set_ylabel('é »åº¦')
        ax4.legend()

        plt.tight_layout()
        plt.show()

        return fig

    def create_kpi_summary(self):
        """KPIã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        print("=" * 50)
        print("          ğŸ“Š KPI ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ ğŸ“Š")
        print("=" * 50)

        for kpi_name, kpi_value in self.kpis.items():
            if 'ratio' in kpi_name.lower() or 'æ¯”ç‡' in kpi_name:
                print(f"{kpi_name:12}: {kpi_value:6.1f}%")
            elif 'é‡‘é¡' in kpi_name or 'å£²ä¸Š' in kpi_name:
                print(f"{kpi_name:12}: Â¥{kpi_value:10,.0f}")
            elif 'é¡§å®¢æ•°' in kpi_name:
                print(f"{kpi_name:12}: {kpi_value:7,.0f}äºº")
            else:
                print(f"{kpi_name:12}: {kpi_value:10.2f}")

        print("=" * 50)

    def create_advanced_analysis(self):
        """é«˜åº¦ãªåˆ†æãƒ¬ãƒãƒ¼ãƒˆ"""
        print("\nğŸ“ˆ é«˜åº¦ãªåˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
        print("-" * 30)

        # é¡§å®¢ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†æ
        segment_analysis = self.data.groupby('customer_type').agg({
            'purchase_amount': ['count', 'mean', 'sum'],
            'satisfaction': 'mean',
            'age': 'mean'
        }).round(2)

        print("é¡§å®¢ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†æ:")
        print(segment_analysis)

        # ãƒãƒ£ãƒãƒ«åŠ¹æœåˆ†æ
        channel_analysis = self.data.groupby('marketing_channel').agg({
            'purchase_amount': ['count', 'mean'],
            'satisfaction': 'mean'
        }).round(2)

        print("\nãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ãƒãƒ£ãƒãƒ«åˆ†æ:")
        print(channel_analysis)

        # åœ°åŸŸåˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
        region_performance = self.data.groupby('region').agg({
            'purchase_amount': ['sum', 'mean'],
            'satisfaction': 'mean'
        }).round(2)

        print("\nåœ°åŸŸåˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:")
        print(region_performance)

# ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½¿ç”¨ä¾‹
dashboard = BusinessDashboard(viz_df)
kpis = dashboard.calculate_kpis()

dashboard.create_kpi_summary()
dashboard_fig = dashboard.create_summary_dashboard()
dashboard.create_advanced_analysis()

print("\nâœ… Week 7 Day 1-4 å®Œäº†ï¼šé«˜åº¦ãªå¯è¦–åŒ–ã‚’ãƒã‚¹ã‚¿ãƒ¼ã—ã¾ã—ãŸï¼")
```

### **Day 5-7: ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãƒ†ãƒªãƒ³ã‚°ã¨å®Ÿè·µåˆ†æ**

````python
print("\n4. ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãƒ†ãƒªãƒ³ã‚°")
print("=" * 40)

print("ğŸ“– åŠ¹æœçš„ãªãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã®æ§‹ç¯‰:")

class DataStoryTeller:
    """ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãƒ†ãƒªãƒ³ã‚°ã‚¯ãƒ©ã‚¹"""

    def __init__(self, data):
        self.data = data
        self.story_elements = {}
        self.insights = []
        self.recommendations = []

    def analyze_business_context(self):
        """ãƒ“ã‚¸ãƒã‚¹æ–‡è„ˆã®åˆ†æ"""
        print("ğŸ¯ ãƒ“ã‚¸ãƒã‚¹èª²é¡Œã®ç‰¹å®š:")

        # å£²ä¸Šã®æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        daily_sales = self.data.groupby('date')['purchase_amount'].sum()

        # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        sales_trend = daily_sales.rolling(window=30).mean()
        trend_direction = 'increasing' if sales_trend.iloc[-1] > sales_trend.iloc[0] else 'decreasing'

        # å­£ç¯€æ€§åˆ†æ
        monthly_sales = self.data.groupby(self.data['date'].dt.month)['purchase_amount'].mean()
        seasonal_variance = monthly_sales.std() / monthly_sales.mean()

        # é¡§å®¢ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†æ
        segment_revenue = self.data.groupby('customer_type')['purchase_amount'].sum()
        top_segment = segment_revenue.idxmax()

        self.story_elements['context'] = {
            'trend_direction': trend_direction,
            'seasonal_variance': seasonal_variance,
            'top_segment': top_segment,
            'total_revenue': self.data['purchase_amount'].sum(),
            'customer_count': len(self.data)
        }

        print(f"ğŸ“Š å£²ä¸Šãƒˆãƒ¬ãƒ³ãƒ‰: {trend_direction}")
        print(f"ğŸ“… å­£ç¯€å¤‰å‹•ä¿‚æ•°: {seasonal_variance:.3f}")
        print(f"ğŸ‘‘ æœ€é«˜åç›Šã‚»ã‚°ãƒ¡ãƒ³ãƒˆ: {top_segment}")

        return self.story_elements['context']

    def identify_key_insights(self):
        """ä¸»è¦ã‚¤ãƒ³ã‚µã‚¤ãƒˆã®ç‰¹å®š"""
        print("\nğŸ’¡ ä¸»è¦ã‚¤ãƒ³ã‚µã‚¤ãƒˆ:")

        insights = []

        # ã‚¤ãƒ³ã‚µã‚¤ãƒˆ1: é¡§å®¢ã‚¿ã‚¤ãƒ—åˆ¥ã®åç›Šæ€§
        revenue_by_type = self.data.groupby('customer_type').agg({
            'purchase_amount': ['sum', 'mean', 'count']
        })

        vip_avg = revenue_by_type.loc['VIP', ('purchase_amount', 'mean')]
        new_avg = revenue_by_type.loc['æ–°è¦', ('purchase_amount', 'mean')]
        vip_premium = (vip_avg / new_avg - 1) * 100

        insights.append({
            'type': 'revenue_gap',
            'message': f"VIPé¡§å®¢ã®å¹³å‡è³¼å…¥é‡‘é¡ã¯æ–°è¦é¡§å®¢ã®{vip_premium:.0f}%é«˜ã„",
            'impact': 'high',
            'data': {'vip_avg': vip_avg, 'new_avg': new_avg}
        })

        # ã‚¤ãƒ³ã‚µã‚¤ãƒˆ2: åœ°åŸŸæ ¼å·®
        region_performance = self.data.groupby('region')['purchase_amount'].agg(['sum', 'mean'])
        top_region = region_performance['sum'].idxmax()
        bottom_region = region_performance['sum'].idxmin()
        region_gap = region_performance.loc[top_region, 'sum'] / region_performance.loc[bottom_region, 'sum']

        insights.append({
            'type': 'regional_disparity',
            'message': f"{top_region}ã®å£²ä¸Šã¯{bottom_region}ã®{region_gap:.1f}å€",
            'impact': 'medium',
            'data': {'top_region': top_region, 'bottom_region': bottom_region, 'gap': region_gap}
        })

        # ã‚¤ãƒ³ã‚µã‚¤ãƒˆ3: æº€è¶³åº¦ã¨è³¼å…¥é‡‘é¡ã®é–¢ä¿‚
        high_satisfaction = self.data[self.data['satisfaction'] >= 4.5]
        low_satisfaction = self.data[self.data['satisfaction'] < 3.5]

        if len(high_satisfaction) > 0 and len(low_satisfaction) > 0:
            satisfaction_impact = (high_satisfaction['purchase_amount'].mean() /
                                 low_satisfaction['purchase_amount'].mean() - 1) * 100

            insights.append({
                'type': 'satisfaction_correlation',
                'message': f"é«˜æº€è¶³åº¦é¡§å®¢ã¯ä½æº€è¶³åº¦é¡§å®¢ã‚ˆã‚Š{satisfaction_impact:.0f}%å¤šãè³¼å…¥",
                'impact': 'high',
                'data': {'satisfaction_impact': satisfaction_impact}
            })

        # ã‚¤ãƒ³ã‚µã‚¤ãƒˆ4: ãƒãƒ£ãƒãƒ«åŠ¹æœ
        channel_performance = self.data.groupby('marketing_channel').agg({
            'purchase_amount': ['mean', 'count']
        })

        best_channel = channel_performance[('purchase_amount', 'mean')].idxmax()
        worst_channel = channel_performance[('purchase_amount', 'mean')].idxmin()
        channel_diff = (channel_performance.loc[best_channel, ('purchase_amount', 'mean')] /
                       channel_performance.loc[worst_channel, ('purchase_amount', 'mean')] - 1) * 100

        insights.append({
            'type': 'channel_effectiveness',
            'message': f"{best_channel}ãƒãƒ£ãƒãƒ«ã¯{worst_channel}ã‚ˆã‚Š{channel_diff:.0f}%åŠ¹æœçš„",
            'impact': 'medium',
            'data': {'best_channel': best_channel, 'worst_channel': worst_channel}
        })

        self.insights = insights

        # ã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚’è¡¨ç¤º
        for i, insight in enumerate(insights, 1):
            impact_icon = "ğŸ”¥" if insight['impact'] == 'high' else "âš¡" if insight['impact'] == 'medium' else "ğŸ’¡"
            print(f"{impact_icon} ã‚¤ãƒ³ã‚µã‚¤ãƒˆ{i}: {insight['message']}")

        return insights

    def generate_recommendations(self):
        """æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ç”Ÿæˆ"""
        print("\nğŸ¯ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:")

        recommendations = []

        # ã‚¤ãƒ³ã‚µã‚¤ãƒˆãƒ™ãƒ¼ã‚¹ã®æ¨å¥¨äº‹é …
        for insight in self.insights:
            if insight['type'] == 'revenue_gap':
                recommendations.append({
                    'priority': 'high',
                    'action': 'VIPé¡§å®¢å‘ã‘ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ã‚µãƒ¼ãƒ“ã‚¹æ‹¡å……',
                    'reasoning': 'VIPé¡§å®¢ã®é«˜ã„è³¼è²·åŠ›ã‚’æ´»ã‹ã™',
                    'expected_impact': 'å£²ä¸Š15-20%å‘ä¸Š'
                })

                recommendations.append({
                    'priority': 'medium',
                    'action': 'æ–°è¦é¡§å®¢ã®VIPè»¢æ›ãƒ—ãƒ­ã‚°ãƒ©ãƒ é–‹ç™º',
                    'reasoning': 'æ–°è¦é¡§å®¢ã‚’é«˜ä¾¡å€¤é¡§å®¢ã«è‚²æˆ',
                    'expected_impact': 'é¡§å®¢ç”Ÿæ¶¯ä¾¡å€¤30%å‘ä¸Š'
                })

            elif insight['type'] == 'regional_disparity':
                recommendations.append({
                    'priority': 'medium',
                    'action': 'ä½ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åœ°åŸŸã®ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°å¼·åŒ–',
                    'reasoning': 'åœ°åŸŸæ ¼å·®ã®è§£æ¶ˆã«ã‚ˆã‚Šå…¨ä½“å£²ä¸Šåº•ä¸Šã’',
                    'expected_impact': 'åœ°åŸŸå£²ä¸Š10-15%å‘ä¸Š'
                })

            elif insight['type'] == 'satisfaction_correlation':
                recommendations.append({
                    'priority': 'high',
                    'action': 'é¡§å®¢æº€è¶³åº¦å‘ä¸Šãƒ—ãƒ­ã‚°ãƒ©ãƒ å®Ÿæ–½',
                    'reasoning': 'æº€è¶³åº¦å‘ä¸ŠãŒç›´æ¥çš„ã«å£²ä¸Šå‘ä¸Šã«ã¤ãªãŒã‚‹',
                    'expected_impact': 'å¹³å‡è³¼å…¥é‡‘é¡10-12%å‘ä¸Š'
                })

            elif insight['type'] == 'channel_effectiveness':
                recommendations.append({
                    'priority': 'medium',
                    'action': 'åŠ¹æœçš„ãƒãƒ£ãƒãƒ«ã¸ã®äºˆç®—é…åˆ†è¦‹ç›´ã—',
                    'reasoning': 'ROIã®é«˜ã„ãƒãƒ£ãƒãƒ«ã«é›†ä¸­æŠ•è³‡',
                    'expected_impact': 'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ROI20%å‘ä¸Š'
                })

        self.recommendations = recommendations

        # æ¨å¥¨äº‹é …ã‚’è¡¨ç¤º
        for i, rec in enumerate(recommendations, 1):
            priority_icon = "ğŸš¨" if rec['priority'] == 'high' else "âš ï¸" if rec['priority'] == 'medium' else "ğŸ’­"
            print(f"{priority_icon} æ¨å¥¨{i}: {rec['action']}")
            print(f"   ç†ç”±: {rec['reasoning']}")
            print(f"   æœŸå¾…åŠ¹æœ: {rec['expected_impact']}")
            print()

        return recommendations

    def create_executive_summary(self):
        """ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼ä½œæˆ"""
        context = self.story_elements['context']

        summary = f"""
=== ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼ ===

ã€ç¾çŠ¶ã€‘
â€¢ ç·å£²ä¸Š: Â¥{context['total_revenue']:,.0f}
â€¢ ç·é¡§å®¢æ•°: {context['customer_count']:,}äºº
â€¢ å£²ä¸Šãƒˆãƒ¬ãƒ³ãƒ‰: {context['trend_direction']}
â€¢ ä¸»åŠ›ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ: {context['top_segment']}é¡§å®¢

ã€ä¸»è¦èª²é¡Œã€‘
"""

        # é«˜ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚’è¿½åŠ 
        high_impact_insights = [i for i in self.insights if i['impact'] == 'high']
        for insight in high_impact_insights:
            summary += f"â€¢ {insight['message']}\n"

        summary += "\nã€æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã€‘\n"

        # é«˜å„ªå…ˆåº¦æ¨å¥¨äº‹é …ã‚’è¿½åŠ 
        high_priority_recs = [r for r in self.recommendations if r['priority'] == 'high']
        for rec in high_priority_recs:
            summary += f"â€¢ {rec['action']}\n"

        summary += f"\nã€æœŸå¾…åŠ¹æœã€‘\nâ€¢ ç·åˆçš„ãªå£²ä¸Šå‘ä¸Š: 20-30%\nâ€¢ é¡§å®¢æº€è¶³åº¦å‘ä¸Š: 15-20%\n"

        return summary

# ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãƒ†ãƒªãƒ³ã‚°å®Ÿè¡Œ
storyteller = DataStoryTeller(viz_df)

# ã‚¹ãƒˆãƒ¼ãƒªãƒ¼æ§‹ç¯‰
context = storyteller.analyze_business_context()
insights = storyteller.identify_key_insights()
recommendations = storyteller.generate_recommendations()

# ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼
executive_summary = storyteller.create_executive_summary()
print(executive_summary)

print("\n5. A/Bãƒ†ã‚¹ãƒˆåˆ†æ")
print("=" * 40)

# A/Bãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
print("ğŸ§ª A/Bãƒ†ã‚¹ãƒˆåˆ†æ:")

np.random.seed(42)
ab_test_data = []

for i in range(2000):
    # ãƒ†ã‚¹ãƒˆã‚°ãƒ«ãƒ¼ãƒ—ã®å‰²ã‚Šå½“ã¦
    test_group = np.random.choice(['A', 'B'], p=[0.5, 0.5])

    # ã‚°ãƒ«ãƒ¼ãƒ—Aã¨Bã§ç•°ãªã‚‹åŠ¹æœ
    if test_group == 'A':
        conversion_rate = 0.15  # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
        purchase_amount = np.random.exponential(800)
    else:
        conversion_rate = 0.18  # 3%ãƒã‚¤ãƒ³ãƒˆå‘ä¸Š
        purchase_amount = np.random.exponential(850)  # å¹³å‡é‡‘é¡ã‚‚å‘ä¸Š

    # ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®æ±ºå®š
    converted = np.random.random() < conversion_rate

    ab_test_data.append({
        'user_id': f'U{i+1:04d}',
        'test_group': test_group,
        'converted': converted,
        'purchase_amount': purchase_amount if converted else 0,
        'days_active': np.random.randint(1, 30)
    })

ab_df = pd.DataFrame(ab_test_data)

print("A/Bãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿:")
print(ab_df.head())

# A/Bãƒ†ã‚¹ãƒˆåˆ†æé–¢æ•°
def ab_test_analysis(df, group_col, metric_col, alpha=0.05):
    """A/Bãƒ†ã‚¹ãƒˆçµ±è¨ˆåˆ†æ"""

    results = {}

    # ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥åŸºæœ¬çµ±è¨ˆ
    group_stats = df.groupby(group_col)[metric_col].agg(['count', 'mean', 'std'])
    results['group_stats'] = group_stats

    # çµ±è¨ˆçš„æ¤œå®š
    group_a = df[df[group_col] == 'A'][metric_col]
    group_b = df[df[group_col] == 'B'][metric_col]

    # tæ¤œå®š
    t_stat, p_value = stats.ttest_ind(group_a, group_b)

    # åŠ¹æœã‚µã‚¤ã‚ºï¼ˆCohen's dï¼‰
    pooled_std = np.sqrt(((len(group_a) - 1) * group_a.std()**2 +
                         (len(group_b) - 1) * group_b.std()**2) /
                        (len(group_a) + len(group_b) - 2))
    effect_size = (group_b.mean() - group_a.mean()) / pooled_std

    # ä¿¡é ¼åŒºé–“
    ci_a = confidence_interval(group_a)
    ci_b = confidence_interval(group_b)

    results['statistical_test'] = {
        't_statistic': t_stat,
        'p_value': p_value,
        'effect_size': effect_size,
        'significant': p_value < alpha,
        'confidence_interval_a': ci_a,
        'confidence_interval_b': ci_b
    }

    return results

# ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç‡ã®A/Bãƒ†ã‚¹ãƒˆ
conversion_results = ab_test_analysis(ab_df, 'test_group', 'converted')

print("\nğŸ“Š ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç‡A/Bãƒ†ã‚¹ãƒˆçµæœ:")
print("ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥çµ±è¨ˆ:")
print(conversion_results['group_stats'])

test_results = conversion_results['statistical_test']
print(f"\nçµ±è¨ˆçš„æ¤œå®š:")
print(f"tçµ±è¨ˆé‡: {test_results['t_statistic']:.4f}")
print(f"på€¤: {test_results['p_value']:.4f}")
print(f"åŠ¹æœã‚µã‚¤ã‚º: {test_results['effect_size']:.4f}")
print(f"çµ±è¨ˆçš„æœ‰æ„æ€§: {'ã‚ã‚Š' if test_results['significant'] else 'ãªã—'}")

# è³¼å…¥é‡‘é¡ã®A/Bãƒ†ã‚¹ãƒˆï¼ˆã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã—ãŸé¡§å®¢ã®ã¿ï¼‰
converted_users = ab_df[ab_df['converted'] == True]
if len(converted_users) > 0:
    purchase_results = ab_test_analysis(converted_users, 'test_group', 'purchase_amount')

    print("\nğŸ’° è³¼å…¥é‡‘é¡A/Bãƒ†ã‚¹ãƒˆçµæœï¼ˆã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³é¡§å®¢ã®ã¿ï¼‰:")
    print("ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥çµ±è¨ˆ:")
    print(purchase_results['group_stats'])

print("\n6. ã‚³ãƒ›ãƒ¼ãƒˆåˆ†æ")
print("=" * 40)

print("ğŸ‘¥ ã‚³ãƒ›ãƒ¼ãƒˆåˆ†æ:")

# ã‚³ãƒ›ãƒ¼ãƒˆåˆ†æãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
cohort_data = []
np.random.seed(42)

start_date = pd.Timestamp('2023-01-01')
for cohort_month in range(12):  # 12ãƒ¶æœˆã®ã‚³ãƒ›ãƒ¼ãƒˆ
    cohort_start = start_date + pd.DateOffset(months=cohort_month)
    initial_users = np.random.randint(100, 500)  # åˆæœŸãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°

    for period in range(12 - cohort_month):  # è¦³æ¸¬æœŸé–“
        # ãƒªãƒ†ãƒ³ã‚·ãƒ§ãƒ³ç‡ï¼ˆæ™‚é–“ã¨ã¨ã‚‚ã«æ¸›å°‘ï¼‰
        retention_rate = 1.0 * (0.85 ** period)  # æœˆ15%ãšã¤æ¸›å°‘
        retained_users = int(initial_users * retention_rate)

        # 1äººã‚ãŸã‚Šã®å£²ä¸Šï¼ˆæ™‚é–“ã¨ã¨ã‚‚ã«å¢—åŠ å‚¾å‘ï¼‰
        revenue_per_user = 100 + period * 10 + np.random.normal(0, 20)
        total_revenue = retained_users * max(0, revenue_per_user)

        cohort_data.append({
            'cohort_month': cohort_start.strftime('%Y-%m'),
            'period': period,
            'initial_users': initial_users,
            'retained_users': retained_users,
            'retention_rate': retained_users / initial_users,
            'revenue_per_user': revenue_per_user,
            'total_revenue': total_revenue
        })

cohort_df = pd.DataFrame(cohort_data)

print("ã‚³ãƒ›ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«:")
print(cohort_df.head(10))

# ã‚³ãƒ›ãƒ¼ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ã®ä½œæˆ
def create_cohort_table(df, cohort_col, period_col, metric_col):
    """ã‚³ãƒ›ãƒ¼ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ"""
    cohort_table = df.pivot_table(
        index=cohort_col,
        columns=period_col,
        values=metric_col,
        fill_value=0
    )
    return cohort_table

# ãƒªãƒ†ãƒ³ã‚·ãƒ§ãƒ³ç‡ã®ã‚³ãƒ›ãƒ¼ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«
retention_cohort = create_cohort_table(cohort_df, 'cohort_month', 'period', 'retention_rate')

print("\nğŸ“Š ãƒªãƒ†ãƒ³ã‚·ãƒ§ãƒ³ç‡ã‚³ãƒ›ãƒ¼ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«:")
print(retention_cohort.round(3))

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚ãŸã‚Šå£²ä¸Šã®ã‚³ãƒ›ãƒ¼ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«
revenue_cohort = create_cohort_table(cohort_df, 'cohort_month', 'period', 'revenue_per_user')

print("\nğŸ’° ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚ãŸã‚Šå£²ä¸Šã‚³ãƒ›ãƒ¼ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«:")
print(revenue_cohort.round(0))

# ã‚³ãƒ›ãƒ¼ãƒˆåˆ†æã®å¯è¦–åŒ–
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

# ãƒªãƒ†ãƒ³ã‚·ãƒ§ãƒ³ç‡ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
sns.heatmap(retention_cohort, annot=True, fmt='.2f', cmap='YlOrRd', ax=ax1)
ax1.set_title('ãƒªãƒ†ãƒ³ã‚·ãƒ§ãƒ³ç‡ã‚³ãƒ›ãƒ¼ãƒˆåˆ†æ')
ax1.set_xlabel('æœŸé–“ï¼ˆæœˆï¼‰')
ax1.set_ylabel('ã‚³ãƒ›ãƒ¼ãƒˆæœˆ')

# å£²ä¸Šãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
sns.heatmap(revenue_cohort, annot=True, fmt='.0f', cmap='YlGnBu', ax=ax2)
ax2.set_title('ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚ãŸã‚Šå£²ä¸Šã‚³ãƒ›ãƒ¼ãƒˆåˆ†æ')
ax2.set_xlabel('æœŸé–“ï¼ˆæœˆï¼‰')
ax2.set_ylabel('ã‚³ãƒ›ãƒ¼ãƒˆæœˆ')

plt.tight_layout()
plt.show()

print("\nâœ… Week 7 å®Œäº†ï¼šé«˜åº¦ãªå¯è¦–åŒ–ã¨çµ±è¨ˆåˆ†æã‚’ãƒã‚¹ã‚¿ãƒ¼ã—ã¾ã—ãŸï¼")

---

## Week 8: å®Ÿå‹™ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç·åˆæ¼”ç¿’

### ğŸ¯ **ã“ã®é€±ã®ã‚´ãƒ¼ãƒ«**
- ã“ã‚Œã¾ã§ã®å­¦ç¿’å†…å®¹ã‚’çµ±åˆã™ã‚‹
- å®Ÿéš›ã®ãƒ“ã‚¸ãƒã‚¹èª²é¡Œã‚’è§£æ±ºã™ã‚‹
- å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹
- ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãƒ¬ãƒ™ãƒ«ã®åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã™ã‚‹

---

### **Day 1-7: ç·åˆå®Ÿå‹™ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ**

```python
print("\nğŸš€ Week 8: å®Ÿå‹™ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç·åˆæ¼”ç¿’")
print("=" * 50)

print("ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦: ECã‚µã‚¤ãƒˆã®å£²ä¸Šæœ€é©åŒ–")
print("""
ã€èƒŒæ™¯ã€‘
ã‚ãªãŸã¯ECã‚µã‚¤ãƒˆé‹å–¶ä¼šç¤¾ã®ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªã‚¹ãƒˆã¨ã—ã¦ã€
å£²ä¸Šä½è¿·ã®åŸå› åˆ†æã¨æ”¹å–„ç­–ã®ææ¡ˆã‚’æ±‚ã‚ã‚‰ã‚Œã¾ã—ãŸã€‚

ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‘
- é¡§å®¢ãƒ‡ãƒ¼ã‚¿ï¼ˆ50,000ä»¶ï¼‰
- å•†å“ãƒ‡ãƒ¼ã‚¿ï¼ˆ1,000ä»¶ï¼‰
- å–å¼•ãƒ‡ãƒ¼ã‚¿ï¼ˆ200,000ä»¶ï¼‰
- ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã€åºƒå‘Šï¼‰
- ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆ100,000ä»¶ï¼‰

ã€ç›®æ¨™ã€‘
1. å£²ä¸Šä½è¿·ã®åŸå› ç‰¹å®š
2. é¡§å®¢ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
3. å•†å“æ¨å¥¨ã‚·ã‚¹ãƒ†ãƒ ã®åŸºç¤åˆ†æ
4. ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°åŠ¹æœæ¸¬å®š
5. å…·ä½“çš„ãªæ”¹å–„ææ¡ˆ

ã€æœŸå¾…æˆæœç‰©ã€‘
- ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ¬ãƒãƒ¼ãƒˆ
- ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼
- ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³
- ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
""")

import warnings
warnings.filterwarnings('ignore')

print("\n1. ãƒ‡ãƒ¼ã‚¿æº–å‚™ã¨æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æ")
print("=" * 40)

# å®Ÿéš›ã®ãƒ“ã‚¸ãƒã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’æ¨¡æ“¬ã—ãŸå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ
class EcommerceDataGenerator:
    """ECã‚µã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå™¨"""

    def __init__(self, seed=42):
        np.random.seed(seed)
        self.customers = None
        self.products = None
        self.transactions = None
        self.reviews = None
        self.campaigns = None

    def generate_customers(self, n_customers=50000):
        """é¡§å®¢ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        customers = []

        for i in range(n_customers):
            # é¡§å®¢åŸºæœ¬æƒ…å ±
            age = np.random.normal(40, 15)
            age = max(18, min(80, age))

            # å¹´é½¢ã«åŸºã¥ãè¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³
            if age < 30:
                income = np.random.normal(35000, 10000)
                tech_savvy = np.random.uniform(0.7, 1.0)
                price_sensitive = np.random.uniform(0.6, 0.9)
            elif age < 50:
                income = np.random.normal(55000, 20000)
                tech_savvy = np.random.uniform(0.5, 0.8)
                price_sensitive = np.random.uniform(0.4, 0.7)
            else:
                income = np.random.normal(45000, 15000)
                tech_savvy = np.random.uniform(0.3, 0.6)
                price_sensitive = np.random.uniform(0.3, 0.6)

            income = max(15000, income)

            customers.append({
                'customer_id': f'C{i+1:05d}',
                'age': int(age),
                'gender': np.random.choice(['M', 'F']),
                'income': income,
                'registration_date': pd.Timestamp('2022-01-01') + pd.Timedelta(days=np.random.randint(0, 730)),
                'city': np.random.choice(['Tokyo', 'Osaka', 'Nagoya', 'Fukuoka', 'Sapporo'],
                                       p=[0.3, 0.2, 0.15, 0.15, 0.2]),
                'tech_savvy': tech_savvy,
                'price_sensitive': price_sensitive,
                'customer_segment': None  # å¾Œã§è¨ˆç®—
            })

        self.customers = pd.DataFrame(customers)

        # é¡§å®¢ã‚»ã‚°ãƒ¡ãƒ³ãƒˆè¨ˆç®—
        self.customers['customer_segment'] = self.customers.apply(
            lambda row: self._calculate_segment(row), axis=1
        )

        return self.customers

    def _calculate_segment(self, customer):
        """é¡§å®¢ã‚»ã‚°ãƒ¡ãƒ³ãƒˆè¨ˆç®—"""
        if customer['income'] > 60000 and customer['tech_savvy'] > 0.7:
            return 'Premium Tech'
        elif customer['income'] > 50000:
            return 'High Value'
        elif customer['age'] < 35 and customer['tech_savvy'] > 0.6:
            return 'Young Digital'
        elif customer['price_sensitive'] > 0.7:
            return 'Price Conscious'
        else:
            return 'Standard'

    def generate_products(self, n_products=1000):
        """å•†å“ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        categories = ['Electronics', 'Fashion', 'Home', 'Books', 'Sports', 'Beauty']
        brands = ['BrandA', 'BrandB', 'BrandC', 'BrandD', 'BrandE']

        products = []

        for i in range(n_products):
            category = np.random.choice(categories)

            # ã‚«ãƒ†ã‚´ãƒªåˆ¥ä¾¡æ ¼è¨­å®š
            if category == 'Electronics':
                base_price = np.random.lognormal(6, 1)  # é«˜ä¾¡æ ¼å¸¯
            elif category == 'Fashion':
                base_price = np.random.lognormal(4, 0.8)
            elif category == 'Home':
                base_price = np.random.lognormal(4.5, 0.9)
            elif category == 'Books':
                base_price = np.random.lognormal(2.5, 0.5)  # ä½ä¾¡æ ¼å¸¯
            elif category == 'Sports':
                base_price = np.random.lognormal(4.2, 0.7)
            else:  # Beauty
                base_price = np.random.lognormal(3.5, 0.6)

            products.append({
                'product_id': f'P{i+1:04d}',
                'category': category,
                'brand': np.random.choice(brands),
                'price': round(base_price, 2),
                'cost': round(base_price * np.random.uniform(0.4, 0.7), 2),
                'launch_date': pd.Timestamp('2020-01-01') + pd.Timedelta(days=np.random.randint(0, 1460)),
                'avg_rating': np.random.normal(4.0, 0.8),
                'review_count': np.random.poisson(50),
                'inventory': np.random.randint(0, 1000)
            })

        self.products = pd.DataFrame(products)
        self.products['avg_rating'] = self.products['avg_rating'].clip(1, 5)
        self.products['profit_margin'] = (self.products['price'] - self.products['cost']) / self.products['price']

        return self.products

    def generate_transactions(self, n_transactions=200000):
        """å–å¼•ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        if self.customers is None or self.products is None:
            raise ValueError("é¡§å®¢ãƒ‡ãƒ¼ã‚¿ã¨å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’å…ˆã«ç”Ÿæˆã—ã¦ãã ã•ã„")

        transactions = []

        for i in range(n_transactions):
            # é¡§å®¢é¸æŠï¼ˆã‚¢ã‚¯ãƒ†ã‚£ãƒ–é¡§å®¢ã»ã©å–å¼•ç¢ºç‡é«˜ï¼‰
            customer = self.customers.sample(1).iloc[0]

            # å•†å“é¸æŠï¼ˆé¡§å®¢ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«åŸºã¥ãé¸å¥½ï¼‰
            if customer['customer_segment'] == 'Premium Tech':
                preferred_categories = ['Electronics', 'Beauty']
                price_factor = 1.5
            elif customer['customer_segment'] == 'High Value':
                preferred_categories = ['Fashion', 'Home']
                price_factor = 1.2
            elif customer['customer_segment'] == 'Young Digital':
                preferred_categories = ['Electronics', 'Fashion', 'Sports']
                price_factor = 0.8
            elif customer['customer_segment'] == 'Price Conscious':
                preferred_categories = ['Books', 'Home']
                price_factor = 0.6
            else:
                preferred_categories = list(self.products['category'].unique())
                price_factor = 1.0

            # å•†å“ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            available_products = self.products[
                (self.products['category'].isin(preferred_categories)) &
                (self.products['price'] <= customer['income'] * price_factor / 1000)
            ]

            if len(available_products) == 0:
                available_products = self.products.sample(10)

            product = available_products.sample(1).iloc[0]

            # å–å¼•è©³ç´°
            quantity = np.random.randint(1, 5)

            # å­£ç¯€æ€§åŠ¹æœ
            transaction_date = pd.Timestamp('2023-01-01') + pd.Timedelta(days=np.random.randint(0, 365))
            seasonal_factor = 1 + 0.3 * np.sin(2 * np.pi * transaction_date.dayofyear / 365)

            # å‰²å¼•é©ç”¨
            discount = np.random.uniform(0, 0.3) if np.random.random() < 0.2 else 0

            final_price = product['price'] * (1 - discount) * seasonal_factor

            transactions.append({
                'transaction_id': f'T{i+1:06d}',
                'customer_id': customer['customer_id'],
                'product_id': product['product_id'],
                'transaction_date': transaction_date,
                'quantity': quantity,
                'unit_price': round(final_price, 2),
                'total_amount': round(final_price * quantity, 2),
                'discount': discount,
                'channel': np.random.choice(['Web', 'Mobile', 'Store'], p=[0.5, 0.4, 0.1])
            })

        self.transactions = pd.DataFrame(transactions)
        return self.transactions

    def generate_reviews(self, n_reviews=100000):
        """ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        if self.transactions is None:
            raise ValueError("å–å¼•ãƒ‡ãƒ¼ã‚¿ã‚’å…ˆã«ç”Ÿæˆã—ã¦ãã ã•ã„")

        # å–å¼•ã®ä¸€éƒ¨ã«ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ä»˜ä¸
        review_transactions = self.transactions.sample(n_reviews, replace=True)

        reviews = []

        for _, transaction in review_transactions.iterrows():
            customer = self.customers[self.customers['customer_id'] == transaction['customer_id']].iloc[0]
            product = self.products[self.products['product_id'] == transaction['product_id']].iloc[0]

            # é¡§å®¢ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã¨å•†å“è©•ä¾¡ã«åŸºã¥ããƒ¬ãƒ“ãƒ¥ãƒ¼
            base_rating = product['avg_rating']

            if customer['customer_segment'] == 'Premium Tech':
                rating_adjustment = np.random.normal(0.5, 0.3)
            elif customer['customer_segment'] == 'Price Conscious':
                rating_adjustment = np.random.normal(-0.3, 0.4)
            else:
                rating_adjustment = np.random.normal(0, 0.3)

            final_rating = max(1, min(5, base_rating + rating_adjustment))

            reviews.append({
                'review_id': f'R{len(reviews)+1:06d}',
                'customer_id': transaction['customer_id'],
                'product_id': transaction['product_id'],
                'transaction_id': transaction['transaction_id'],
                'rating': round(final_rating),
                'review_date': transaction['transaction_date'] + pd.Timedelta(days=np.random.randint(1, 30)),
                'helpful_votes': np.random.poisson(3)
            })

        self.reviews = pd.DataFrame(reviews)
        return self.reviews

# ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Ÿè¡Œ
print("ğŸ“Š å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆä¸­...")
data_generator = EcommerceDataGenerator()

customers = data_generator.generate_customers(50000)
products = data_generator.generate_products(1000)
transactions = data_generator.generate_transactions(200000)
reviews = data_generator.generate_reviews(100000)

print("âœ… ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†")
print(f"é¡§å®¢ãƒ‡ãƒ¼ã‚¿: {len(customers):,}ä»¶")
print(f"å•†å“ãƒ‡ãƒ¼ã‚¿: {len(products):,}ä»¶")
print(f"å–å¼•ãƒ‡ãƒ¼ã‚¿: {len(transactions):,}ä»¶")
print(f"ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿: {len(reviews):,}ä»¶")

print("\nğŸ“Š åŸºæœ¬çš„ãªæ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æ:")

# ãƒ‡ãƒ¼ã‚¿æ¦‚è¦³
print("ã€é¡§å®¢ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ã€‘")
print(customers.describe(include='all'))

print("\nã€å•†å“ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ã€‘")
print(products.describe(include='all'))

print("\nã€å–å¼•ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ã€‘")
print(transactions.describe(include='all'))

print("\n2. ãƒ‡ãƒ¼ã‚¿çµ±åˆã¨ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°")
print("=" * 40)

class EcommerceAnalyzer:
    """ECã‚µã‚¤ãƒˆåˆ†æç·åˆã‚¯ãƒ©ã‚¹"""

    def __init__(self, customers, products, transactions, reviews):
        self.customers = customers
        self.products = products
        self.transactions = transactions
        self.reviews = reviews
        self.integrated_data = None
        self.customer_features = None
        self.product_features = None

    def integrate_data(self):
        """ãƒ‡ãƒ¼ã‚¿çµ±åˆ"""
        print("ğŸ”— ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Ÿè¡Œä¸­...")

        # å–å¼•ãƒ‡ãƒ¼ã‚¿ã«å•†å“æƒ…å ±ã‚’çµåˆ
        trans_product = pd.merge(self.transactions, self.products, on='product_id', how='left')

        # é¡§å®¢æƒ…å ±ã‚’çµåˆ
        integrated = pd.merge(trans_product, self.customers, on='customer_id', how='left')

        # ãƒ¬ãƒ“ãƒ¥ãƒ¼æƒ…å ±ã‚’é›†è¨ˆã—ã¦çµåˆ
        review_agg = self.reviews.groupby('transaction_id').agg({
            'rating': 'mean',
            'helpful_votes': 'sum'
        }).reset_index()

        integrated = pd.merge(integrated, review_agg, on='transaction_id', how='left')

        self.integrated_data = integrated
        print(f"âœ… çµ±åˆãƒ‡ãƒ¼ã‚¿ä½œæˆå®Œäº†: {len(integrated):,}è¡Œ")

        return integrated

    def create_customer_features(self):
        """é¡§å®¢ç‰¹å¾´é‡ä½œæˆ"""
        print("ğŸ‘¥ é¡§å®¢ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°...")

        # åŸºæœ¬çš„ãªå–å¼•çµ±è¨ˆ
        customer_stats = self.integrated_data.groupby('customer_id').agg({
            'total_amount': ['sum', 'mean', 'count', 'std'],
            'quantity': 'sum',
            'discount': 'mean',
            'rating': 'mean',
            'transaction_date': ['min', 'max']
        }).reset_index()

        # åˆ—åã‚’å¹³å¦åŒ–
        customer_stats.columns = ['customer_id', 'total_spent', 'avg_order_value', 'order_count',
                                'spend_volatility', 'total_quantity', 'avg_discount', 'avg_rating',
                                'first_purchase', 'last_purchase']

        # è¿½åŠ ç‰¹å¾´é‡è¨ˆç®—
        customer_stats['days_as_customer'] = (customer_stats['last_purchase'] - customer_stats['first_purchase']).dt.days
        customer_stats['purchase_frequency'] = customer_stats['order_count'] / (customer_stats['days_as_customer'] + 1) * 365
        customer_stats['spend_volatility'] = customer_stats['spend_volatility'].fillna(0)

        # RFMåˆ†æ
        reference_date = self.integrated_data['transaction_date'].max()
        customer_stats['recency'] = (reference_date - customer_stats['last_purchase']).dt.days
        customer_stats['frequency'] = customer_stats['order_count']
        customer_stats['monetary'] = customer_stats['total_spent']

        # RFMã‚¹ã‚³ã‚¢è¨ˆç®—
        customer_stats['R_score'] = pd.qcut(customer_stats['recency'], 5, labels=[5,4,3,2,1])
        customer_stats['F_score'] = pd.qcut(customer_stats['frequency'].rank(method='first'), 5, labels=[1,2,3,4,5])
        customer_stats['M_score'] = pd.qcut(customer_stats['monetary'], 5, labels=[1,2,3,4,5])

        # ç·åˆRFMã‚¹ã‚³ã‚¢
        customer_stats['RFM_score'] = (customer_stats['R_score'].astype(int) +
                                     customer_stats['F_score'].astype(int) +
                                     customer_stats['M_score'].astype(int))

        # é¡§å®¢åŸºæœ¬æƒ…å ±ã¨ãƒãƒ¼ã‚¸
        customer_features = pd.merge(customer_stats, self.customers, on='customer_id', how='left')

        # é¡§å®¢ä¾¡å€¤ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ
        def customer_value_segment(row):
            if row['RFM_score'] >= 12:
                return 'Champions'
            elif row['RFM_score'] >= 10:
                return 'Loyal Customers'
            elif row['RFM_score'] >= 8:
                return 'Potential Loyalists'
            elif row['RFM_score'] >= 6:
                return 'At Risk'
            else:
                return 'Lost'

        customer_features['value_segment'] = customer_features.apply(customer_value_segment, axis=1)

        self.customer_features = customer_features
        print(f"âœ… é¡§å®¢ç‰¹å¾´é‡ä½œæˆå®Œäº†: {len(customer_features):,}é¡§å®¢")

        return customer_features

    def create_product_features(self):
        """å•†å“ç‰¹å¾´é‡ä½œæˆ"""
        print("ğŸ“¦ å•†å“ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°...")

        # åŸºæœ¬çš„ãªå£²ä¸Šçµ±è¨ˆ
        product_stats = self.integrated_data.groupby('product_id').agg({
            'total_amount': ['sum', 'mean', 'count'],
            'quantity': 'sum',
            'discount': 'mean',
            'rating': ['mean', 'count'],
            'transaction_date': ['min', 'max']
        }).reset_index()

        # åˆ—åã‚’å¹³å¦åŒ–
        product_stats.columns = ['product_id', 'total_revenue', 'avg_price_paid', 'sale_count',
                               'total_units_sold', 'avg_discount_given', 'avg_customer_rating',
                               'rating_count', 'first_sale', 'last_sale']

        # è¿½åŠ ç‰¹å¾´é‡
        product_stats['days_on_market'] = (product_stats['last_sale'] - product_stats['first_sale']).dt.days
        product_stats['sales_velocity'] = product_stats['sale_count'] / (product_stats['days_on_market'] + 1) * 365

        # å•†å“åŸºæœ¬æƒ…å ±ã¨ãƒãƒ¼ã‚¸
        product_features = pd.merge(product_stats, self.products, on='product_id', how='left')

        # å•†å“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ
        def product_performance_segment(row):
            if row['total_revenue'] > product_features['total_revenue'].quantile(0.8):
                return 'Star'
            elif row['total_revenue'] > product_features['total_revenue'].quantile(0.6):
                return 'Rising'
            elif row['total_revenue'] > product_features['total_revenue'].quantile(0.4):
                return 'Steady'
            elif row['total_revenue'] > product_features['total_revenue'].quantile(0.2):
                return 'Declining'
            else:
                return 'Dog'

        product_features['performance_segment'] = product_features.apply(product_performance_segment, axis=1)

        self.product_features = product_features
        print(f"âœ… å•†å“ç‰¹å¾´é‡ä½œæˆå®Œäº†: {len(product_features):,}å•†å“")

        return product_features

    def analyze_sales_trends(self):
        """å£²ä¸Šãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ"""
        print("ğŸ“ˆ å£²ä¸Šãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ...")

        # æ—¥æ¬¡å£²ä¸Š
        daily_sales = self.integrated_data.groupby('transaction_date').agg({
            'total_amount': 'sum',
            'transaction_id': 'count'
        }).reset_index()
        daily_sales.columns = ['date', 'revenue', 'transaction_count']

        # ç§»å‹•å¹³å‡
        daily_sales['revenue_ma7'] = daily_sales['revenue'].rolling(window=7).mean()
        daily_sales['revenue_ma30'] = daily_sales['revenue'].rolling(window=30).mean()

        # æˆé•·ç‡
        daily_sales['revenue_growth'] = daily_sales['revenue'].pct_change() * 100

        # æœˆæ¬¡é›†è¨ˆ
        monthly_sales = self.integrated_data.groupby(self.integrated_data['transaction_date'].dt.to_period('M')).agg({
            'total_amount': 'sum',
            'customer_id': 'nunique',
            'transaction_id': 'count'
        }).reset_index()
        monthly_sales.columns = ['month', 'revenue', 'unique_customers', 'transactions']
        monthly_sales['avg_order_value'] = monthly_sales['revenue'] / monthly_sales['transactions']
        monthly_sales['revenue_per_customer'] = monthly_sales['revenue'] / monthly_sales['unique_customers']

        return daily_sales, monthly_sales

    def segment_analysis(self):
        """ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†æ"""
        print("ğŸ¯ é¡§å®¢ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†æ...")

        if self.customer_features is None:
            self.create_customer_features()

        # ä¾¡å€¤ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥åˆ†æ
        segment_analysis = self.customer_features.groupby('value_segment').agg({
            'customer_id': 'count',
            'total_spent': ['mean', 'sum'],
            'avg_order_value': 'mean',
            'order_count': 'mean',
            'purchase_frequency': 'mean',
            'recency': 'mean'
        }).round(2)

        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚»ã‚°ãƒ¡ãƒ³ãƒˆè³¼è²·è¡Œå‹•
        category_segment = self.integrated_data.merge(
            self.customer_features[['customer_id', 'value_segment']],
            on='customer_id'
        ).groupby(['value_segment', 'category'])['total_amount'].sum().unstack(fill_value=0)

        return segment_analysis, category_segment

    def product_recommendation_analysis(self):
        """å•†å“æ¨å¥¨åˆ†æ"""
        print("ğŸ›’ å•†å“æ¨å¥¨åˆ†æ...")

        # é¡§å®¢-å•†å“ãƒãƒˆãƒªãƒƒã‚¯ã‚¹
        customer_product_matrix = self.integrated_data.groupby(['customer_id', 'product_id'])['total_amount'].sum().unstack(fill_value=0)

        # å•†å“é–“ç›¸é–¢ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãªå”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰
        product_correlation = customer_product_matrix.corr()

        # äººæ°—å•†å“ï¼ˆè³¼å…¥å›æ•°é †ï¼‰
        popular_products = self.integrated_data.groupby('product_id').agg({
            'customer_id': 'nunique',
            'total_amount': 'sum',
            'quantity': 'sum'
        }).sort_values('customer_id', ascending=False)

        return product_correlation, popular_products

# åˆ†æå®Ÿè¡Œ
analyzer = EcommerceAnalyzer(customers, products, transactions, reviews)

# ãƒ‡ãƒ¼ã‚¿çµ±åˆ
integrated_data = analyzer.integrate_data()

# ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
customer_features = analyzer.create_customer_features()
product_features = analyzer.create_product_features()

# åˆ†æå®Ÿè¡Œ
daily_sales, monthly_sales = analyzer.analyze_sales_trends()
segment_analysis, category_segment = analyzer.segment_analysis()
product_correlation, popular_products = analyzer.product_recommendation_analysis()

print("\n3. ä¸»è¦åˆ†æçµæœ")
print("=" * 40)

print("ğŸ“Š å£²ä¸Šã‚µãƒãƒªãƒ¼:")
print(f"ç·å£²ä¸Š: Â¥{integrated_data['total_amount'].sum():,.0f}")
print(f"ç·å–å¼•æ•°: {len(integrated_data):,}")
print(f"ãƒ¦ãƒ‹ãƒ¼ã‚¯é¡§å®¢æ•°: {integrated_data['customer_id'].nunique():,}")
print(f"å¹³å‡æ³¨æ–‡é‡‘é¡: Â¥{integrated_data['total_amount'].mean():.0f}")

print("\nğŸ‘¥ é¡§å®¢ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†æ:")
print(segment_analysis)

print("\nğŸ“¦ å•†å“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:")
print("ã‚«ãƒ†ã‚´ãƒªåˆ¥å£²ä¸Š:")
category_revenue = integrated_data.groupby('category')['total_amount'].sum().sort_values(ascending=False)
print(category_revenue)

print("\nğŸ¯ äººæ°—å•†å“TOP10:")
top_products = product_features.nlargest(10, 'total_revenue')[['product_id', 'category', 'brand', 'total_revenue', 'sale_count']]
print(top_products)

print("\n4. ãƒ“ã‚¸ãƒã‚¹ã‚¤ãƒ³ã‚µã‚¤ãƒˆã¨æ”¹å–„ææ¡ˆ")
print("=" * 40)

class BusinessInsightEngine:
    """ãƒ“ã‚¸ãƒã‚¹ã‚¤ãƒ³ã‚µã‚¤ãƒˆåˆ†æã‚¨ãƒ³ã‚¸ãƒ³"""

    def __init__(self, analyzer):
        self.analyzer = analyzer
        self.insights = []
        self.recommendations = []

    def identify_revenue_issues(self):
        """å£²ä¸Šèª²é¡Œã®ç‰¹å®š"""
        print("ğŸ’¡ å£²ä¸Šèª²é¡Œã®ç‰¹å®š:")

        # æœˆæ¬¡å£²ä¸Šãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        monthly_data = self.analyzer.integrated_data.groupby(
            self.analyzer.integrated_data['transaction_date'].dt.to_period('M')
        )['total_amount'].sum()

        # å£²ä¸Šæˆé•·ç‡
        growth_rate = monthly_data.pct_change().mean() * 100

        if growth_rate < 0:
            self.insights.append({
                'type': 'revenue_decline',
                'severity': 'high',
                'message': f'æœˆæ¬¡å£²ä¸Šæˆé•·ç‡ãŒãƒã‚¤ãƒŠã‚¹ ({growth_rate:.1f}%)',
                'data': growth_rate
            })

        # é¡§å®¢ç²å¾—ã‚³ã‚¹ãƒˆ vs LTVåˆ†æ
        customer_ltv = self.analyzer.customer_features['total_spent'].mean()
        avg_orders = self.analyzer.customer_features['order_count'].mean()

        if customer_ltv < avg_orders * 50:  # ä»®ã®åŸºæº–
            self.insights.append({
                'type': 'low_ltv',
                'severity': 'medium',
                'message': f'é¡§å®¢ç”Ÿæ¶¯ä¾¡å€¤ãŒä½ã„ (å¹³å‡Â¥{customer_ltv:.0f})',
                'data': customer_ltv
            })

        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥èª²é¡Œ
        champions = self.analyzer.customer_features[
            self.analyzer.customer_features['value_segment'] == 'Champions'
        ]

        if len(champions) / len(self.analyzer.customer_features) < 0.1:
            self.insights.append({
                'type': 'low_champions',
                'severity': 'high',
                'message': f'Championsé¡§å®¢ã®å‰²åˆãŒä½ã„ ({len(champions)/len(self.analyzer.customer_features)*100:.1f}%)',
                'data': len(champions) / len(self.analyzer.customer_features)
            })

        return self.insights

    def analyze_customer_behavior(self):
        """é¡§å®¢è¡Œå‹•åˆ†æ"""
        print("ğŸ‘¥ é¡§å®¢è¡Œå‹•åˆ†æ:")

        # ãƒãƒ£ãƒ¼ãƒ³åˆ†æ
        recent_customers = self.analyzer.customer_features[
            self.analyzer.customer_features['recency'] <= 30
        ]
        at_risk_customers = self.analyzer.customer_features[
            self.analyzer.customer_features['recency'] > 90
        ]

        churn_rate = len(at_risk_customers) / len(self.analyzer.customer_features)

        if churn_rate > 0.3:
            self.insights.append({
                'type': 'high_churn_risk',
                'severity': 'high',
                'message': f'ãƒãƒ£ãƒ¼ãƒ³ãƒªã‚¹ã‚¯é¡§å®¢ãŒå¤šã„ ({churn_rate*100:.1f}%)',
                'data': churn_rate
            })

        # è³¼å…¥é »åº¦åˆ†æ
        low_frequency = self.analyzer.customer_features[
            self.analyzer.customer_features['purchase_frequency'] < 2
        ]

        if len(low_frequency) / len(self.analyzer.customer_features) > 0.5:
            self.insights.append({
                'type': 'low_purchase_frequency',
                'severity': 'medium',
                'message': f'è³¼å…¥é »åº¦ã®ä½ã„é¡§å®¢ãŒå¤šã„ ({len(low_frequency)/len(self.analyzer.customer_features)*100:.1f}%)',
                'data': len(low_frequency) / len(self.analyzer.customer_features)
            })

        return self.insights

    def generate_recommendations(self):
        """æ”¹å–„ææ¡ˆç”Ÿæˆ"""
        print("ğŸ¯ æ”¹å–„ææ¡ˆ:")

        recommendations = []

        for insight in self.insights:
            if insight['type'] == 'revenue_decline':
                recommendations.extend([
                    {
                        'priority': 'P1',
                        'category': 'å£²ä¸Šæ”¹å–„',
                        'action': 'Championsé¡§å®¢å‘ã‘ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ãƒ—ãƒ­ã‚°ãƒ©ãƒ é–‹ç™º',
                        'rationale': 'é«˜ä¾¡å€¤é¡§å®¢ã‹ã‚‰ã®å£²ä¸Šã‚’æœ€å¤§åŒ–',
                        'expected_impact': 'å£²ä¸Š15-20%å‘ä¸Š',
                        'timeline': '3ãƒ¶æœˆ'
                    },
                    {
                        'priority': 'P1',
                        'category': 'å£²ä¸Šæ”¹å–„',
                        'action': 'ã‚¯ãƒ­ã‚¹ã‚»ãƒ«ãƒ»ã‚¢ãƒƒãƒ—ã‚»ãƒ«æ–½ç­–å¼·åŒ–',
                        'rationale': 'æ—¢å­˜é¡§å®¢ã®è³¼å…¥å˜ä¾¡å‘ä¸Š',
                        'expected_impact': 'AOV 10-15%å‘ä¸Š',
                        'timeline': '2ãƒ¶æœˆ'
                    }
                ])

            elif insight['type'] == 'high_churn_risk':
                recommendations.append({
                    'priority': 'P1',
                    'category': 'é¡§å®¢ç¶­æŒ',
                    'action': 'ãƒãƒ£ãƒ¼ãƒ³äºˆé˜²ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³å®Ÿæ–½',
                    'rationale': '90æ—¥ä»¥ä¸Šéè³¼å…¥é¡§å®¢ã®å‘¼ã³æˆ»ã—',
                    'expected_impact': 'ãƒãƒ£ãƒ¼ãƒ³ç‡20%å‰Šæ¸›',
                    'timeline': '1ãƒ¶æœˆ'
                })

            elif insight['type'] == 'low_purchase_frequency':
                recommendations.append({
                    'priority': 'P2',
                    'category': 'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ',
                    'action': 'ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºãƒ‰ãƒ¡ãƒ¼ãƒ«é…ä¿¡å¼·åŒ–',
                    'rationale': 'é¡§å®¢ã¨ã®æ¥è§¦é »åº¦å‘ä¸Š',
                    'expected_impact': 'è³¼å…¥é »åº¦30%å‘ä¸Š',
                    'timeline': '2ãƒ¶æœˆ'
                })

            elif insight['type'] == 'low_champions':
                recommendations.append({
                    'priority': 'P2',
                    'category': 'é¡§å®¢è‚²æˆ',
                    'action': 'ãƒ­ã‚¤ãƒ¤ãƒ«ãƒ†ã‚£ãƒ—ãƒ­ã‚°ãƒ©ãƒ æ”¹å–„',
                    'rationale': 'ä¸­ä¾¡å€¤é¡§å®¢ã®Championsè»¢æ›ä¿ƒé€²',
                    'expected_impact': 'Championsæ¯”ç‡2å€',
                    'timeline': '6ãƒ¶æœˆ'
                })

        # å•†å“é–¢é€£ã®æ¨å¥¨äº‹é …
        top_categories = self.analyzer.integrated_data.groupby('category')['total_amount'].sum().nlargest(3)

        recommendations.append({
            'priority': 'P2',
            'category': 'å•†å“æˆ¦ç•¥',
            'action': f'ä¸»åŠ›ã‚«ãƒ†ã‚´ãƒª({", ".join(top_categories.index)})ã®å•†å“æ‹¡å……',
            'rationale': 'å£²ä¸Šã®å¤šã„ã‚«ãƒ†ã‚´ãƒªã§ã®ã‚·ã‚§ã‚¢æ‹¡å¤§',
            'expected_impact': 'ã‚«ãƒ†ã‚´ãƒªå£²ä¸Š10%å‘ä¸Š',
            'timeline': '4ãƒ¶æœˆ'
        })

        self.recommendations = recommendations
        return recommendations

# ã‚¤ãƒ³ã‚µã‚¤ãƒˆåˆ†æå®Ÿè¡Œ
insight_engine = BusinessInsightEngine(analyzer)
insights = insight_engine.identify_revenue_issues()
behavior_insights = insight_engine.analyze_customer_behavior()
recommendations = insight_engine.generate_recommendations()

print("\nğŸ’¡ ä¸»è¦ã‚¤ãƒ³ã‚µã‚¤ãƒˆ:")
for i, insight in enumerate(insights, 1):
    severity_icon = "ğŸš¨" if insight['severity'] == 'high' else "âš ï¸"
    print(f"{severity_icon} {i}. {insight['message']}")

print("\nğŸ¯ æ”¹å–„ææ¡ˆ:")
for i, rec in enumerate(recommendations, 1):
    priority_icon = "ğŸ”¥" if rec['priority'] == 'P1' else "âš¡"
    print(f"{priority_icon} {rec['priority']} {rec['action']}")
    print(f"   ã‚«ãƒ†ã‚´ãƒª: {rec['category']}")
    print(f"   æ ¹æ‹ : {rec['rationale']}")
    print(f"   æœŸå¾…åŠ¹æœ: {rec['expected_impact']}")
    print(f"   å®Ÿæ–½æœŸé–“: {rec['timeline']}")
    print()

print("\n5. ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼")
print("=" * 40)

def create_executive_summary():
    """ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼ä½œæˆ"""

    # KPIè¨ˆç®—
    total_revenue = integrated_data['total_amount'].sum()
    total_customers = integrated_data['customer_id'].nunique()
    avg_order_value = integrated_data['total_amount'].mean()

    champions_count = len(customer_features[customer_features['value_segment'] == 'Champions'])
    champions_ratio = champions_count / len(customer_features) * 100

    at_risk_count = len(customer_features[customer_features['value_segment'] == 'At Risk'])
    at_risk_ratio = at_risk_count / len(customer_features) * 100

    summary = f"""
=== ECã‚µã‚¤ãƒˆå£²ä¸Šæœ€é©åŒ– ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼ ===

ã€ç¾çŠ¶åˆ†æã€‘
ğŸ“Š ãƒ“ã‚¸ãƒã‚¹æ¦‚æ³
â€¢ ç·å£²ä¸Š: Â¥{total_revenue:,.0f}
â€¢ ç·é¡§å®¢æ•°: {total_customers:,}äºº
â€¢ å¹³å‡æ³¨æ–‡é‡‘é¡: Â¥{avg_order_value:.0f}
â€¢ Championsé¡§å®¢æ¯”ç‡: {champions_ratio:.1f}%
â€¢ ãƒªã‚¹ã‚¯é¡§å®¢æ¯”ç‡: {at_risk_ratio:.1f}%

ã€ä¸»è¦èª²é¡Œã€‘
"""

    # é«˜å„ªå…ˆåº¦ã®èª²é¡Œã‚’è¿½åŠ 
    high_severity_insights = [i for i in insights if i['severity'] == 'high']
    for insight in high_severity_insights:
        summary += f"ğŸš¨ {insight['message']}\n"

    summary += f"""
ã€æ”¹å–„æ©Ÿä¼šã€‘
ğŸ’° åç›Šå‘ä¸Šãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«
â€¢ Championsé¡§å®¢æ‹¡å……ã«ã‚ˆã‚Šå£²ä¸Š20%å‘ä¸Šå¯èƒ½
â€¢ ã‚¯ãƒ­ã‚¹ã‚»ãƒ«å¼·åŒ–ã§AOV 15%å‘ä¸Šå¯èƒ½
â€¢ ãƒãƒ£ãƒ¼ãƒ³é˜²æ­¢ã§å¹´é–“å£²ä¸Š10%ä¿è­·å¯èƒ½

ã€æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã€‘
"""

    # P1å„ªå…ˆåº¦ã®æ¨å¥¨äº‹é …
    p1_recommendations = [r for r in recommendations if r['priority'] == 'P1']
    for rec in p1_recommendations:
        summary += f"ğŸ”¥ {rec['action']} ({rec['timeline']})\n"

    summary += f"""
ã€æœŸå¾…æˆæœã€‘
ğŸ“ˆ å®Ÿæ–½ã«ã‚ˆã‚‹æœŸå¾…åŠ¹æœ
â€¢ çŸ­æœŸï¼ˆ3ãƒ¶æœˆï¼‰: å£²ä¸Š15%å‘ä¸Š
â€¢ ä¸­æœŸï¼ˆ6ãƒ¶æœˆï¼‰: é¡§å®¢ç”Ÿæ¶¯ä¾¡å€¤25%å‘ä¸Š
â€¢ é•·æœŸï¼ˆ12ãƒ¶æœˆï¼‰: æŒç¶šçš„æˆé•·åŸºç›¤ç¢ºç«‹

ã€æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã€‘
1. P1æ–½ç­–ã®å³åº§å®Ÿè¡Œ
2. é€±æ¬¡KPIãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ä½“åˆ¶æ§‹ç¯‰
3. æœˆæ¬¡åŠ¹æœæ¸¬å®šã¨PDCAå®Ÿæ–½
"""

    return summary

executive_summary = create_executive_summary()
print(executive_summary)

print("\n6. æœ€çµ‚æˆæœç‰©ã¨ãƒã‚¯ã‚¹ãƒˆã‚¹ãƒ†ãƒƒãƒ—")
print("=" * 40)

print("ğŸ“‹ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæˆæœç‰©:")
print("""
âœ… å®Œæˆã—ãŸæˆæœç‰©:
1. åŒ…æ‹¬çš„ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ¬ãƒãƒ¼ãƒˆ
2. é¡§å®¢ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³çµæœ
3. å•†å“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
4. ãƒ“ã‚¸ãƒã‚¹ã‚¤ãƒ³ã‚µã‚¤ãƒˆç‰¹å®š
5. å…·ä½“çš„æ”¹å–„ææ¡ˆ
6. ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼

ğŸ“Š åˆ†æã§ä½¿ç”¨ã—ãŸæ‰‹æ³•:
â€¢ ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã¨å‰å‡¦ç†
â€¢ ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° (apply/lambdaæ´»ç”¨)
â€¢ ãƒ‡ãƒ¼ã‚¿çµåˆ (merge/concat)
â€¢ é«˜åº¦ãªé›†è¨ˆã¨groupbyæ“ä½œ
â€¢ æ™‚ç³»åˆ—åˆ†æ
â€¢ çµ±è¨ˆçš„åˆ†æ
â€¢ å¯è¦–åŒ–ã¨ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆ
â€¢ RFMåˆ†æ
â€¢ ã‚³ãƒ›ãƒ¼ãƒˆåˆ†æ
â€¢ å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°åŸºç¤

ğŸ› ï¸ å®Ÿè£…ã—ãŸã‚¹ã‚­ãƒ«:
â€¢ pandasæ“ä½œã®å®Œå…¨ç¿’å¾—
â€¢ ãƒ“ã‚¸ãƒã‚¹èª²é¡Œè§£æ±ºã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
â€¢ ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãƒ†ãƒªãƒ³ã‚°
â€¢ ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–å‘ã‘ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
""")

print("\nğŸš€ ãƒã‚¯ã‚¹ãƒˆã‚¹ãƒ†ãƒƒãƒ—:")
print("""
ã€å³åº§ã«å®Ÿè¡Œå¯èƒ½ã€‘
1. ç‰¹å®šã—ãŸæ”¹å–„æ–½ç­–ã®å®Ÿè£…
2. KPIãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®æ—¥æ¬¡é‹ç”¨
3. A/Bãƒ†ã‚¹ãƒˆã«ã‚ˆã‚‹æ–½ç­–åŠ¹æœæ¤œè¨¼

ã€ã‚¹ã‚­ãƒ«ç™ºå±•ã€‘
1. æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹é«˜åº¦ãªäºˆæ¸¬åˆ†æ
2. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰
3. ã‚ˆã‚Šé«˜åº¦ãªæ¨å¥¨ã‚·ã‚¹ãƒ†ãƒ é–‹ç™º
4.å› æœæ¨è«–ã«ã‚ˆã‚‹æ–½ç­–åŠ¹æœæ¸¬å®š

ã€ãƒ“ã‚¸ãƒã‚¹ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã€‘
1. ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ–ãƒ³ãªæ„æ€æ±ºå®šæ–‡åŒ–ã®æ§‹ç¯‰
2. ç¶™ç¶šçš„ãªæ”¹å–„ãƒ—ãƒ­ã‚»ã‚¹ã®ç¢ºç«‹
3. ç«¶åˆå„ªä½æ€§ã®ç¢ºä¿
""")

print("\nğŸ“ ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼")
print("=" * 50)
print("""
pandas DataFrame å®Ÿå‹™å®Œå…¨ãƒã‚¹ã‚¿ãƒ¼è¬›åº§ Advancedç‰ˆã‚’å®Œèµ°ã—ã¾ã—ãŸï¼

ã‚ãªãŸã¯ä»¥ä¸‹ã‚’ç¿’å¾—ã—ã¾ã—ãŸ:

ğŸ† Technical Skills:
âœ… pandasæ“ä½œã®å®Œå…¨ç¿’å¾—
âœ… å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®åŠ¹ç‡çš„å‡¦ç†
âœ… è¤‡é›‘ãªãƒ‡ãƒ¼ã‚¿çµåˆãƒ»å¤‰æ›
âœ… é«˜åº¦ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
âœ… çµ±è¨ˆåˆ†æã¨å¯è¦–åŒ–
âœ… å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰

ğŸ¯ Business Skills:
âœ… ãƒ“ã‚¸ãƒã‚¹èª²é¡Œã®æ§‹é€ åŒ–
âœ… ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®ã‚¤ãƒ³ã‚µã‚¤ãƒˆæŠ½å‡º
âœ… ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–å‘ã‘ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
âœ… ã‚¢ã‚¯ã‚·ãƒ§ãƒŠãƒ–ãƒ«ãªæ”¹å–„ææ¡ˆ
âœ… ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãƒ†ãƒªãƒ³ã‚°

ğŸš€ Next Level:
ã‚ãªãŸã¯ä»Šã‚„å®Ÿå‹™ã§å³æˆ¦åŠ›ã¨ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚
ç¶™ç¶šçš„ãªå­¦ç¿’ã¨å®Ÿè·µã§ã€ã•ã‚‰ãªã‚‹é«˜ã¿ã‚’ç›®æŒ‡ã—ã¾ã—ã‚‡ã†ï¼

Keep analyzing, keep growing! ğŸ“Šâœ¨
""")

print("\nâœ… Week 8 å®Œäº†ï¼šå®Ÿå‹™ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’å®Œé‚ã—ã¾ã—ãŸï¼")
print("âœ… å…¨8é€±é–“ã®å­¦ç¿’ãƒ—ãƒ­ã‚°ãƒ©ãƒ å®Œäº†ï¼")

---

## ğŸ“š ç·åˆã¾ã¨ã‚ã¨ä»Šå¾Œã®å­¦ç¿’ãƒ‘ã‚¹

### **ç¿’å¾—ã—ãŸã‚¹ã‚­ãƒ«ã®ç·æ£‹è­œ**

```python
print("\nğŸ“Š ç¿’å¾—ã‚¹ã‚­ãƒ«ç·ã¾ã¨ã‚")
print("=" * 50)

skills_mastered = {
    'Week 1: pandasåŸºç¤': [
        'DataFrameã®å†…éƒ¨æ§‹é€ ç†è§£',
        'ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿ä½œæˆ',
        'ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–',
        'ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°',
        'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–'
    ],

    'Week 2: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ“ä½œ': [
        'ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å®Œå…¨ç†è§£',
        'éšå±¤ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆMultiIndexï¼‰',
        'é«˜é€Ÿãƒ‡ãƒ¼ã‚¿é¸æŠãƒ†ã‚¯ãƒ‹ãƒƒã‚¯',
        'å‹•çš„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°',
        'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒã¨æœ€é©åŒ–'
    ],

    'Week 3: ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°': [
        'æ¬ æå€¤ã®ç¨®é¡ã¨å¯¾å‡¦æ³•',
        'æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹æ¬ æå€¤è£œå®Œ',
        'ç•°å¸¸å€¤æ¤œå‡ºæ‰‹æ³•',
        'æ–‡å­—åˆ—ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°',
        'è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³'
    ],

    'Week 4: ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°': [
        'apply()ã¨lambdaå¼ã®å®Œå…¨æ´»ç”¨',
        'æ™‚ç³»åˆ—ç‰¹å¾´é‡ä½œæˆ',
        'çµ±è¨ˆçš„ç‰¹å¾´é‡ç”Ÿæˆ',
        'ç›¸äº’ä½œç”¨ç‰¹å¾´é‡',
        'ã‚«ã‚¹ã‚¿ãƒ å¤‰æ›å™¨ã®å®Ÿè£…'
    ],

    'Week 5: ãƒ‡ãƒ¼ã‚¿çµåˆ': [
        'merge()ã¨concat()ã®å®Œå…¨æ”»ç•¥',
        'è¤‡é›‘ãªçµåˆãƒ‘ã‚¿ãƒ¼ãƒ³',
        'çµåˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°',
        'å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®åŠ¹ç‡çš„çµåˆ',
        'å“è³ªãƒã‚§ãƒƒã‚¯ä»˜ãçµåˆ'
    ],

    'Week 6: é›†è¨ˆãƒ»æ™‚ç³»åˆ—åˆ†æ': [
        'é«˜åº¦ãªgroupbyæ“ä½œ',
        'ã‚«ã‚¹ã‚¿ãƒ é›†è¨ˆé–¢æ•°',
        'æ™‚ç³»åˆ—ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°',
        'å­£ç¯€æ€§åˆ†è§£',
        'ãƒ“ã‚¸ãƒã‚¹æŒ‡æ¨™è¨ˆç®—'
    ],

    'Week 7: å¯è¦–åŒ–ãƒ»çµ±è¨ˆåˆ†æ': [
        'é«˜åº¦ãªå¯è¦–åŒ–ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯',
        'çµ±è¨ˆçš„ä»®èª¬æ¤œå®š',
        'å›å¸°åˆ†æ',
        'A/Bãƒ†ã‚¹ãƒˆåˆ†æ',
        'ã‚³ãƒ›ãƒ¼ãƒˆåˆ†æ'
    ],

    'Week 8: å®Ÿå‹™ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ': [
        'å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ',
        'åŒ…æ‹¬çš„ãƒ‡ãƒ¼ã‚¿åˆ†æ',
        'ãƒ“ã‚¸ãƒã‚¹ã‚¤ãƒ³ã‚µã‚¤ãƒˆæŠ½å‡º',
        'ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ',
        'æ”¹å–„ææ¡ˆã®å…·ä½“åŒ–'
    ]
}

for week, skills in skills_mastered.items():
    print(f"\nğŸ¯ {week}")
    for skill in skills:
        print(f"  âœ… {skill}")
````

### **æ¨å¥¨å­¦ç¿’ãƒ‘ã‚¹**

```python
print("\nğŸ—ºï¸ ä»Šå¾Œã®å­¦ç¿’ãƒ‘ã‚¹")
print("=" * 50)

learning_paths = {
    'æ©Ÿæ¢°å­¦ç¿’ã‚¹ãƒšã‚·ãƒ£ãƒªã‚¹ãƒˆ': [
        'scikit-learnå®Œå…¨ãƒã‚¹ã‚¿ãƒ¼',
        'XGBoost, LightGBMæ´»ç”¨',
        'ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚° (TensorFlow/PyTorch)',
        'AutoML (H2O.ai, AutoGluon)',
        'MLOps (MLflow, Kubeflow)'
    ],

    'ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢': [
        'Apache Spark / PySpark',
        'ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰ (Airflow)',
        'ã‚¯ãƒ©ã‚¦ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹ (AWS, GCP, Azure)',
        'ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç† (Kafka, Flink)',
        'ãƒ‡ãƒ¼ã‚¿ã‚¦ã‚§ã‚¢ãƒã‚¦ã‚¹è¨­è¨ˆ'
    ],

    'ãƒ“ã‚¸ãƒã‚¹ã‚¢ãƒŠãƒªã‚¹ãƒˆ': [
        'Tableau / Power BI ä¸Šç´š',
        'çµ±è¨ˆå­¦ãƒ»å®Ÿé¨“è¨­è¨ˆ',
        'ãƒ“ã‚¸ãƒã‚¹æˆ¦ç•¥åˆ†æ',
        'ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆåˆ†æ',
        'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°åˆ†æ'
    ],

    'ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆ': [
        'é«˜åº¦ãªçµ±è¨ˆæ‰‹æ³•',
        'å› æœæ¨è«– (causal inference)',
        'æ™‚ç³»åˆ—äºˆæ¸¬ (Prophet, ARIMA)',
        'è‡ªç„¶è¨€èªå‡¦ç† (NLP)',
        'æ¨å¥¨ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰'
    ]
}

for path, skills in learning_paths.items():
    print(f"\nğŸ¯ {path}")
    for skill in skills:
        print(f"  ğŸ“š {skill}")
```

### **ç¶™ç¶šçš„æˆé•·ã®ãŸã‚ã®ææ¡ˆ**

```python
print("\nğŸ’ª ç¶™ç¶šçš„æˆé•·ã®ãŸã‚ã«")
print("=" * 50)

print("""
ğŸ”„ å®šæœŸçš„ãªå®Ÿè·µ:
â€¢ é€±1å›ï¼šæ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®åˆ†æå®Ÿè·µ
â€¢ æœˆ1å›ï¼šéå»ã®ã‚³ãƒ¼ãƒ‰ã®ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°
â€¢ å››åŠæœŸ1å›ï¼šæ–°ã—ã„æ‰‹æ³•ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®å­¦ç¿’

ğŸ¤ ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£å‚åŠ :
â€¢ Kaggleç«¶æŠ€ã¸ã®å‚åŠ 
â€¢ ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æ´»å‹•
â€¢ ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè²¢çŒ®
â€¢ å‹‰å¼·ä¼šãƒ»ã‚«ãƒ³ãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹å‚åŠ 

ğŸ“– ç¶™ç¶šå­¦ç¿’:
â€¢ æœ€æ–°ã®pandasæ©Ÿèƒ½è¿½è·¡
â€¢ æ–°ã—ã„åˆ†ææ‰‹æ³•ã®ç¿’å¾—
â€¢ ãƒ“ã‚¸ãƒã‚¹é ˜åŸŸçŸ¥è­˜ã®æ‹¡å……
â€¢ æŠ€è¡“ãƒ–ãƒ­ã‚°ãƒ»è«–æ–‡ã®å®šæœŸçš„ãªèª­æ›¸

ğŸ¯ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå®Ÿè·µ:
â€¢ å€‹äººãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ç¶™ç¶š
â€¢ å®Ÿå‹™èª²é¡Œã¸ã®ç©æ¥µçš„é©ç”¨
â€¢ ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®å……å®Ÿ
â€¢ æˆæœã®å®šé‡çš„æ¸¬å®š
""")

print("\nğŸŒŸ æœ€å¾Œã«")
print("=" * 50)
print("""
ã“ã®8é€±é–“ã§ã€ã‚ãªãŸã¯pandasã®çœŸã®ãƒã‚¹ã‚¿ãƒ¼ã«ãªã‚Šã¾ã—ãŸã€‚
ã—ã‹ã—ã€ã“ã‚Œã¯æ–°ãŸãªã‚¹ã‚¿ãƒ¼ãƒˆãƒ©ã‚¤ãƒ³ã«ç«‹ã£ãŸã«éãã¾ã›ã‚“ã€‚

ãƒ‡ãƒ¼ã‚¿ã®ä¸–ç•Œã¯æ—¥ã€…é€²åŒ–ã—ã¦ã„ã¾ã™ã€‚
å­¦ã³ç¶šã‘ã€å®Ÿè·µã—ç¶šã‘ã€ä¾¡å€¤ã‚’å‰µé€ ã—ç¶šã‘ã¦ãã ã•ã„ã€‚

ã‚ãªãŸã®ãƒ‡ãƒ¼ã‚¿åˆ†æã‚¹ã‚­ãƒ«ãŒã€
ãƒ“ã‚¸ãƒã‚¹ã®æˆåŠŸã¨ç¤¾ä¼šã®ç™ºå±•ã«è²¢çŒ®ã™ã‚‹ã“ã¨ã‚’æœŸå¾…ã—ã¦ã„ã¾ã™ã€‚

Happy Data Analysis! ğŸ¼ğŸ“Šâœ¨
""")
```

---

**ğŸ‰ pandas DataFrame å®Ÿå‹™å®Œå…¨ãƒã‚¹ã‚¿ãƒ¼è¬›åº§ Advanced ç‰ˆ å®Œäº†ï¼**

ã“ã‚Œã§ 8 é€±é–“ã«ã‚ãŸã‚‹åŒ…æ‹¬çš„ãªå­¦ç¿’ãƒ—ãƒ­ã‚°ãƒ©ãƒ ãŒå®Œæˆã—ã¾ã—ãŸã€‚å®Ÿå‹™ã§å³åº§ã«æ´»ç”¨ã§ãã‚‹é«˜åº¦ãª pandas ã‚¹ã‚­ãƒ«ã‚’ç¿’å¾—ã—ã€ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã¨ã—ã¦ã®åŸºç›¤ã‚’ç¯‰ãä¸Šã’ã¾ã—ãŸï¼
