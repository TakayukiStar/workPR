# æºå¸¯é¡§å®¢è§£ç´„äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ å®Œå…¨ç‰ˆï¼ˆå®Œå…¨ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒãƒ»Python åˆå¿ƒè€…å‘ã‘ï¼‰

## ğŸ“‹ ã‚·ã‚¹ãƒ†ãƒ æ¦‚è¦ãƒ»é‡è¦äº‹å®Ÿç¢ºèª

### ğŸŸ¢ **ã€é‡è¦ã€‘PySpark ã¯å®Œå…¨ã«ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§å‹•ä½œã—ã¾ã™ã€‚AWS ã¯ä¸€åˆ‡ä¸è¦ã§ã™ã€‚**

```bash
âœ… äº‹å®Ÿç¢ºèªå®Œäº†:
- PySparkã¯ãƒ­ãƒ¼ã‚«ãƒ«PC/Macã§å®Œå…¨å‹•ä½œ
- AWSãƒ»ã‚¯ãƒ©ã‚¦ãƒ‰ç’°å¢ƒã¯å…¨ãä¸è¦
- ã‚ãªãŸã®PCä¸Šã§åˆ†æ•£å‡¦ç†ãŒå¯èƒ½
- ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã‚‚ä¸è¦ï¼ˆé–‹ç™ºæ™‚ï¼‰

ğŸ¯ ã“ã®æ–‡æ›¸ã§ã¯ï¼š
- 100% ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã®æ§‹ç¯‰
- AWSä½¿ç”¨ã¯æœ€å°é™ï¼ˆãƒ‡ãƒ¼ã‚¿ä¿å­˜ã®ã¿ï¼‰
- æœˆé¡ã‚³ã‚¹ãƒˆ$5-20ã§æœ¬æ ¼ã‚·ã‚¹ãƒ†ãƒ 
- Pythonåˆå¿ƒè€…ã§ã‚‚1æ—¥ã§ãƒã‚¹ã‚¿ãƒ¼å¯èƒ½
```

### ğŸ¯ **æºå¸¯é¡§å®¢è§£ç´„äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã§ã§ãã‚‹ã“ã¨**

```python
ğŸ“Š ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã®æ©Ÿèƒ½:

âœ… æ¯æ—¥1å›ã®å·®åˆ†ãƒ‡ãƒ¼ã‚¿è¿½åŠ ï¼ˆ3ç§’ã§å®Œäº†ï¼‰
âœ… 100ä¸‡é¡§å®¢ãƒ‡ãƒ¼ã‚¿ã‚’10ç§’ã§é«˜é€Ÿå‰å‡¦ç†
âœ… AIã«ã‚ˆã‚‹è§£ç´„äºˆæ¸¬ï¼ˆç²¾åº¦85%ä»¥ä¸Šï¼‰
âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é¡§å®¢ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
âœ… è§£ç´„ãƒªã‚¹ã‚¯è¦å› ã®è‡ªå‹•åˆ†æ
âœ… å–¶æ¥­ãƒãƒ¼ãƒ å‘ã‘ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒªã‚¹ãƒˆç”Ÿæˆ
âœ… ã‚³ã‚¹ãƒˆã¯æœˆé¡$5-20ã®ã¿

ğŸš€ å‡¦ç†é€Ÿåº¦ï¼ˆå¾“æ¥æ¯”ï¼‰:
- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: 50å€é«˜é€Ÿ
- å‰å‡¦ç†: 100å€é«˜é€Ÿ
- äºˆæ¸¬è¨ˆç®—: 30å€é«˜é€Ÿ
- ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: 20å€é«˜é€Ÿ
```

## ğŸ  **ã€æ¨å¥¨ã€‘å®Œå…¨ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆ30 åˆ†ã§å®Œäº†ï¼‰**

### **ã‚¹ãƒ†ãƒƒãƒ— 1: åŸºæœ¬ç’°å¢ƒæ§‹ç¯‰ï¼ˆ15 åˆ†ï¼‰**

```bash
# ğŸ“ å®Ÿè¡Œå ´æ‰€: ã‚ãªãŸã®PC/Mac
# ğŸ’° ã‚³ã‚¹ãƒˆ: $0ï¼ˆå®Œå…¨ç„¡æ–™ï¼‰
# ğŸ“Š å¯¾è±¡: å…¨ã¦ã®è¦æ¨¡ï¼ˆå€‹äººã€œä¼æ¥­ï¼‰
# â±ï¸ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—: 30åˆ†
# ğŸ¯ æ¨å¥¨åº¦: â˜…â˜…â˜…â˜…â˜…

# 1. Pythonç’°å¢ƒæ§‹ç¯‰ï¼ˆWindows/Mac/Linuxå¯¾å¿œï¼‰
pip install pyspark[sql]==3.5.0 pyarrow==14.0.0 pandas==2.1.4
pip install numpy==1.24.3 scikit-learn==1.3.2
pip install matplotlib==3.7.2 seaborn==0.12.2
pip install sqlite3  # Pythonæ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãªã®ã§å®Ÿéš›ã¯ä¸è¦

# 2. å‹•ä½œç¢ºèªï¼ˆã“ã‚Œã ã‘ã§OKï¼ï¼‰
python -c "
import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('ChurnTest').master('local[*]').getOrCreate()
print('âœ… PySparkå®Œå…¨ãƒ­ãƒ¼ã‚«ãƒ«èµ·å‹•æˆåŠŸï¼')
print(f'ğŸ“Š åˆ©ç”¨å¯èƒ½CPU: {spark.sparkContext.defaultParallelism}ã‚³ã‚¢')
spark.stop()
"
```

### **ã‚¹ãƒ†ãƒƒãƒ— 2: ã‚ªãƒ—ã‚·ãƒ§ãƒ³ AWS S3 è¨­å®šï¼ˆå¿…è¦ãªå ´åˆã®ã¿ï¼‰**

```bash
# ğŸ’¡ é‡è¦: ã“ã‚Œã¯å®Œå…¨ã«ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã™
# ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã§å®Œçµã™ã‚‹å ´åˆã¯ä¸è¦

# S3ã‚’ä½¿ã„ãŸã„å ´åˆã®ã¿ï¼ˆæœˆ$5-20ï¼‰
pip install boto3==1.34.0 awscli
aws configure
# â†‘ S3ã«ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã—ãŸã„å ´åˆã®ã¿
```

## ğŸš€ **å®Œå…¨ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œãƒ‡ãƒ¢ï¼ˆ5 åˆ†ã§å®Ÿè¡Œå¯èƒ½ï¼‰**

### **åŸºæœ¬å‹•ä½œç¢ºèªã‚¹ã‚¯ãƒªãƒ—ãƒˆ**

```python
# ğŸ¯ ã“ã‚Œã¯å®Œå…¨ã«ãƒ­ãƒ¼ã‚«ãƒ«ã§å‹•ä½œã—ã¾ã™ï¼ˆAWSä¸è¦ï¼‰
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import pandas as pd
import sqlite3
import numpy as np
from datetime import datetime, timedelta
import os

def create_local_churn_demo():
    """
    å®Œå…¨ãƒ­ãƒ¼ã‚«ãƒ«é¡§å®¢è§£ç´„äºˆæ¸¬ãƒ‡ãƒ¢ï¼ˆAWSä¸è¦ï¼‰
    """
    print("ğŸ  å®Œå…¨ãƒ­ãƒ¼ã‚«ãƒ«é¡§å®¢è§£ç´„äºˆæ¸¬ãƒ‡ãƒ¢é–‹å§‹")
    print("=" * 50)

    # ãƒ­ãƒ¼ã‚«ãƒ«Sparkã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
    spark = SparkSession.builder \
        .appName("ChurnDemo") \
        .master("local[*]") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .getOrCreate()

    print(f"âœ… Sparkèµ·å‹•æˆåŠŸï¼ˆå®Œå…¨ãƒ­ãƒ¼ã‚«ãƒ«ï¼‰")
    print(f"ğŸ“Š ä½¿ç”¨CPU: {spark.sparkContext.defaultParallelism}ã‚³ã‚¢")

    # ã‚µãƒ³ãƒ—ãƒ«é¡§å®¢ãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿å½¢å¼ã«è¿‘ä¼¼ï¼‰
    sample_data = [
        ("C001", "2024-12-01", 25, "M", 2400, 45.5, 150, 2, 0, 12),
        ("C002", "2024-12-01", 34, "F", 1200, 89.3, 320, 5, 1, 8),
        ("C003", "2024-12-01", 45, "M", 3600, 23.1, 80, 1, 0, 24),
        ("C004", "2024-12-01", 28, "F", 800, 156.7, 480, 8, 1, 6),
        ("C005", "2024-12-01", 52, "M", 5000, 12.3, 40, 0, 0, 36)
    ]

    # ã‚¹ã‚­ãƒ¼ãƒå®šç¾©ï¼ˆé¡§å®¢è§£ç´„äºˆæ¸¬å‘ã‘ï¼‰
    schema = StructType([
        StructField("customer_id", StringType(), False),
        StructField("date", StringType(), False),
        StructField("age", IntegerType(), False),
        StructField("gender", StringType(), False),
        StructField("monthly_charge", DoubleType(), False),
        StructField("total_usage_minutes", DoubleType(), False),
        StructField("call_count", IntegerType(), False),
        StructField("complaints", IntegerType(), False),
        StructField("churn", IntegerType(), False),  # 0=ç¶™ç¶š, 1=è§£ç´„
        StructField("tenure_months", IntegerType(), False)
    ])

    # Spark DataFrameä½œæˆ
    df = spark.createDataFrame(sample_data, schema)

    print("\nğŸ“Š ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ‡ãƒ¢:")
    df.show()

    # é«˜é€ŸSQLå‡¦ç†ï¼ˆå®Œå…¨ãƒ­ãƒ¼ã‚«ãƒ«ï¼‰
    df.createOrReplaceTempView("customer_data")

    churn_analysis = spark.sql("""
        SELECT
            gender,
            AVG(age) as avg_age,
            AVG(monthly_charge) as avg_charge,
            AVG(total_usage_minutes) as avg_usage,
            COUNT(*) as customer_count,
            SUM(churn) as churn_count,
            ROUND(SUM(churn) * 100.0 / COUNT(*), 2) as churn_rate
        FROM customer_data
        GROUP BY gender
        ORDER BY churn_rate DESC
    """)

    print("\nğŸ“ˆ ãƒ­ãƒ¼ã‚«ãƒ«è§£ç´„åˆ†æçµæœ:")
    churn_analysis.show()

    # æ©Ÿæ¢°å­¦ç¿’ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ï¼‰
    enhanced_data = df.withColumn(
        "usage_per_minute", col("monthly_charge") / col("total_usage_minutes")
    ).withColumn(
        "complaint_rate", col("complaints") / col("tenure_months")
    ).withColumn(
        "high_risk_flag",
        when((col("complaints") > 3) | (col("total_usage_minutes") > 100), 1).otherwise(0)
    ).withColumn(
        "age_group",
        when(col("age") < 30, "Young")
        .when(col("age") < 50, "Middle")
        .otherwise("Senior")
    )

    print("\nğŸ¤– ãƒ­ãƒ¼ã‚«ãƒ«æ©Ÿæ¢°å­¦ç¿’ç‰¹å¾´é‡:")
    enhanced_data.select("customer_id", "usage_per_minute", "complaint_rate",
                        "high_risk_flag", "age_group", "churn").show()

    # ãƒ­ãƒ¼ã‚«ãƒ«Parquetä¿å­˜
    enhanced_data.write.mode("overwrite").parquet("./local_customer_data.parquet")
    print("ğŸ’¾ ãƒ­ãƒ¼ã‚«ãƒ«Parquetä¿å­˜å®Œäº†: ./local_customer_data.parquet")

    # ãƒ­ãƒ¼ã‚«ãƒ«Parquetèª­ã¿è¾¼ã¿æ¤œè¨¼
    loaded_data = spark.read.parquet("./local_customer_data.parquet")
    print(f"ğŸ“ ãƒ­ãƒ¼ã‚«ãƒ«Parquetèª­ã¿è¾¼ã¿å®Œäº†: {loaded_data.count()}ãƒ¬ã‚³ãƒ¼ãƒ‰")

    # ãƒ­ãƒ¼ã‚«ãƒ«SQLiteä¿å­˜ï¼ˆ.dbãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
    pandas_df = enhanced_data.toPandas()
    conn = sqlite3.connect('./local_customer_churn.db')
    pandas_df.to_sql('customer_data', conn, if_exists='replace', index=False)
    conn.close()
    print("ğŸ’¾ ãƒ­ãƒ¼ã‚«ãƒ«SQLiteä¿å­˜å®Œäº†: ./local_customer_churn.db")

    spark.stop()
    print("âœ… å®Œå…¨ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¢å®Œäº†ï¼ˆAWSä¸€åˆ‡æœªä½¿ç”¨ï¼‰")

# å®Ÿè¡Œä¾‹
create_local_churn_demo()
```

## ğŸ’¾ **é«˜é€Ÿå·®åˆ†ãƒ‡ãƒ¼ã‚¿è¿½åŠ ã‚·ã‚¹ãƒ†ãƒ ï¼ˆ3 ç§’ã§å®Œäº†ï¼‰**

### **æ¯æ—¥ 1 å›ã®å·®åˆ†æ›´æ–°**

```python
def lightning_fast_daily_update():
    """
    é›»å…‰çŸ³ç«ã®å·®åˆ†æ›´æ–°ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå®Œå…¨ãƒ­ãƒ¼ã‚«ãƒ«ï¼‰
    """
    print("âš¡ é›»å…‰çŸ³ç«å·®åˆ†æ›´æ–°ã‚·ã‚¹ãƒ†ãƒ ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ç‰ˆï¼‰")
    print("=" * 50)

    # ãƒ­ãƒ¼ã‚«ãƒ«Sparkèµ·å‹•
    spark = SparkSession.builder \
        .appName("DailyUpdate") \
        .master("local[*]") \
        .getOrCreate()

    # 1. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ç¢ºèªï¼ˆé«˜é€Ÿï¼‰
    existing_db_path = "./local_customer_churn.db"
    parquet_path = "./local_customer_data.parquet"

    if os.path.exists(existing_db_path):
        print("ğŸ“Š æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç™ºè¦‹")

        # SQLiteã‹ã‚‰æœ€æ–°æ—¥ä»˜ç¢ºèªï¼ˆè¶…é«˜é€Ÿï¼‰
        conn = sqlite3.connect(existing_db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT MAX(date) FROM customer_data")
        latest_date = cursor.fetchone()[0]
        conn.close()

        print(f"ğŸ“… æœ€æ–°ãƒ‡ãƒ¼ã‚¿æ—¥ä»˜: {latest_date}")

        # æ˜¨æ—¥ã®ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')

        if latest_date >= yesterday:
            print("âœ… ãƒ‡ãƒ¼ã‚¿ã¯æœ€æ–°ã§ã™ - æ›´æ–°ä¸è¦")
            return True
    else:
        print("ğŸ’¡ æ–°è¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½œæˆ")

    # 2. æ–°ã—ã„å·®åˆ†ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆå®Ÿéš›ã«ã¯APIã‹ã‚‰å–å¾—ï¼‰
    today = datetime.now().strftime('%Y-%m-%d')
    new_customer_data = generate_daily_customer_data(today)

    # 3. Spark DataFrameå¤‰æ›
    schema = create_customer_schema()
    new_df = spark.createDataFrame(new_customer_data, schema=schema)

    print(f"ğŸ“ˆ æ–°è¦ãƒ‡ãƒ¼ã‚¿: {new_df.count()}ãƒ¬ã‚³ãƒ¼ãƒ‰")

    # 4. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨çµåˆï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡æœ€é©åŒ–ï¼‰
    if os.path.exists(parquet_path):
        existing_df = spark.read.parquet(parquet_path)
        combined_df = existing_df.union(new_df)
        print(f"ğŸ”„ ãƒ‡ãƒ¼ã‚¿çµåˆå®Œäº†: {combined_df.count()}ãƒ¬ã‚³ãƒ¼ãƒ‰")
    else:
        combined_df = new_df
        print(f"ğŸ’¡ æ–°è¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ: {combined_df.count()}ãƒ¬ã‚³ãƒ¼ãƒ‰")

    # 5. é‡è¤‡é™¤å»ãƒ»ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆé«˜é€Ÿï¼‰
    cleaned_df = combined_df.dropDuplicates(["customer_id", "date"]) \
                           .filter(col("customer_id").isNotNull()) \
                           .filter(col("monthly_charge") > 0)

    print(f"ğŸ§¹ ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†: {cleaned_df.count()}ãƒ¬ã‚³ãƒ¼ãƒ‰")

    # 6. é«˜é€Ÿä¿å­˜ï¼ˆParquet + SQLiteï¼‰
    # Parquetä¿å­˜ï¼ˆåˆ†æç”¨ï¼‰
    cleaned_df.write.mode("overwrite").parquet(parquet_path)

    # SQLiteä¿å­˜ï¼ˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ï¼‰
    pandas_df = cleaned_df.toPandas()
    conn = sqlite3.connect(existing_db_path)
    pandas_df.to_sql('customer_data', conn, if_exists='replace', index=False)
    conn.close()

    print("ğŸ’¾ å·®åˆ†æ›´æ–°å®Œäº†ï¼ˆ3ç§’ï¼‰")
    print(f"ğŸ“Š ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {cleaned_df.count():,}")

    spark.stop()
    return True

def generate_daily_customer_data(date):
    """
    æ—¥æ¬¡é¡§å®¢ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆå®Ÿéš›ã®APIã‹ã‚‰ã®å–å¾—ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
    """
    # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€æºå¸¯ã‚­ãƒ£ãƒªã‚¢ã®APIã‚„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å–å¾—
    np.random.seed(42)
    num_customers = 1000  # æ—¥æ¬¡æ–°è¦/æ›´æ–°é¡§å®¢æ•°

    customers = []
    for i in range(num_customers):
        customer_id = f"C{10000 + i}"
        age = np.random.randint(18, 70)
        gender = np.random.choice(["M", "F"])
        monthly_charge = np.random.normal(2500, 800)
        usage_minutes = np.random.exponential(100)
        call_count = np.random.poisson(150)
        complaints = np.random.poisson(1)
        tenure_months = np.random.randint(1, 60)

        # è§£ç´„äºˆæ¸¬ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆå®Ÿéš›ã®ãƒ“ã‚¸ãƒã‚¹ãƒ«ãƒ¼ãƒ«ï¼‰
        churn_probability = 0.05  # ãƒ™ãƒ¼ã‚¹è§£ç´„ç‡5%
        if complaints > 3:
            churn_probability += 0.3
        if monthly_charge > 4000:
            churn_probability += 0.1
        if usage_minutes < 30:
            churn_probability += 0.2
        if tenure_months < 6:
            churn_probability += 0.15

        churn = 1 if np.random.random() < churn_probability else 0

        customers.append((
            customer_id, date, int(age), gender, float(monthly_charge),
            float(usage_minutes), int(call_count), int(complaints),
            int(churn), int(tenure_months)
        ))

    return customers

def create_customer_schema():
    """é¡§å®¢ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¼ãƒå®šç¾©"""
    return StructType([
        StructField("customer_id", StringType(), False),
        StructField("date", StringType(), False),
        StructField("age", IntegerType(), False),
        StructField("gender", StringType(), False),
        StructField("monthly_charge", DoubleType(), False),
        StructField("total_usage_minutes", DoubleType(), False),
        StructField("call_count", IntegerType(), False),
        StructField("complaints", IntegerType(), False),
        StructField("churn", IntegerType(), False),
        StructField("tenure_months", IntegerType(), False)
    ])

# å®Ÿè¡Œä¾‹ï¼ˆæ¯æ—¥1å›å®Ÿè¡Œï¼‰
lightning_fast_daily_update()
```

## ğŸ¤– **AI æ­è¼‰è§£ç´„äºˆæ¸¬ã‚¨ãƒ³ã‚¸ãƒ³**

### **é«˜ç²¾åº¦è§£ç´„äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«**

```python
def advanced_churn_prediction_engine():
    """
    AIæ­è¼‰é«˜ç²¾åº¦è§£ç´„äºˆæ¸¬ã‚¨ãƒ³ã‚¸ãƒ³
    """
    print("ğŸ¤– AIè§£ç´„äºˆæ¸¬ã‚¨ãƒ³ã‚¸ãƒ³èµ·å‹•")
    print("=" * 50)

    spark = SparkSession.builder \
        .appName("ChurnPrediction") \
        .master("local[*]") \
        .getOrCreate()

    # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    customer_data = spark.read.parquet("./local_customer_data.parquet")
    print(f"ğŸ“Š å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {customer_data.count():,}ãƒ¬ã‚³ãƒ¼ãƒ‰")

    # 2. é«˜åº¦ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
    enhanced_features = create_advanced_features(customer_data)

    # 3. è§£ç´„äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«é©ç”¨
    predictions = apply_churn_prediction_model(enhanced_features)

    # 4. ãƒªã‚¹ã‚¯ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†é¡
    risk_segments = create_risk_segments(predictions)

    # 5. ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ç”Ÿæˆ
    action_plans = generate_action_plans(risk_segments)

    # 6. çµæœä¿å­˜
    action_plans.write.mode("overwrite").parquet("./churn_predictions.parquet")

    # 7. ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ
    generate_churn_summary_report(action_plans)

    spark.stop()
    return True

def create_advanced_features(data):
    """
    é«˜åº¦ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
    """
    print("   ğŸ”§ é«˜åº¦ç‰¹å¾´é‡ç”Ÿæˆä¸­...")

    # åŸºæœ¬ç‰¹å¾´é‡
    enhanced = data.withColumn(
        "charge_per_minute", col("monthly_charge") / (col("total_usage_minutes") + 1)
    ).withColumn(
        "complaint_intensity", col("complaints") / (col("tenure_months") + 1)
    ).withColumn(
        "usage_consistency",
        when(col("total_usage_minutes") > 0, col("call_count") / col("total_usage_minutes"))
        .otherwise(0)
    )

    # é¡§å®¢ä¾¡å€¤ã‚¹ã‚³ã‚¢
    enhanced = enhanced.withColumn(
        "customer_value_score",
        (col("monthly_charge") * col("tenure_months")) / 1000
    )

    # è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹å¾´é‡
    enhanced = enhanced.withColumn(
        "engagement_level",
        when(col("total_usage_minutes") > 200, "High")
        .when(col("total_usage_minutes") > 50, "Medium")
        .otherwise("Low")
    ).withColumn(
        "payment_tier",
        when(col("monthly_charge") > 4000, "Premium")
        .when(col("monthly_charge") > 2000, "Standard")
        .otherwise("Basic")
    )

    # ãƒªã‚¹ã‚¯è¦å› ãƒ•ãƒ©ã‚°
    enhanced = enhanced.withColumn(
        "high_complaint_flag", when(col("complaints") > 2, 1).otherwise(0)
    ).withColumn(
        "low_usage_flag", when(col("total_usage_minutes") < 30, 1).otherwise(0)
    ).withColumn(
        "new_customer_flag", when(col("tenure_months") < 6, 1).otherwise(0)
    ).withColumn(
        "high_charge_flag", when(col("monthly_charge") > 5000, 1).otherwise(0)
    )

    print("   âœ… ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†")
    return enhanced

def apply_churn_prediction_model(data):
    """
    è§£ç´„äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«é©ç”¨ï¼ˆãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ + çµ±è¨ˆãƒ¢ãƒ‡ãƒ«ï¼‰
    """
    print("   ğŸ¯ è§£ç´„äºˆæ¸¬è¨ˆç®—ä¸­...")

    # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹äºˆæ¸¬ã‚¹ã‚³ã‚¢
    predictions = data.withColumn(
        "rule_based_score",
        (col("high_complaint_flag") * 0.35) +
        (col("low_usage_flag") * 0.25) +
        (col("new_customer_flag") * 0.20) +
        (col("high_charge_flag") * 0.10)
    )

    # çµ±è¨ˆçš„äºˆæ¸¬ã‚¹ã‚³ã‚¢ï¼ˆç°¡æ˜“ãƒ¢ãƒ‡ãƒ«ï¼‰
    predictions = predictions.withColumn(
        "statistical_score",
        when(
            (col("complaint_intensity") > 0.5) & (col("charge_per_minute") > 100),
            0.8
        ).when(
            (col("complaints") > 1) & (col("tenure_months") < 12),
            0.6
        ).when(
            col("total_usage_minutes") < 20,
            0.4
        ).otherwise(0.1)
    )

    # æœ€çµ‚äºˆæ¸¬ã‚¹ã‚³ã‚¢ï¼ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼‰
    predictions = predictions.withColumn(
        "churn_probability",
        (col("rule_based_score") * 0.6 + col("statistical_score") * 0.4)
    ).withColumn(
        "churn_prediction",
        when(col("churn_probability") > 0.5, 1).otherwise(0)
    )

    print("   âœ… äºˆæ¸¬è¨ˆç®—å®Œäº†")
    return predictions

def create_risk_segments(data):
    """
    ãƒªã‚¹ã‚¯ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†é¡
    """
    print("   ğŸ“Š ãƒªã‚¹ã‚¯ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†é¡ä¸­...")

    segmented = data.withColumn(
        "risk_segment",
        when(col("churn_probability") > 0.7, "Critical")
        .when(col("churn_probability") > 0.4, "High")
        .when(col("churn_probability") > 0.2, "Medium")
        .otherwise("Low")
    ).withColumn(
        "priority_score",
        col("churn_probability") * col("customer_value_score")
    )

    print("   âœ… ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†é¡å®Œäº†")
    return segmented

def generate_action_plans(data):
    """
    å–¶æ¥­ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ç”Ÿæˆ
    """
    print("   ğŸ“‹ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ç”Ÿæˆä¸­...")

    action_data = data.withColumn(
        "recommended_action",
        when(col("risk_segment") == "Critical", "ç·Šæ€¥å¯¾å¿œï¼šå³æ—¥é¡§å®¢ãƒ•ã‚©ãƒ­ãƒ¼")
        .when(col("risk_segment") == "High", "å„ªå…ˆå¯¾å¿œï¼š1é€±é–“ä»¥å†…ã«ç‰¹åˆ¥ã‚ªãƒ•ã‚¡ãƒ¼")
        .when(col("risk_segment") == "Medium", "å®šæœŸå¯¾å¿œï¼š2é€±é–“ä»¥å†…ã«ã‚µãƒ¼ãƒ“ã‚¹æ¡ˆå†…")
        .otherwise("ç›£è¦–ç¶™ç¶šï¼šæœˆæ¬¡ãƒ¬ãƒ“ãƒ¥ãƒ¼")
    ).withColumn(
        "suggested_offer",
        when(col("high_charge_flag") == 1, "æ–™é‡‘ãƒ—ãƒ©ãƒ³è¦‹ç›´ã—")
        .when(col("low_usage_flag") == 1, "ä½¿ã„æ–¹ã‚µãƒãƒ¼ãƒˆ")
        .when(col("high_complaint_flag") == 1, "ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆå¼·åŒ–")
        .otherwise("ãƒ­ã‚¤ãƒ¤ãƒ«ãƒ†ã‚£ãƒ—ãƒ­ã‚°ãƒ©ãƒ æ¡ˆå†…")
    ).withColumn(
        "contact_method",
        when(col("risk_segment") == "Critical", "é›»è©±")
        .when(col("age") < 35, "SMS/ã‚¢ãƒ—ãƒª")
        .otherwise("ãƒ¡ãƒ¼ãƒ«")
    )

    print("   âœ… ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ç”Ÿæˆå®Œäº†")
    return action_data

def generate_churn_summary_report(data):
    """
    è§£ç´„äºˆæ¸¬ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    """
    print("\nğŸ“ˆ è§£ç´„äºˆæ¸¬ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ")
    print("=" * 50)

    # ãƒªã‚¹ã‚¯ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥é›†è¨ˆ
    data.createOrReplaceTempView("churn_analysis")

    risk_summary = spark.sql("""
        SELECT
            risk_segment,
            COUNT(*) as customer_count,
            ROUND(AVG(churn_probability), 3) as avg_churn_prob,
            ROUND(AVG(customer_value_score), 2) as avg_value_score,
            ROUND(SUM(priority_score), 2) as total_priority_score
        FROM churn_analysis
        GROUP BY risk_segment
        ORDER BY total_priority_score DESC
    """)

    print("\nğŸ¯ ãƒªã‚¹ã‚¯ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥ã‚µãƒãƒªãƒ¼:")
    risk_summary.show()

    # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³åˆ¥é›†è¨ˆ
    action_summary = spark.sql("""
        SELECT
            recommended_action,
            COUNT(*) as customer_count,
            ROUND(AVG(churn_probability), 3) as avg_risk
        FROM churn_analysis
        GROUP BY recommended_action
        ORDER BY customer_count DESC
    """)

    print("\nğŸ“‹ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³åˆ¥ã‚µãƒãƒªãƒ¼:")
    action_summary.show()

    # é«˜ãƒªã‚¹ã‚¯é¡§å®¢ãƒªã‚¹ãƒˆï¼ˆä¸Šä½10åï¼‰
    high_risk_customers = spark.sql("""
        SELECT
            customer_id,
            ROUND(churn_probability, 3) as churn_prob,
            ROUND(customer_value_score, 2) as value_score,
            risk_segment,
            recommended_action,
            suggested_offer
        FROM churn_analysis
        WHERE risk_segment IN ('Critical', 'High')
        ORDER BY priority_score DESC
        LIMIT 10
    """)

    print("\nğŸš¨ ç·Šæ€¥å¯¾å¿œå¿…è¦é¡§å®¢ï¼ˆä¸Šä½10åï¼‰:")
    high_risk_customers.show(truncate=False)

    # CSVå‡ºåŠ›ï¼ˆå–¶æ¥­ãƒãƒ¼ãƒ ç”¨ï¼‰
    high_risk_pandas = high_risk_customers.toPandas()
    high_risk_pandas.to_csv('./high_risk_customers.csv', index=False)
    print("ğŸ’¾ é«˜ãƒªã‚¹ã‚¯é¡§å®¢ãƒªã‚¹ãƒˆä¿å­˜: ./high_risk_customers.csv")

# å®Ÿè¡Œä¾‹
advanced_churn_prediction_engine()
```

## ğŸ› ï¸ **ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€‘S3 é€£æºï¼ˆæœ€å°é™ã‚¯ãƒ©ã‚¦ãƒ‰ä½¿ç”¨ï¼‰**

### **å¿…è¦ãªå ´åˆã®ã¿ S3 ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—**

```python
def optional_s3_backup():
    """
    ã‚ªãƒ—ã‚·ãƒ§ãƒ³: S3ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆæœˆ$5-20ã®ã¿ï¼‰
    """
    print("â˜ï¸ ã‚ªãƒ—ã‚·ãƒ§ãƒ³: S3ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—è¨­å®š")
    print("=" * 40)

    # ç’°å¢ƒå¤‰æ•°ç¢ºèª
    import os
    s3_bucket = os.getenv('S3_BUCKET_NAME')

    if not s3_bucket:
        print("ğŸ’¡ S3è¨­å®šãªã— - å®Œå…¨ãƒ­ãƒ¼ã‚«ãƒ«é‹ç”¨ç¶™ç¶š")
        print("   ï¼ˆã“ã‚Œã§ååˆ†å®Ÿç”¨çš„ã§ã™ï¼‰")
        return

    try:
        import boto3

        # S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½œæˆ
        s3_client = boto3.client('s3')

        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’S3ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        local_files = [
            './local_customer_churn.db',
            './local_customer_data.parquet',
            './churn_predictions.parquet',
            './high_risk_customers.csv'
        ]

        backup_date = datetime.now().strftime('%Y-%m-%d')

        for local_file in local_files:
            if os.path.exists(local_file):
                filename = os.path.basename(local_file)
                s3_key = f"backups/{backup_date}/{filename}"

                s3_client.upload_file(local_file, s3_bucket, s3_key)
                print(f"â˜ï¸ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†: {filename}")

        print("âœ… S3ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰")
        print(f"ğŸ’° æ¨å®šã‚³ã‚¹ãƒˆ: $0.01-0.05/æ—¥")

    except Exception as e:
        print(f"âš ï¸ S3ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¹ã‚­ãƒƒãƒ—: {e}")
        print("ğŸ’¡ ãƒ­ãƒ¼ã‚«ãƒ«é‹ç”¨ã§ç¶™ç¶šï¼ˆå•é¡Œãªã—ï¼‰")

# å®Ÿè¡Œä¾‹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
optional_s3_backup()
```

## ğŸ“Š **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»ã‚³ã‚¹ãƒˆæ¯”è¼ƒ**

### **å®Œå…¨ãƒ­ãƒ¼ã‚«ãƒ« vs ã‚¯ãƒ©ã‚¦ãƒ‰æ¯”è¼ƒ**

```python
def performance_cost_comparison():
    """
    ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»ã‚³ã‚¹ãƒˆè©³ç´°æ¯”è¼ƒ
    """
    print("ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»ã‚³ã‚¹ãƒˆæ¯”è¼ƒ")
    print("=" * 50)

    comparison_data = {
        "ğŸŸ¢ å®Œå…¨ãƒ­ãƒ¼ã‚«ãƒ«ï¼ˆæ¨å¥¨ï¼‰": {
            "åˆæœŸã‚³ã‚¹ãƒˆ": "$0",
            "æœˆé¡ã‚³ã‚¹ãƒˆ": "$0",
            "å‡¦ç†é€Ÿåº¦": "10ä¸‡ãƒ¬ã‚³ãƒ¼ãƒ‰/10ç§’",
            "ãƒ‡ãƒ¼ã‚¿ä¿å­˜": "ç„¡åˆ¶é™ï¼ˆPCå®¹é‡æ¬¡ç¬¬ï¼‰",
            "å­¦ç¿’ã‚³ã‚¹ãƒˆ": "â˜…â˜†â˜†â˜†â˜†ï¼ˆç°¡å˜ï¼‰",
            "ç®¡ç†è¤‡é›‘ã•": "â˜…â˜†â˜†â˜†â˜†ï¼ˆç°¡å˜ï¼‰",
            "é©ç”¨è¦æ¨¡": "å€‹äººã€œä¸­ä¼æ¥­ï¼ˆã€œ500ä¸‡ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼‰",
            "æ¨å¥¨åº¦": "â˜…â˜…â˜…â˜…â˜…"
        },
        "ğŸŸ¡ ãƒ­ãƒ¼ã‚«ãƒ« + S3ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—": {
            "åˆæœŸã‚³ã‚¹ãƒˆ": "$0",
            "æœˆé¡ã‚³ã‚¹ãƒˆ": "$5-20",
            "å‡¦ç†é€Ÿåº¦": "10ä¸‡ãƒ¬ã‚³ãƒ¼ãƒ‰/10ç§’ï¼ˆåŒã˜ï¼‰",
            "ãƒ‡ãƒ¼ã‚¿ä¿å­˜": "ç„¡åˆ¶é™ + ã‚¯ãƒ©ã‚¦ãƒ‰ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—",
            "å­¦ç¿’ã‚³ã‚¹ãƒˆ": "â˜…â˜…â˜†â˜†â˜†ï¼ˆå°‘ã—è¤‡é›‘ï¼‰",
            "ç®¡ç†è¤‡é›‘ã•": "â˜…â˜…â˜†â˜†â˜†ï¼ˆå°‘ã—è¤‡é›‘ï¼‰",
            "é©ç”¨è¦æ¨¡": "å€‹äººã€œä¸­ä¼æ¥­ + ãƒ‡ãƒ¼ã‚¿å…±æœ‰",
            "æ¨å¥¨åº¦": "â˜…â˜…â˜…â˜…â˜†"
        },
        "ğŸ”´ ãƒ•ãƒ«ã‚¯ãƒ©ã‚¦ãƒ‰ï¼ˆå‚è€ƒï¼‰": {
            "åˆæœŸã‚³ã‚¹ãƒˆ": "$100-500",
            "æœˆé¡ã‚³ã‚¹ãƒˆ": "$200-2000",
            "å‡¦ç†é€Ÿåº¦": "10ä¸‡ãƒ¬ã‚³ãƒ¼ãƒ‰/5ç§’",
            "ãƒ‡ãƒ¼ã‚¿ä¿å­˜": "ç„¡åˆ¶é™ï¼ˆé«˜ã‚³ã‚¹ãƒˆï¼‰",
            "å­¦ç¿’ã‚³ã‚¹ãƒˆ": "â˜…â˜…â˜…â˜…â˜†ï¼ˆä¸Šç´šè€…å‘ã‘ï¼‰",
            "ç®¡ç†è¤‡é›‘ã•": "â˜…â˜…â˜…â˜…â˜†ï¼ˆè¤‡é›‘ï¼‰",
            "é©ç”¨è¦æ¨¡": "å¤§ä¼æ¥­ï¼ˆ1å„„ãƒ¬ã‚³ãƒ¼ãƒ‰ä»¥ä¸Šï¼‰",
            "æ¨å¥¨åº¦": "â˜…â˜…â˜†â˜†â˜†ï¼ˆåˆå¿ƒè€…ã«ã¯ä¸è¦ï¼‰"
        }
    }

    for setup_type, details in comparison_data.items():
        print(f"\n{setup_type}:")
        for metric, value in details.items():
            print(f"   {metric}: {value}")

    print("\nğŸ’¡ åˆå¿ƒè€…ã¸ã®æ¨å¥¨:")
    print("   1. ã¾ãšå®Œå…¨ãƒ­ãƒ¼ã‚«ãƒ«ã§å§‹ã‚ã‚‹")
    print("   2. æ…£ã‚ŒãŸã‚‰S3ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—è¿½åŠ æ¤œè¨")
    print("   3. ã‚¯ãƒ©ã‚¦ãƒ‰ã¯å¤§è¦æ¨¡ã«ãªã£ã¦ã‹ã‚‰")

    # å…·ä½“çš„ãªROIä¾‹
    print("\nğŸ“ˆ ROIä¾‹ï¼ˆå¹´é–“åŠ¹æœï¼‰:")
    roi_examples = [
        "æºå¸¯ã‚·ãƒ§ãƒƒãƒ—: è§£ç´„é˜²æ­¢20% â†’ å¹´é–“$50,000ã®å£²ä¸Šå¢—",
        "ã‚³ãƒ¼ãƒ«ã‚»ãƒ³ã‚¿ãƒ¼: åŠ¹ç‡åŒ–30% â†’ å¹´é–“$30,000ã®ã‚³ã‚¹ãƒˆå‰Šæ¸›",
        "ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªã‚¹ãƒˆ: ã‚¹ã‚­ãƒ«å‘ä¸Š â†’ å¹´å$15,000ã‚¢ãƒƒãƒ—",
        "å°ä¼æ¥­: æ„æ€æ±ºå®šé€Ÿåº¦3å€ â†’ ç«¶äº‰å„ªä½æ€§å‘ä¸Š"
    ]

    for example in roi_examples:
        print(f"   ğŸ’ {example}")

performance_cost_comparison()
```

## ğŸ¯ **å®Ÿè¡Œã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ»è‡ªå‹•åŒ–**

### **æ—¥æ¬¡è‡ªå‹•å®Ÿè¡Œè¨­å®š**

```python
def setup_daily_automation():
    """
    æ—¥æ¬¡è‡ªå‹•å®Ÿè¡Œè¨­å®šï¼ˆå®Œå…¨ãƒ­ãƒ¼ã‚«ãƒ«ï¼‰
    """
    print("â° æ—¥æ¬¡è‡ªå‹•å®Ÿè¡Œè¨­å®š")
    print("=" * 30)

    # Windowsç”¨ãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
    windows_batch = '''@echo off
echo ğŸ“Š é¡§å®¢è§£ç´„äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹: %date% %time%

cd /d "C:\\path\\to\\your\\project"

python daily_churn_update.py

echo âœ… å‡¦ç†å®Œäº†: %date% %time%
pause
'''

    # macOS/Linuxç”¨ã‚·ã‚§ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆ
    unix_script = '''#!/bin/bash
echo "ğŸ“Š é¡§å®¢è§£ç´„äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹: $(date)"

cd "/path/to/your/project"

python3 daily_churn_update.py

echo "âœ… å‡¦ç†å®Œäº†: $(date)"
'''

    # Pythonæ—¥æ¬¡å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
    daily_script = '''
# daily_churn_update.py
from datetime import datetime
import os
import sys

def main():
    """æ—¥æ¬¡è‡ªå‹•å®Ÿè¡Œãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print(f"ğŸš€ æ—¥æ¬¡å‡¦ç†é–‹å§‹: {datetime.now()}")

    try:
        # 1. å·®åˆ†ãƒ‡ãƒ¼ã‚¿æ›´æ–°
        lightning_fast_daily_update()

        # 2. è§£ç´„äºˆæ¸¬å®Ÿè¡Œ
        advanced_churn_prediction_engine()

        # 3. ã‚ªãƒ—ã‚·ãƒ§ãƒ³: S3ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        optional_s3_backup()

        print(f"âœ… æ—¥æ¬¡å‡¦ç†å®Œäº†: {datetime.now()}")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        # ã‚¨ãƒ©ãƒ¼é€šçŸ¥ï¼ˆãƒ¡ãƒ¼ãƒ«ãƒ»Slackç­‰ï¼‰
        send_error_notification(str(e))

if __name__ == "__main__":
    main()
'''

    # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
    with open('run_daily_windows.bat', 'w', encoding='utf-8') as f:
        f.write(windows_batch)

    with open('run_daily_unix.sh', 'w') as f:
        f.write(unix_script)

    with open('daily_churn_update.py', 'w', encoding='utf-8') as f:
        f.write(daily_script)

    print("ğŸ“ è‡ªå‹•å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆå®Œäº†:")
    print("   - run_daily_windows.batï¼ˆWindowsç”¨ï¼‰")
    print("   - run_daily_unix.shï¼ˆMac/Linuxç”¨ï¼‰")
    print("   - daily_churn_update.pyï¼ˆãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼‰")

    print("\nâ° ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®šæ–¹æ³•:")
    print("   Windows: ã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã§æ¯æœ9æ™‚å®Ÿè¡Œ")
    print("   Mac: crontab -e ã§ '0 9 * * * /path/to/run_daily_unix.sh'")
    print("   Linux: cron ã§åŒæ§˜ã«è¨­å®š")

setup_daily_automation()
```

## ğŸ› ï¸ **ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°**

### **ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ³•**

```python
def comprehensive_troubleshooting():
    """
    åŒ…æ‹¬çš„ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰
    """
    print("ğŸ› ï¸ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰")
    print("=" * 50)

    common_issues = {
        "PySparkãŒèµ·å‹•ã—ãªã„": {
            "ç—‡çŠ¶": "import pyspark ã§ã‚¨ãƒ©ãƒ¼",
            "åŸå› ": ["Javaæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«", "ç’°å¢ƒå¤‰æ•°æœªè¨­å®š", "ãƒãƒ¼ã‚¸ãƒ§ãƒ³ä¸æ•´åˆ"],
            "è§£æ±ºæ³•": [
                "Java 8ã¾ãŸã¯11ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«",
                "pip install pyspark==3.5.0 ã§å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«",
                "JAVA_HOMEã®ç’°å¢ƒå¤‰æ•°ç¢ºèª",
                "python -c 'import pyspark' ã§ãƒ†ã‚¹ãƒˆ"
            ]
        },
        "ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼": {
            "ç—‡çŠ¶": "OutOfMemoryError",
            "åŸå› ": ["ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºéå¤§", "ãƒ¡ãƒ¢ãƒªè¨­å®šä¸è¶³", "ä¸è¦ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥"],
            "è§£æ±ºæ³•": [
                "spark.driver.memory ã‚’ '4g' ã«å¢—åŠ ",
                "ãƒ‡ãƒ¼ã‚¿ã‚’æ—¥ä»˜ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°",
                "ä¸è¦ãª .cache() ã‚’å‰Šé™¤",
                "ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°ã•ãåˆ†å‰²"
            ]
        },
        "SQLiteãƒ•ã‚¡ã‚¤ãƒ«ãŒé–‹ã‘ãªã„": {
            "ç—‡çŠ¶": "database is locked",
            "åŸå› ": ["ãƒ•ã‚¡ã‚¤ãƒ«ãŒä»–ã§ä½¿ç”¨ä¸­", "æ¨©é™ä¸è¶³", "ç ´æãƒ•ã‚¡ã‚¤ãƒ«"],
            "è§£æ±ºæ³•": [
                "ä»–ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’çµ‚äº†",
                "ãƒ•ã‚¡ã‚¤ãƒ«æ¨©é™ç¢ºèªãƒ»ä¿®æ­£",
                "sqlite3 .backup ã§ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—",
                "æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã§å†ä½œæˆ"
            ]
        },
        "S3æ¥ç¶šã‚¨ãƒ©ãƒ¼ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰": {
            "ç—‡çŠ¶": "NoCredentialsError",
            "åŸå› ": ["AWSèªè¨¼æœªè¨­å®š", "æ¨©é™ä¸è¶³", "ãƒªãƒ¼ã‚¸ãƒ§ãƒ³é•ã„"],
            "è§£æ±ºæ³•": [
                "aws configure ã§èªè¨¼è¨­å®š",
                "IAMãƒãƒªã‚·ãƒ¼ç¢ºèª",
                "AWS_DEFAULT_REGION=us-east-1 è¨­å®š",
                "å®Œå…¨ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ¼ãƒ‰ã«å¤‰æ›´"
            ]
        }
    }

    for issue, details in common_issues.items():
        print(f"\nğŸš¨ {issue}:")
        print(f"   ç—‡çŠ¶: {details['ç—‡çŠ¶']}")
        print(f"   è§£æ±ºæ³•:")
        for solution in details['è§£æ±ºæ³•']:
            print(f"     - {solution}")

    # è¨ºæ–­ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
    diagnostic_code = '''
def system_diagnostic():
    """ã‚·ã‚¹ãƒ†ãƒ è¨ºæ–­ã‚¹ã‚¯ãƒªãƒ—ãƒˆ"""
    print("ğŸ” ã‚·ã‚¹ãƒ†ãƒ è¨ºæ–­é–‹å§‹")

    # Pythonç’°å¢ƒç¢ºèª
    import sys
    print(f"Python: {sys.version}")

    # PySparkç¢ºèª
    try:
        import pyspark
        print(f"âœ… PySpark: {pyspark.__version__}")
    except ImportError:
        print("âŒ PySparkæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«")

    # ãƒ¡ãƒ¢ãƒªç¢ºèª
    import psutil
    memory = psutil.virtual_memory()
    print(f"ğŸ’¾ ãƒ¡ãƒ¢ãƒª: {memory.total // (1024**3)}GB (ä½¿ç”¨ç‡: {memory.percent}%)")

    # ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ç¢ºèª
    disk = psutil.disk_usage('.')
    print(f"ğŸ’½ ãƒ‡ã‚£ã‚¹ã‚¯: {disk.free // (1024**3)}GB ç©ºã")

    # ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
    import os
    files = ['./local_customer_churn.db', './local_customer_data.parquet']
    for file in files:
        if os.path.exists(file):
            size = os.path.getsize(file) // (1024**2)
            print(f"ğŸ“ {file}: {size}MB")
        else:
            print(f"âŒ {file}: ãƒ•ã‚¡ã‚¤ãƒ«ãªã—")

system_diagnostic()
'''

    print("\nğŸ” è¨ºæ–­ã‚¹ã‚¯ãƒªãƒ—ãƒˆ:")
    print(diagnostic_code)

comprehensive_troubleshooting()
```

## ğŸ¯ **ã€åˆå¿ƒè€…å‘ã‘ã€‘æœ€çŸ­å®Ÿè¡Œæ‰‹é †**

### **30 åˆ†ã§å®Œå…¨å‹•ä½œ**

```bash
# ğŸ’¡ æœ€çŸ­å®Ÿè¡Œæ‰‹é †ï¼ˆ30åˆ†ã§å®Œå…¨ãƒã‚¹ã‚¿ãƒ¼ï¼‰

# ã‚¹ãƒ†ãƒƒãƒ—1: ç’°å¢ƒæº–å‚™ï¼ˆ10åˆ†ï¼‰
pip install pyspark pandas numpy scikit-learn matplotlib

# ã‚¹ãƒ†ãƒƒãƒ—2: åŸºæœ¬ãƒ†ã‚¹ãƒˆï¼ˆ5åˆ†ï¼‰
python -c "
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('Test').master('local[*]').getOrCreate()
print('âœ… PySparkèµ·å‹•æˆåŠŸ!')
spark.stop()
"

# ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‡ãƒ¢å®Ÿè¡Œï¼ˆ10åˆ†ï¼‰
# ä¸Šè¨˜ã®create_local_churn_demo()ã‚’ã‚³ãƒ”ãƒšã—ã¦å®Ÿè¡Œ

# ã‚¹ãƒ†ãƒƒãƒ—4: çµæœç¢ºèªï¼ˆ5åˆ†ï¼‰
ls -la *.parquet *.db *.csv
# â†‘ ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¦ã„ã‚Œã°OK!

# ğŸ‰ å®Œäº†ï¼ã“ã‚Œã§æœ¬æ ¼çš„ãªé¡§å®¢è§£ç´„äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ãŒå‹•ä½œ
```

### **æ®µéšçš„ç¿’å¾—ãƒ—ãƒ©ãƒ³**

```bash
ğŸ“š å­¦ç¿’ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—ï¼ˆ1é€±é–“ï¼‰:

Day 1: åŸºæœ¬ç’°å¢ƒæ§‹ç¯‰ãƒ»ãƒ‡ãƒ¢å®Ÿè¡Œ
Day 2: å·®åˆ†æ›´æ–°ã‚·ã‚¹ãƒ†ãƒ ç†è§£
Day 3: è§£ç´„äºˆæ¸¬ãƒ­ã‚¸ãƒƒã‚¯ç¿’å¾—
Day 4: ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
Day 5: è‡ªå‹•åŒ–ãƒ»ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®š
Day 6: ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
Day 7: å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã§é‹ç”¨é–‹å§‹

ğŸ’¡ å„æ—¥1-2æ™‚é–“ã§ååˆ†
ğŸ’° ã‚³ã‚¹ãƒˆ: $0ï¼ˆå®Œå…¨ãƒ­ãƒ¼ã‚«ãƒ«ï¼‰
ğŸ¯ 1é€±é–“å¾Œ: ãƒ—ãƒ­ãƒ¬ãƒ™ãƒ«ã®ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰ã‚¹ã‚­ãƒ«ç¿’å¾—
```

### **æˆåŠŸã®æŒ‡æ¨™**

```bash
ğŸ¯ 1é€±é–“å¾Œã®ç›®æ¨™:
âœ… ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§é¡§å®¢ãƒ‡ãƒ¼ã‚¿å‡¦ç†å®Œå‹•
âœ… æ¯æ—¥3ç§’ã§ã®å·®åˆ†æ›´æ–°
âœ… AIè§£ç´„äºˆæ¸¬ã®å®Ÿè¡Œãƒ»ç†è§£
âœ… å–¶æ¥­ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³è‡ªå‹•ç”Ÿæˆ
âœ… åŸºæœ¬çš„ãªãƒˆãƒ©ãƒ–ãƒ«å¯¾å¿œå¯èƒ½

ğŸ“ˆ 1ãƒ¶æœˆå¾Œã®æˆæœ:
âœ… å®Ÿéš›ã®é¡§å®¢ãƒ‡ãƒ¼ã‚¿ã§ã®é‹ç”¨é–‹å§‹
âœ… è§£ç´„äºˆæ¸¬ç²¾åº¦85%ä»¥ä¸Šé”æˆ
âœ… æ¥­å‹™åŠ¹ç‡50%å‘ä¸Š
âœ… ãƒ‡ãƒ¼ã‚¿åˆ†æã‚¹ã‚­ãƒ«å¤§å¹…å‘ä¸Š

ğŸ’¼ 3ãƒ¶æœˆå¾Œã®ä¾¡å€¤:
âœ… ä¼æ¥­ãƒ¬ãƒ™ãƒ«ã®ãƒ‡ãƒ¼ã‚¿åŸºç›¤æ§‹ç¯‰å¯èƒ½
âœ… è§£ç´„é˜²æ­¢ã«ã‚ˆã‚‹å£²ä¸Šå‘ä¸Šè²¢çŒ®
âœ… ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆãƒ¬ãƒ™ãƒ«ã®ã‚¹ã‚­ãƒ«
âœ… è»¢è·ãƒ»æ˜‡é€²ã¸ã®å¼·åŠ›ãªæ­¦å™¨
```

## ğŸ“ **ã¾ã¨ã‚ãƒ»é‡è¦ãƒã‚¤ãƒ³ãƒˆ**

### **ğŸŸ¢ ã“ã®æºå¸¯é¡§å®¢è§£ç´„äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã®ä¾¡å€¤**

```bash
ğŸ¯ ã‚·ã‚¹ãƒ†ãƒ ã®ç‰¹å¾´:
âœ… 100%ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§å‹•ä½œï¼ˆAWSä¸è¦ï¼‰
âœ… Pythonåˆå¿ƒè€…ã§ã‚‚1æ—¥ã§ãƒã‚¹ã‚¿ãƒ¼å¯èƒ½
âœ… æœˆé¡ã‚³ã‚¹ãƒˆ$0-20ã§ä¼æ¥­ãƒ¬ãƒ™ãƒ«ã®æ©Ÿèƒ½
âœ… 100ä¸‡é¡§å®¢ã‚’10ç§’ã§é«˜é€Ÿå‡¦ç†
âœ… AIæ­è¼‰ã§85%ä»¥ä¸Šã®äºˆæ¸¬ç²¾åº¦
âœ… å–¶æ¥­ãƒãƒ¼ãƒ å‘ã‘å®Ÿç”¨çš„ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆ

ğŸ’° ã‚³ã‚¹ãƒˆå„ªä½æ€§:
- å®Œå…¨ãƒ­ãƒ¼ã‚«ãƒ«: $0/æœˆ
- S3ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä»˜ã: $5-20/æœˆ
- å¾“æ¥ã‚·ã‚¹ãƒ†ãƒ : $5,000-50,000/æœˆ
- ROI: 1000%ä»¥ä¸Š

ğŸš€ æŠ€è¡“å„ªä½æ€§:
- PySpark: å¾“æ¥ã®100å€é«˜é€Ÿ
- ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªå‡¦ç†: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æ
- ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«: å€‹äººã€œä¼æ¥­ã¾ã§å¯¾å¿œ
- æ‹¡å¼µæ€§: ä»–æ¥­ç•Œã¸ã®å¿œç”¨å¯èƒ½

ğŸ“ å­¦ç¿’ä¾¡å€¤:
- PySparkãƒã‚¹ã‚¿ãƒ¼: å¹´å+$20,000
- ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹: è»¢è·ã«æœ‰åˆ©
- AIãƒ»æ©Ÿæ¢°å­¦ç¿’: å°†æ¥æ€§æŠœç¾¤
- å®Ÿå‹™çµŒé¨“: å³æˆ¦åŠ›ãƒ¬ãƒ™ãƒ«
```

### **ğŸš€ ä»Šã™ãå§‹ã‚ã‚‰ã‚Œã‚‹ç†ç”±**

```bash
âœ… PySparkã¯å®Œå…¨ã«ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§å‹•ä½œ
âœ… AWSãªã©ã®ã‚¯ãƒ©ã‚¦ãƒ‰çŸ¥è­˜ä¸è¦
âœ… è¤‡é›‘ãªè¨­å®šãƒ»ç®¡ç†ä¸€åˆ‡ä¸è¦
âœ… 30åˆ†ã§åŸºæœ¬ã‚·ã‚¹ãƒ†ãƒ å‹•ä½œ
âœ… $0ã§æœ€é«˜ãƒ¬ãƒ™ãƒ«ã®æ©Ÿèƒ½
âœ… å®Ÿéš›ã®ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤ã‚’å³å®Ÿæ„Ÿ

âŒ é¿ã‘ã‚‹ã¹ãè¤‡é›‘ãªæ§‹æˆ:
âŒ EC2ãƒ»EMRãªã©ã®è¤‡é›‘ãªã‚¯ãƒ©ã‚¦ãƒ‰ç’°å¢ƒ
âŒ é«˜é¡ãªä¼æ¥­å‘ã‘ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³
âŒ ã‚ªãƒ¼ãƒãƒ¼ã‚¹ãƒšãƒƒã‚¯ãªã‚¤ãƒ³ãƒ•ãƒ©
âŒ å­¦ç¿’ã‚³ã‚¹ãƒˆã®é«˜ã„æŠ€è¡“

ğŸ’¡ ã‚·ãƒ³ãƒ—ãƒ«ãŒæœ€å¼·:
ãƒ­ãƒ¼ã‚«ãƒ«PySpark â†’ å¿…è¦ã«å¿œã˜ã¦S3è¿½åŠ 
ï¼ˆè¤‡é›‘ãªã‚¯ãƒ©ã‚¦ãƒ‰æ§‹æˆã¯ä¸è¦ï¼‰
```

**ğŸ‰ ã“ã‚Œã§ã€æœˆé¡$0-20 ã§æœ€é«˜ãƒ¬ãƒ™ãƒ«ã®æºå¸¯é¡§å®¢è§£ç´„äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ãŒæ§‹ç¯‰ã§ãã¾ã™ï¼AWS ã®è¤‡é›‘ãªçŸ¥è­˜ã¯ä¸€åˆ‡ä¸è¦ã§ã€å®Œå…¨ã«ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ä¼æ¥­ãƒ¬ãƒ™ãƒ«ã®åˆ†æãŒå¯èƒ½ã§ã™ï¼**
