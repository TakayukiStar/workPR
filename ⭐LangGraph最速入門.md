# LangGraphå®Œå…¨ãƒã‚¹ã‚¿ãƒ¼ã‚¬ã‚¤ãƒ‰ 2025

**åˆå¿ƒè€…ã‹ã‚‰å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã¾ã§ - ã“ã®ä¸€å†Šã§å®Œçµã™ã‚‹æ±ºå®šç‰ˆã‚¬ã‚¤ãƒ‰**

---

## ğŸ“– æœ¬æ›¸ã®ä½¿ã„æ–¹

### ã“ã®ã‚¬ã‚¤ãƒ‰ã§é”æˆã§ãã‚‹ã“ã¨

âœ… LangGraphã®åŸºç¤ã‹ã‚‰å®Ÿè·µã¾ã§å®Œå…¨ç¿’å¾—  
âœ… Google AI Studio (Gemini) ã¨ã®çµ±åˆã‚’ãƒã‚¹ã‚¿ãƒ¼  
âœ… Tavilyæ¤œç´¢APIã‚’æ´»ç”¨ã—ãŸå®Ÿç”¨çš„ãªAIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ§‹ç¯‰  
âœ… å®Ÿã‚µãƒ¼ãƒ“ã‚¹ã§ä½¿ãˆã‚‹æœ¬æ ¼çš„ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè£…  
âœ… ã‚¨ãƒ©ãƒ¼å‡¦ç†ã€ä¸¦åˆ—å®Ÿè¡Œã€è¤‡é›‘ãªåˆ†å²ã®å®Ÿè£…

### å¯¾è±¡èª­è€…

- LangGraphæœªçµŒé¨“è€…
- PythonåŸºç¤çŸ¥è­˜ãŒã‚ã‚‹æ–¹
- å®Ÿå‹™ã§AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æ§‹ç¯‰ã—ãŸã„æ–¹

### å­¦ç¿’ã®é€²ã‚æ–¹

1. **ç¬¬1ç« ã¯å¿…èª­** - å…¨ã¦ã®åŸºç¤ã¨ãªã‚Šã¾ã™
2. **ã‚³ãƒ¼ãƒ‰ã¯å¿…ãšå®Ÿè¡Œ** - èª­ã‚€ã ã‘ã§ãªãå‹•ã‹ã—ã¦ç†è§£
3. **APIã‚­ãƒ¼ã‚’äº‹å‰æº–å‚™** - å¾Œè¿°ã®æ‰‹é †ã§å–å¾—ã—ã¦ãã ã•ã„

---

## ğŸ¯ ç›®æ¬¡

### ç¬¬0ç« : ç’°å¢ƒæº–å‚™ã¨APIã‚­ãƒ¼å–å¾—
- [0-1. å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«](#0-1-å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«)
- [0-2. APIã‚­ãƒ¼ã®å–å¾—æ–¹æ³•](#0-2-apiã‚­ãƒ¼ã®å–å¾—æ–¹æ³•)
- [0-3. APIã‚­ãƒ¼ã®è¨­å®šæ–¹æ³•](#0-3-apiã‚­ãƒ¼ã®è¨­å®šæ–¹æ³•)

### ç¬¬1ç« : LangGraphã®åŸºæœ¬æ¦‚å¿µ
- [1-1. LangGraphã¨ã¯ä½•ã‹](#1-1-langgraphã¨ã¯ä½•ã‹)
- [1-2. å¿…é ˆè¦ç´  vs ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¦ç´ ](#1-2-å¿…é ˆè¦ç´ -vs-ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¦ç´ )
- [1-3. StateGraphã®ä»•çµ„ã¿](#1-3-stategraphã®ä»•çµ„ã¿)
- [1-4. START/ENDã®çœŸå®Ÿ](#1-4-startendã®çœŸå®Ÿ)

### ç¬¬2ç« : æœ€å°æ§‹æˆã®å®Ÿè£…
- [2-1. 17è¡Œã§å‹•ãæœ€å°ã‚³ãƒ¼ãƒ‰](#2-1-17è¡Œã§å‹•ãæœ€å°ã‚³ãƒ¼ãƒ‰)
- [2-2. å®Ÿè·µçš„ãªåŸºæœ¬å®Ÿè£…](#2-2-å®Ÿè·µçš„ãªåŸºæœ¬å®Ÿè£…)
- [2-3. ã‚°ãƒ©ãƒ•å¯è¦–åŒ–ã®å®Ÿè£…](#2-3-ã‚°ãƒ©ãƒ•å¯è¦–åŒ–ã®å®Ÿè£…)
- [2-4. ã‚ˆãã‚ã‚‹è³ªå•30é¸](#2-4-ã‚ˆãã‚ã‚‹è³ªå•30é¸)

### ç¬¬3ç« : æ¡ä»¶åˆ†å²ã¨ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
- [3-1. æ¡ä»¶åˆ†å²ã®åŸºæœ¬](#3-1-æ¡ä»¶åˆ†å²ã®åŸºæœ¬)
- [3-2. è¤‡æ•°ãƒ«ãƒ¼ãƒˆã®å®Ÿè£…](#3-2-è¤‡æ•°ãƒ«ãƒ¼ãƒˆã®å®Ÿè£…)
- [3-3. å‹•çš„ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°](#3-3-å‹•çš„ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°)
- [3-4. ç¬¬3ç« å®Œå…¨å®Ÿè£…ä¾‹](#3-4-ç¬¬3ç« å®Œå…¨å®Ÿè£…ä¾‹)

### ç¬¬4ç« : ãƒ„ãƒ¼ãƒ«çµ±åˆ - Tavilyæ¤œç´¢
- [4-1. Tavilyæ¤œç´¢ã®åŸºæœ¬](#4-1-tavilyæ¤œç´¢ã®åŸºæœ¬)
- [4-2. LLMã¨ãƒ„ãƒ¼ãƒ«ã®é€£æº](#4-2-llmã¨ãƒ„ãƒ¼ãƒ«ã®é€£æº)
- [4-3. å®Ÿç”¨çš„ãªæ¤œç´¢ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ](#4-3-å®Ÿç”¨çš„ãªæ¤œç´¢ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ)
- [4-4. ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Ÿè¡Œ](#4-4-ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Ÿè¡Œ)
- [4-5. è¤‡æ•°ãƒ„ãƒ¼ãƒ«ã®çµ±åˆ](#4-5-è¤‡æ•°ãƒ„ãƒ¼ãƒ«ã®çµ±åˆ)

### ç¬¬5ç« : ãƒ«ãƒ¼ãƒ—å‡¦ç†ã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
- [5-1. ãƒ«ãƒ¼ãƒ—å‡¦ç†ã®åŸºæœ¬](#5-1-ãƒ«ãƒ¼ãƒ—å‡¦ç†ã®åŸºæœ¬)
- [5-2. æœ€å¤§è©¦è¡Œå›æ•°ã®åˆ¶å¾¡](#5-2-æœ€å¤§è©¦è¡Œå›æ•°ã®åˆ¶å¾¡)
- [5-3. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æˆ¦ç•¥](#5-3-ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æˆ¦ç•¥)
- [5-4. ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ã®å®Ÿè£…](#5-4-ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ã®å®Ÿè£…)
- [5-5. ç¬¬5ç« å®Œå…¨å®Ÿè£…ä¾‹](#5-5-ç¬¬5ç« å®Œå…¨å®Ÿè£…ä¾‹)

### ç¬¬6ç« : ä¸¦åˆ—å‡¦ç†ã¨éåŒæœŸå®Ÿè¡Œ
- [6-1. ä¸¦åˆ—ãƒãƒ¼ãƒ‰ã®å®Ÿè£…](#6-1-ä¸¦åˆ—ãƒãƒ¼ãƒ‰ã®å®Ÿè£…)
- [6-2. éåŒæœŸå‡¦ç†ã®ãƒ‘ã‚¿ãƒ¼ãƒ³](#6-2-éåŒæœŸå‡¦ç†ã®ãƒ‘ã‚¿ãƒ¼ãƒ³)
- [6-3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–](#6-3-ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–)
- [6-4. ç¬¬6ç« å®Œå…¨å®Ÿè£…ä¾‹](#6-4-ç¬¬6ç« å®Œå…¨å®Ÿè£…ä¾‹)

### ç¬¬7ç« : å®Ÿè·µçš„ãªã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰
- [7-1. ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ ](#7-1-ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ )
- [7-2. è¤‡é›‘ãªåˆ†å²ãƒ•ãƒ­ãƒ¼](#7-2-è¤‡é›‘ãªåˆ†å²ãƒ•ãƒ­ãƒ¼)
- [7-3. æœ¬ç•ªç’°å¢ƒã¸ã®å±•é–‹](#7-3-æœ¬ç•ªç’°å¢ƒã¸ã®å±•é–‹)
- [7-4. ç¬¬7ç« å®Œå…¨å®Ÿè£…ä¾‹](#7-4-ç¬¬7ç« å®Œå…¨å®Ÿè£…ä¾‹)

### ä»˜éŒ²
- [A. ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°å®Œå…¨ç‰ˆ](#ä»˜éŒ²a-ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°å®Œå…¨ç‰ˆ)
- [B. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°](#ä»˜éŒ²b-ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°)
- [C. ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹é›†](#ä»˜éŒ²c-ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹é›†)
- [3-4. ç¬¬3ç« å®Œå…¨å®Ÿè£…ä¾‹](#3-4-ç¬¬3ç« å®Œå…¨å®Ÿè£…ä¾‹)

---

# ç¬¬0ç« : ç’°å¢ƒæº–å‚™ã¨APIã‚­ãƒ¼å–å¾—

## 0-1. å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

### ã™ã¹ã¦ä¸€æ‹¬ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆæ¨å¥¨ï¼‰

```bash
# åŸºæœ¬ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ + å¯è¦–åŒ– + æ¤œç´¢ + éåŒæœŸ
pip install langgraph langchain-google-genai langchain-community \
            tavily-python pillow aiohttp tenacity

# å¯è¦–åŒ–ç”¨ï¼ˆã©ã¡ã‚‰ã‹ä¸€æ–¹ã§OKï¼‰
pip install pygraphviz  # æ¨å¥¨
# ã¾ãŸã¯
pip install grandalf    # pygraphvizãŒå‹•ã‹ãªã„å ´åˆ
```

### å€‹åˆ¥ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰

```bash
# ã‚³ã‚¢æ©Ÿèƒ½ï¼ˆå¿…é ˆï¼‰
pip install langgraph
pip install langchain-google-genai
pip install langchain-community

# æ¤œç´¢æ©Ÿèƒ½ï¼ˆç¬¬4ç« ä»¥é™ï¼‰
pip install tavily-python

# å¯è¦–åŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€å¼·ãæ¨å¥¨ï¼‰
pip install pillow
pip install pygraphviz  # ã¾ãŸã¯ grandalf

# éåŒæœŸå‡¦ç†ï¼ˆç¬¬6ç« ä»¥é™ï¼‰
pip install aiohttp

# ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ï¼ˆç¬¬5ç« ä»¥é™ï¼‰
pip install tenacity
```

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª

```python
# ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ã‚¨ãƒ©ãƒ¼ãŒå‡ºãªã‘ã‚Œã°OK
import langgraph
import langchain_google_genai
import tavily
print("âœ… ã™ã¹ã¦ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒæ­£ã—ãã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã™")
```

---

## 0-2. APIã‚­ãƒ¼ã®å–å¾—æ–¹æ³•

### Google AI Studio APIã‚­ãƒ¼å–å¾—

**æ‰€è¦æ™‚é–“: ç´„2åˆ†**

1. **Google AI Studioã«ã‚¢ã‚¯ã‚»ã‚¹**
   - URL: https://aistudio.google.com/apikey
   - Googleã‚¢ã‚«ã‚¦ãƒ³ãƒˆã§ãƒ­ã‚°ã‚¤ãƒ³

2. **APIã‚­ãƒ¼ã‚’ä½œæˆ**
   - ã€ŒCreate API Keyã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
   - ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’é¸æŠï¼ˆã¾ãŸã¯æ–°è¦ä½œæˆï¼‰
   - APIã‚­ãƒ¼ãŒç”Ÿæˆã•ã‚Œã‚‹ï¼ˆ`AIza...` ã§å§‹ã¾ã‚‹æ–‡å­—åˆ—ï¼‰

3. **APIã‚­ãƒ¼ã‚’ã‚³ãƒ”ãƒ¼**
   - è¡¨ç¤ºã•ã‚ŒãŸAPIã‚­ãƒ¼å…¨ä½“ã‚’ã‚³ãƒ”ãƒ¼
   - **é‡è¦**: ã“ã®ã‚­ãƒ¼ã¯å†è¡¨ç¤ºã§ããªã„ãŸã‚ã€å®‰å…¨ãªå ´æ‰€ã«ä¿å­˜

**æ³¨æ„äº‹é …:**
- APIã‚­ãƒ¼ã¯ä»–äººã«è¦‹ã›ãªã„
- GitHubãªã©ã«å…¬é–‹ã—ãªã„
- ç’°å¢ƒå¤‰æ•°ã¾ãŸã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§ç®¡ç†

### Tavily Search APIã‚­ãƒ¼å–å¾—

**æ‰€è¦æ™‚é–“: ç´„3åˆ†**

1. **Tavilyã«ã‚¢ã‚«ã‚¦ãƒ³ãƒˆç™»éŒ²**
   - URL: https://tavily.com/
   - ã€ŒSign Upã€ã‹ã‚‰ã‚¢ã‚«ã‚¦ãƒ³ãƒˆä½œæˆ

2. **APIã‚­ãƒ¼ã‚’å–å¾—**
   - ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã«ãƒ­ã‚°ã‚¤ãƒ³
   - ã€ŒAPI Keysã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ç§»å‹•
   - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§1ã¤ã®APIã‚­ãƒ¼ãŒç™ºè¡Œæ¸ˆã¿

3. **ç„¡æ–™ãƒ—ãƒ©ãƒ³ã®åˆ¶é™ç¢ºèª**
   - æœˆé–“1,000ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¾ã§ç„¡æ–™
   - ãã‚Œä»¥ä¸Šã¯æœ‰æ–™ãƒ—ãƒ©ãƒ³

---

## 0-3. APIã‚­ãƒ¼ã®è¨­å®šæ–¹æ³•

### æ–¹æ³•1: ã‚³ãƒ¼ãƒ‰å†…ã§ç›´æ¥è¨­å®šï¼ˆå­¦ç¿’ç”¨ï¼‰

```python
import os

# Gemini APIã‚­ãƒ¼ï¼ˆç’°å¢ƒå¤‰æ•°åã¯è‡ªç”±ã§ã™ãŒã€çµ±ä¸€æ¨å¥¨ï¼‰
os.environ["GEMINI_API_KEY"] = "AIza..."  # ã“ã“ã«å®Ÿéš›ã®ã‚­ãƒ¼ã‚’è²¼ã‚Šä»˜ã‘

# Tavily APIã‚­ãƒ¼ï¼ˆç¬¬4ç« ä»¥é™ã§ä½¿ç”¨ï¼‰
os.environ["TAVILY_API_KEY"] = "tvly-..."  # ã“ã“ã«å®Ÿéš›ã®ã‚­ãƒ¼ã‚’è²¼ã‚Šä»˜ã‘
```

**âš ï¸ ã“ã®æ–¹æ³•ã®æ³¨æ„ç‚¹:**
- ã‚³ãƒ¼ãƒ‰ã‚’GitHubã«ãƒ—ãƒƒã‚·ãƒ¥ã—ãªã„ã“ã¨
- å­¦ç¿’ãƒ»ãƒ†ã‚¹ãƒˆç›®çš„ã®ã¿ã§ä½¿ç”¨
- æœ¬ç•ªç’°å¢ƒã§ã¯æ–¹æ³•2ã‚’æ¨å¥¨

### æ–¹æ³•2: .envãƒ•ã‚¡ã‚¤ãƒ«ã§ç®¡ç†ï¼ˆæ¨å¥¨ï¼‰

#### ã‚¹ãƒ†ãƒƒãƒ—1: python-dotenvã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
pip install python-dotenv
```

#### ã‚¹ãƒ†ãƒƒãƒ—2: .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ

ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã« `.env` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ:

```bash
# .env
GEMINI_API_KEY=AIza...
TAVILY_API_KEY=tvly-...
```

#### ã‚¹ãƒ†ãƒƒãƒ—3: .gitignoreã«è¿½åŠ 

```bash
# .gitignore
.env
```

#### ã‚¹ãƒ†ãƒƒãƒ—4: ã‚³ãƒ¼ãƒ‰ã§èª­ã¿è¾¼ã¿

```python
import os
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

# ç’°å¢ƒå¤‰æ•°ã¨ã—ã¦åˆ©ç”¨å¯èƒ½
gemini_api_key = os.environ["GEMINI_API_KEY"]
tavily_api_key = os.environ["TAVILY_API_KEY"]

# ç¢ºèª
if not gemini_api_key:
    raise ValueError("âŒ GEMINI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
print("âœ… APIã‚­ãƒ¼ã®èª­ã¿è¾¼ã¿æˆåŠŸ")
```

### APIã‚­ãƒ¼è¨­å®šã®ç¢ºèªã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```python
import os

def check_api_keys():
    """APIã‚­ãƒ¼ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª"""
    
    issues = []
    
    # Gemini APIã‚­ãƒ¼ã®ç¢ºèª
    gemini_key = os.environ.get("GEMINI_API_KEY")
    if not gemini_key:
        issues.append("âŒ GEMINI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    elif gemini_key == "your-gemini-api-key-here":
        issues.append("âŒ GEMINI_API_KEY ãŒãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã®ã¾ã¾ã§ã™")
    else:
        print(f"âœ… GEMINI_API_KEY: {gemini_key[:10]}...")
    
    # Tavily APIã‚­ãƒ¼ã®ç¢ºèª
    tavily_key = os.environ.get("TAVILY_API_KEY")
    if not tavily_key:
        print("â„¹ï¸ TAVILY_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆç¬¬4ç« ä»¥é™ã§å¿…è¦ï¼‰")
    elif tavily_key == "your-tavily-api-key-here":
        print("â„¹ï¸ TAVILY_API_KEY ãŒãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã®ã¾ã¾ã§ã™")
    else:
        print(f"âœ… TAVILY_API_KEY: {tavily_key[:10]}...")
    
    if issues:
        print("\n".join(issues))
        raise ValueError("APIã‚­ãƒ¼ã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„")
    
    print("\nâœ… ã™ã¹ã¦ã®APIã‚­ãƒ¼ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã¾ã™")

# å®Ÿè¡Œ
check_api_keys()
```

---

# ç¬¬1ç« : LangGraphã®åŸºæœ¬æ¦‚å¿µ

## 1-1. LangGraphã¨ã¯ä½•ã‹

### LangGraphã®æœ¬è³ª

LangGraphã¯**ã‚¹ãƒ†ãƒ¼ãƒˆãƒ•ãƒ«ãªAIãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**ã§ã™ã€‚

**å¾“æ¥ã®ãƒã‚§ãƒ¼ãƒ³å‹AIã¨ã®é•ã„:**

```python
# âŒ å¾“æ¥ã®ãƒã‚§ãƒ¼ãƒ³å‹ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ã ãŒæŸ”è»Ÿæ€§ã«æ¬ ã‘ã‚‹ï¼‰
chain = prompt | llm | output_parser
result = chain.invoke({"input": "è³ªå•"})

# âœ… LangGraphï¼ˆè¤‡é›‘ãªãƒ•ãƒ­ãƒ¼ã‚’è¡¨ç¾å¯èƒ½ï¼‰
graph = StateGraph(State)
graph.add_node("analyze", analyze_input)
graph.add_node("search", search_web)
graph.add_node("summarize", summarize_results)
graph.add_conditional_edges("analyze", route_decision)
# ... æŸ”è»Ÿãªåˆ¶å¾¡ãƒ•ãƒ­ãƒ¼
```

### LangGraphãŒå¾—æ„ãªã“ã¨

1. **è¤‡é›‘ãªæ¡ä»¶åˆ†å²**
   - ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã«å¿œã˜ã¦ç•°ãªã‚‹å‡¦ç†ãƒ«ãƒ¼ãƒˆã‚’é¸æŠ
   - LLMã®åˆ¤æ–­ã§æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ±ºå®š

2. **ãƒ«ãƒ¼ãƒ—å‡¦ç†**
   - æº€è¶³ã„ãçµæœãŒå¾—ã‚‰ã‚Œã‚‹ã¾ã§ç¹°ã‚Šè¿”ã—
   - ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆçš„ãªæŒ¯ã‚‹èˆã„

3. **ä¸¦åˆ—å®Ÿè¡Œ**
   - è¤‡æ•°ã®ã‚¿ã‚¹ã‚¯ã‚’åŒæ™‚å®Ÿè¡Œ
   - çµæœã‚’çµ±åˆ

4. **ã‚¹ãƒ†ãƒ¼ãƒˆç®¡ç†**
   - ãƒãƒ¼ãƒ‰é–“ã§ãƒ‡ãƒ¼ã‚¿ã‚’å…±æœ‰
   - å®Ÿè¡Œå±¥æ­´ã®è¿½è·¡

### å®Ÿéš›ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹

```
# ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒœãƒƒãƒˆ
User Input â†’ Intentåˆ†é¡ â†’ 
    â”œâ”€ FAQæ¤œç´¢ â†’ å›ç­”ç”Ÿæˆ
    â”œâ”€ ãƒã‚±ãƒƒãƒˆä½œæˆ â†’ æ‹…å½“è€…ã‚¢ã‚µã‚¤ãƒ³
    â””â”€ ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ â†’ äººé–“ã«è»¢é€

# ãƒªã‚µãƒ¼ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
Query â†’ Webæ¤œç´¢ â†’ çµæœè©•ä¾¡ â†’
    â””â”€ ä¸ååˆ† â†’ è¿½åŠ æ¤œç´¢ï¼ˆãƒ«ãƒ¼ãƒ—ï¼‰
    â””â”€ ååˆ† â†’ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

# ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
Topic â†’ ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ³ç”Ÿæˆ â†’
    â”œâ”€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³1åŸ·ç­†ï¼ˆä¸¦åˆ—ï¼‰
    â”œâ”€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³2åŸ·ç­†ï¼ˆä¸¦åˆ—ï¼‰
    â””â”€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³3åŸ·ç­†ï¼ˆä¸¦åˆ—ï¼‰
    â†’ çµ±åˆ â†’ æ ¡æ­£ â†’ å®Œæˆ
```

---

## 1-2. å¿…é ˆè¦ç´  vs ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¦ç´ 

### ğŸ”´ çµ¶å¯¾ã«å¿…è¦ãª5ã¤ã®è¦ç´ 

#### 1. StateGraph

```python
from langgraph.graph import StateGraph

workflow = StateGraph(State)  # ã“ã‚ŒãŒãªã„ã¨ä½•ã‚‚å§‹ã¾ã‚‰ãªã„
```

**å½¹å‰²**: ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®è¨­è¨ˆå›³ã‚’ä½œæˆ

#### 2. State (TypedDict)

```python
from typing import TypedDict

class State(TypedDict):
    input: str
    output: str
```

**å½¹å‰²**: ãƒãƒ¼ãƒ‰é–“ã§å…±æœ‰ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®å‹ã‚’å®šç¾©

**ãªãœå¿…é ˆ?**
- ãƒãƒ¼ãƒ‰é–“ã§ãƒ‡ãƒ¼ã‚¿ã‚’å—ã‘æ¸¡ã™ãŸã‚
- å‹å®‰å…¨æ€§ã‚’ç¢ºä¿ã™ã‚‹ãŸã‚

#### 3. add_node()

```python
workflow.add_node("process", process_function)
```

**å½¹å‰²**: å®Ÿè¡Œã™ã‚‹å‡¦ç†ï¼ˆãƒãƒ¼ãƒ‰ï¼‰ã‚’è¿½åŠ 

#### 4. add_edge()

```python
workflow.add_edge(START, "process")
workflow.add_edge("process", END)
```

**å½¹å‰²**: ãƒãƒ¼ãƒ‰åŒå£«ã‚’æ¥ç¶š

#### 5. compile()

```python
app = workflow.compile()
```

**å½¹å‰²**: å®Ÿè¡Œå¯èƒ½ãªå½¢å¼ã«å¤‰æ›

**ãªãœå¿…é ˆ?**
- `workflow`ã¯è¨­è¨ˆå›³ï¼ˆå®Ÿè¡Œä¸å¯ï¼‰
- `app`ã¯å®Ÿè¡Œå¯èƒ½ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³

---

### ğŸŸ¡ è‡ªå‹•ã§ç”¨æ„ã•ã‚Œã‚‹ã‚‚ã®ï¼ˆæš—é»™çš„ï¼‰

#### START / END

```python
from langgraph.graph import START, END

# ã“ã‚Œã‚‰ã¯è‡ªå‹•çš„ã«åˆ©ç”¨å¯èƒ½
workflow.add_edge(START, "first_node")
workflow.add_edge("last_node", END)
```

**çœŸå®Ÿ:**
- `START`ã¨`END`ã¯**å®šæ•°**ã¨ã—ã¦æä¾›ã•ã‚Œã‚‹
- å†…éƒ¨çš„ã«ã¯`__start__`ã¨`__end__`ã¨ã„ã†åå‰ã«å¤‰æ›ã•ã‚Œã‚‹
- æ›¸ã‹ãªãã¦ã‚‚ã‚°ãƒ©ãƒ•ã¯å‹•ããŒã€**99%ã®ã‚±ãƒ¼ã‚¹ã§ä½¿ã†**

#### __start__ / __end__

```python
# âŒ ã‚³ãƒ¼ãƒ‰ã§æ›¸ã‹ãªã„
workflow.add_edge("__start__", "node")

# âœ… ä»£ã‚ã‚Šã«STARTã‚’ä½¿ã†
workflow.add_edge(START, "node")
```

**çœŸå®Ÿ:**
- ã“ã‚Œã‚‰ã¯**å†…éƒ¨å**
- ã‚°ãƒ©ãƒ•å¯è¦–åŒ–æ™‚ã«è¡¨ç¤ºã•ã‚Œã‚‹
- **ç›´æ¥ä½¿ã†ã“ã¨ã¯çµ¶å¯¾ã«ãªã„**

---

### ğŸŸ¢ ã‚ã‚‹ã¨ä¾¿åˆ©ãªã‚‚ã®ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

#### set_entry_point() / set_finish_point()

```python
# å¤ã„æ›¸ãæ–¹ï¼ˆéæ¨å¥¨ï¼‰
workflow.set_entry_point("start_node")
workflow.set_finish_point("end_node")

# âœ… æ¨å¥¨ã•ã‚Œã‚‹æ›¸ãæ–¹
workflow.add_edge(START, "start_node")
workflow.add_edge("end_node", END)
```

**ãªãœéæ¨å¥¨?**
- `add_edge(START, ...)`ã®æ–¹ãŒä¸€è²«æ€§ãŒã‚ã‚‹
- å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã‚‚`START`/`END`ã‚’æ¨å¥¨

#### ã‚°ãƒ©ãƒ•å¯è¦–åŒ–

```python
# ãªãã¦ã‚‚å‹•ããŒã€ã‚ã‚‹ã¨è¶…ä¾¿åˆ©
png_data = app.get_graph().draw_mermaid_png()
```

**æ¨å¥¨åº¦**: â­â­â­â­â­ï¼ˆè¤‡é›‘ãªã‚°ãƒ©ãƒ•ã§ã¯å¿…é ˆãƒ¬ãƒ™ãƒ«ï¼‰

---

## 1-3. StateGraphã®ä»•çµ„ã¿

### Stateã®å½¹å‰²

**Stateã¯ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å…¨ä½“ã§å…±æœ‰ã•ã‚Œã‚‹ã€Œãƒ¡ãƒ¢ãƒªã€**

```python
from typing import TypedDict

class State(TypedDict):
    input: str       # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
    output: str      # æœ€çµ‚çµæœ
    intermediate: str  # ä¸­é–“çµæœ
    count: int       # å‡¦ç†å›æ•°
```

### ãƒãƒ¼ãƒ‰é–¢æ•°ã®ãƒ«ãƒ¼ãƒ«

#### çµ¶å¯¾ã«å®ˆã‚‹ã¹ã2ã¤ã®ãƒ«ãƒ¼ãƒ«

**ãƒ«ãƒ¼ãƒ«1: å¼•æ•°ã¯å¿…ãš `state`**

```python
# âœ… æ­£ã—ã„
def my_node(state: State) -> dict:
    user_input = state["input"]
    return {"output": "å‡¦ç†æ¸ˆã¿"}

# âŒ é–“é•ã„ - å¼•æ•°åãŒé•ã†
def my_node(data: State) -> dict:
    return {"output": "å‡¦ç†æ¸ˆã¿"}

# âŒ é–“é•ã„ - å‹ãƒ’ãƒ³ãƒˆãªã—ï¼ˆå‹•ããŒéæ¨å¥¨ï¼‰
def my_node(state):
    return {"output": "å‡¦ç†æ¸ˆã¿"}
```

**ãƒ«ãƒ¼ãƒ«2: æˆ»ã‚Šå€¤ã¯å¿…ãš `dict`**

```python
# âœ… æ­£ã—ã„
def my_node(state: State) -> dict:
    return {"output": "çµæœ"}

# âŒ é–“é•ã„ - strã‚’è¿”ã™
def my_node(state: State) -> str:
    return "çµæœ"

# âŒ é–“é•ã„ - Stateã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™
def my_node(state: State) -> State:
    return state
```

### Stateã®æ›´æ–°ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 

**é‡è¦: æˆ»ã‚Šå€¤ã®`dict`ã¯è‡ªå‹•çš„ã«ãƒãƒ¼ã‚¸ã•ã‚Œã‚‹**

```python
from typing import TypedDict

class State(TypedDict):
    input: str
    output: str
    count: int

def increment_node(state: State) -> dict:
    # ç¾åœ¨ã® state
    # {"input": "test", "output": "", "count": 0}
    
    # countã ã‘æ›´æ–°ã—ã¦è¿”ã™
    return {"count": state["count"] + 1}
    
    # è‡ªå‹•ãƒãƒ¼ã‚¸å¾Œã® state
    # {"input": "test", "output": "", "count": 1}
    # â†‘ inputã¨outputã¯ãã®ã¾ã¾æ®‹ã‚‹ï¼
```

**ãƒãƒ¼ã‚¸ã®ãƒ«ãƒ¼ãƒ«:**

```python
# å®Ÿè¡Œå‰
state = {"input": "Hello", "output": "", "count": 0}

# ãƒãƒ¼ãƒ‰ãŒè¿”ã™
return {"output": "Processed", "count": 1}

# è‡ªå‹•ãƒãƒ¼ã‚¸å¾Œ
state = {"input": "Hello", "output": "Processed", "count": 1}
#        â†‘ ãã®ã¾ã¾    â†‘ æ›´æ–°          â†‘ æ›´æ–°
```

### è¤‡æ•°ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®æ›´æ–°

```python
def process_node(state: State) -> dict:
    # è¤‡æ•°ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’åŒæ™‚æ›´æ–°
    return {
        "output": "å®Œäº†",
        "count": state["count"] + 1,
        "intermediate": "ä¸­é–“ãƒ‡ãƒ¼ã‚¿"
    }
```

### ã‚ˆãã‚ã‚‹ç–‘å•: å…¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿”ã™å¿…è¦ã¯?

**A: ã‚ã‚Šã¾ã›ã‚“ï¼æ›´æ–°ã—ãŸã„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã ã‘è¿”ã›ã°OK**

```python
# âœ… ã“ã‚Œã§OKï¼ˆoutputã ã‘æ›´æ–°ï¼‰
def node1(state: State) -> dict:
    return {"output": "çµæœ"}

# âœ… ã“ã‚Œã‚‚OKï¼ˆcountã ã‘æ›´æ–°ï¼‰
def node2(state: State) -> dict:
    return {"count": state["count"] + 1}

# âŒ ã“ã‚Œã¯å†—é•·ï¼ˆä¸è¦ï¼‰
def node3(state: State) -> dict:
    return {
        "input": state["input"],    # æ›´æ–°ã—ã¦ãªã„ã®ã«è¿”ã—ã¦ã„ã‚‹
        "output": "çµæœ",
        "count": state["count"]     # æ›´æ–°ã—ã¦ãªã„ã®ã«è¿”ã—ã¦ã„ã‚‹
    }
```

---

## 1-4. START/ENDã®çœŸå®Ÿ

### ã‚ˆãã‚ã‚‹èª¤è§£ã‚’è§£æ¶ˆ

#### èª¤è§£1: "STARTã¨ENDã¯æ›¸ã‹ãªã„ã¨ã„ã‘ãªã„"

**çœŸå®Ÿ: æ›¸ã‹ãªãã¦ã‚‚å‹•ãã€ã§ã‚‚99%æ›¸ã**

```python
# âŒ ç†è«–ä¸Šã¯å‹•ããŒã€å®Ÿè·µã§ã¯ä½¿ã‚ãªã„
workflow = StateGraph(State)
workflow.add_node("node1", func1)
workflow.add_node("node2", func2)
workflow.add_edge("node1", "node2")
app = workflow.compile()

# âœ… å®Ÿè·µã§ã¯ã“ã†æ›¸ã
workflow = StateGraph(State)
workflow.add_node("node1", func1)
workflow.add_node("node2", func2)
workflow.add_edge(START, "node1")  # æ˜ç¤ºçš„ãªé–‹å§‹ç‚¹
workflow.add_edge("node2", END)     # æ˜ç¤ºçš„ãªçµ‚äº†ç‚¹
app = workflow.compile()
```

#### èª¤è§£2: "__start__ã‚’ä½¿ã†ã¹ã"

**çœŸå®Ÿ: çµ¶å¯¾ã«ä½¿ã‚ãªã„**

```python
# âŒ çµ¶å¯¾ã«ã“ã†æ›¸ã‹ãªã„
workflow.add_edge("__start__", "node1")

# âœ… å¿…ãšã“ã†æ›¸ã
workflow.add_edge(START, "node1")
```

**ãªãœ?**

| é …ç›® | `START` | `__start__` |
|------|---------|-------------|
| ç”¨é€” | ã‚³ãƒ¼ãƒ‰ã§ä½¿ã† | ã‚°ãƒ©ãƒ•è¡¨ç¤ºã®ã¿ |
| å…¬é–‹API | âœ… ã¯ã„ | âŒ ã„ã„ãˆ |
| æ¨å¥¨ | âœ… æ¨å¥¨ | âŒ éæ¨å¥¨ |

#### èª¤è§£3: "STARTã¨ENDã¯ç‰¹åˆ¥ãªãƒãƒ¼ãƒ‰"

**çœŸå®Ÿ: ãŸã ã®ãƒãƒ¼ã‚«ãƒ¼ï¼ˆå®šæ•°ï¼‰**

```python
from langgraph.graph import START, END

# å®Ÿéš›ã®å®Ÿè£…ï¼ˆç°¡ç•¥ç‰ˆï¼‰
START = "__start__"  # å˜ãªã‚‹æ–‡å­—åˆ—å®šæ•°
END = "__end__"      # å˜ãªã‚‹æ–‡å­—åˆ—å®šæ•°

# ã ã‹ã‚‰ã“ã†ä½¿ãˆã‚‹
workflow.add_edge(START, "first")  # "__start__" â†’ "first"
workflow.add_edge("last", END)     # "last" â†’ "__end__"
```

### ã‚°ãƒ©ãƒ•å¯è¦–åŒ–ã§ã®è¡¨ç¤º

**ã‚³ãƒ¼ãƒ‰ã¨ã‚°ãƒ©ãƒ•è¡¨ç¤ºã®å¯¾å¿œè¡¨**

| ã‚³ãƒ¼ãƒ‰ã§æ›¸ã | ã‚°ãƒ©ãƒ•ã§è¡¨ç¤º |
|------------|-------------|
| `START` | `__start__` |
| `"my_node"` | `my_node` |
| `"å‡¦ç†1"` | `å‡¦ç†1` |
| `END` | `__end__` |

**ä¾‹:**

```python
workflow.add_edge(START, "analyze")
workflow.add_edge("analyze", "process")
workflow.add_edge("process", END)
```

**ã‚°ãƒ©ãƒ•è¡¨ç¤º:**
```
__start__ â†’ analyze â†’ process â†’ __end__
```

### å®Ÿè·µçš„ãªä½¿ã„æ–¹

```python
import os
from langgraph.graph import StateGraph, START, END
from typing import TypedDict

# APIã‚­ãƒ¼è¨­å®š
os.environ["GEMINI_API_KEY"] = "your-api-key"

class State(TypedDict):
    data: str

def node_a(state: State) -> dict:
    return {"data": state["data"] + " â†’ A"}

def node_b(state: State) -> dict:
    return {"data": state["data"] + " â†’ B"}

# âœ… æ¨å¥¨ãƒ‘ã‚¿ãƒ¼ãƒ³
workflow = StateGraph(State)
workflow.add_node("a", node_a)
workflow.add_node("b", node_b)

# æ˜ç¤ºçš„ãªé–‹å§‹ã¨çµ‚äº†
workflow.add_edge(START, "a")
workflow.add_edge("a", "b")
workflow.add_edge("b", END)

app = workflow.compile()

result = app.invoke({"data": "é–‹å§‹"})
print(result["data"])  # "é–‹å§‹ â†’ A â†’ B"
```

---

# ç¬¬2ç« : æœ€å°æ§‹æˆã®å®Ÿè£…

## 2-1. 17è¡Œã§å‹•ãæœ€å°ã‚³ãƒ¼ãƒ‰

### çµ¶å¯¾æœ€å°é™ã®ã‚³ãƒ¼ãƒ‰

```python
import os
from typing import TypedDict
from langgraph.graph import StateGraph, START, END
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage

os.environ["GEMINI_API_KEY"] = "your-actual-api-key-here"

class State(TypedDict):
    output: str

llm = ChatGoogleGenerativeAI(
    google_api_key=os.environ["GEMINI_API_KEY"],
    model="gemini-2.0-flash-exp"
)

workflow = StateGraph(State)
workflow.add_node("llm", lambda s: {"output": llm.invoke([HumanMessage("ã“ã‚“ã«ã¡ã¯")]).content})
workflow.add_edge(START, "llm")
workflow.add_edge("llm", END)
app = workflow.compile()

print(app.invoke({"output": ""})["output"])
```

**ã“ã‚ŒãŒå‹•ãæœ€å°å˜ä½ã§ã™ã€‚ä»¥é™ã¯ã“ã‚Œã‚’èª­ã¿ã‚„ã™ãå±•é–‹ã—ã¦ã„ãã¾ã™ã€‚**

---

## 2-2. å®Ÿè·µçš„ãªåŸºæœ¬å®Ÿè£…

### ã‚¹ãƒ†ãƒƒãƒ—1: ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

```python
import os
from typing import TypedDict
from langgraph.graph import StateGraph, START, END
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage
```

**å„ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®å½¹å‰²:**

| ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« | å¿…é ˆ? | å½¹å‰² |
|-----------|------|------|
| `os` | ğŸ”´å¿…é ˆ | ç’°å¢ƒå¤‰æ•°ã®è¨­å®šãƒ»å–å¾— |
| `TypedDict` | ğŸ”´ | Stateã®å‹å®šç¾© |
| `StateGraph` | ğŸ”´ | ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ä½œæˆ |
| `START, END` | ğŸŸ¡æ¨å¥¨ | é–‹å§‹ãƒ»çµ‚äº†ãƒãƒ¼ã‚«ãƒ¼ |
| `ChatGoogleGenerativeAI` | ğŸ”´ | Gemini LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ |
| `HumanMessage` | ğŸ”´ | LLMã¸ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼ |

### ã‚¹ãƒ†ãƒƒãƒ—2: APIã‚­ãƒ¼è¨­å®š

```python
import os

# APIã‚­ãƒ¼è¨­å®šï¼ˆå¿…é ˆï¼‰
os.environ["GEMINI_API_KEY"] = "AIza..."

# æ¤œè¨¼
if not os.environ.get("GEMINI_API_KEY"):
    raise ValueError("âŒ APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

print(f"âœ… APIã‚­ãƒ¼è¨­å®šOK: {os.environ['GEMINI_API_KEY'][:10]}...")
```

### ã‚¹ãƒ†ãƒƒãƒ—3: Stateå®šç¾©

```python
class State(TypedDict):
    input: str   # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
    output: str  # LLMå‡ºåŠ›
```

**Stateã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹:**

```python
# âœ… æ˜ç¢ºãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å
class State(TypedDict):
    user_query: str
    llm_response: str
    search_results: str
    final_answer: str

# âŒ æ›–æ˜§ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å
class State(TypedDict):
    data: str
    result: str
    info: str
```

### ã‚¹ãƒ†ãƒƒãƒ—4: LLMåˆæœŸåŒ–

```python
llm = ChatGoogleGenerativeAI(
    google_api_key=os.environ["GEMINI_API_KEY"],  # APIã‚­ãƒ¼æ˜ç¤º
    model="gemini-2.0-flash-exp",                 # ãƒ¢ãƒ‡ãƒ«æŒ‡å®š
    temperature=0.7,                              # å‰µé€ æ€§ï¼ˆ0ã€œ1ï¼‰
    max_tokens=1024                               # æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
)
```

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è§£èª¬:**

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | èª¬æ˜ | æ¨å¥¨å€¤ |
|-----------|----------|------|-------|
| `google_api_key` | ç’°å¢ƒå¤‰æ•° | APIã‚­ãƒ¼ | æ˜ç¤ºæ¨å¥¨ |
| `model` | - | ä½¿ç”¨ãƒ¢ãƒ‡ãƒ« | `gemini-2.0-flash-exp` |
| `temperature` | 1.0 | ãƒ©ãƒ³ãƒ€ãƒ æ€§ | 0.7ï¼ˆãƒãƒ©ãƒ³ã‚¹å‹ï¼‰ |
| `max_tokens` | - | å‡ºåŠ›ä¸Šé™ | 1024ã€œ2048 |

**temperatureã®é¸ã³æ–¹:**

```python
# äº‹å®Ÿãƒ™ãƒ¼ã‚¹ã®å›ç­”
llm_factual = ChatGoogleGenerativeAI(
    google_api_key=os.environ["GEMINI_API_KEY"],
    model="gemini-2.0-flash-exp",
    temperature=0.1  # ã»ã¼æ±ºå®šçš„
)

# ãƒãƒ©ãƒ³ã‚¹å‹
llm_balanced = ChatGoogleGenerativeAI(
    google_api_key=os.environ["GEMINI_API_KEY"],
    model="gemini-2.0-flash-exp",
    temperature=0.7
)

# å‰µé€ çš„ãªå‡ºåŠ›
llm_creative = ChatGoogleGenerativeAI(
    google_api_key=os.environ["GEMINI_API_KEY"],
    model="gemini-2.0-flash-exp",
    temperature=0.9
)
```

### ã‚¹ãƒ†ãƒƒãƒ—5: ãƒãƒ¼ãƒ‰é–¢æ•°å®šç¾©

```python
def call_gemini(state: State) -> dict:
    """
    Gemini APIã‚’å‘¼ã³å‡ºã™ãƒãƒ¼ãƒ‰
    
    Args:
        state: ç¾åœ¨ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çŠ¶æ…‹
        
    Returns:
        dict: æ›´æ–°ã™ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
    """
    # 1. å…¥åŠ›å–å¾—
    user_input = state["input"]
    
    # 2. LLMå‘¼ã³å‡ºã—
    response = llm.invoke([HumanMessage(content=user_input)])
    
    # 3. çµæœã‚’è¿”ã™
    return {"output": response.content}
```

**HumanMessageã®ä½¿ã„æ–¹:**

```python
# åŸºæœ¬å½¢
message = HumanMessage(content="ã“ã‚“ã«ã¡ã¯")

# è¤‡æ•°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
from langchain_core.messages import SystemMessage

messages = [
    SystemMessage(content="ã‚ãªãŸã¯è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™"),
    HumanMessage(content="LangGraphã«ã¤ã„ã¦æ•™ãˆã¦")
]
response = llm.invoke(messages)
```

### ã‚¹ãƒ†ãƒƒãƒ—6: ã‚°ãƒ©ãƒ•æ§‹ç¯‰

```python
# 1. ã‚°ãƒ©ãƒ•ä½œæˆ
workflow = StateGraph(State)

# 2. ãƒãƒ¼ãƒ‰è¿½åŠ 
workflow.add_node("gemini", call_gemini)

# 3. ã‚¨ãƒƒã‚¸è¿½åŠ ï¼ˆãƒ•ãƒ­ãƒ¼å®šç¾©ï¼‰
workflow.add_edge(START, "gemini")  # é–‹å§‹ â†’ gemini
workflow.add_edge("gemini", END)     # gemini â†’ çµ‚äº†

# 4. ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
app = workflow.compile()
```

**add_nodeã®æŸ”è»Ÿæ€§:**

```python
# ãƒ‘ã‚¿ãƒ¼ãƒ³1: é–¢æ•°ã‚’æ¸¡ã™
def my_func(state: State) -> dict:
    return {"output": "çµæœ"}
workflow.add_node("node1", my_func)

# ãƒ‘ã‚¿ãƒ¼ãƒ³2: ãƒ©ãƒ ãƒ€å¼
workflow.add_node("node2", lambda s: {"output": "çµæœ"})

# ãƒ‘ã‚¿ãƒ¼ãƒ³3: ã‚¯ãƒ©ã‚¹ãƒ¡ã‚½ãƒƒãƒ‰
class MyProcessor:
    def process(self, state: State) -> dict:
        return {"output": "çµæœ"}

processor = MyProcessor()
workflow.add_node("node3", processor.process)
```

### ã‚¹ãƒ†ãƒƒãƒ—7: å®Ÿè¡Œ

```python
# æœ€å°å®Ÿè¡Œ
result = app.invoke({"input": "ã“ã‚“ã«ã¡ã¯", "output": ""})
print(result["output"])

# å®Ÿè·µçš„ãªå®Ÿè¡Œé–¢æ•°
def run_workflow(app, user_input: str, verbose: bool = True):
    """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œ"""
    if verbose:
        print(f"ğŸ“ å…¥åŠ›: {user_input}")
    
    result = app.invoke({"input": user_input, "output": ""})
    
    if verbose:
        print(f"âœ… å‡ºåŠ›: {result['output']}")
    
    return result

# ä½¿ç”¨ä¾‹
run_workflow(app, "LangGraphã®ç‰¹å¾´ã‚’æ•™ãˆã¦ãã ã•ã„")
```

---

## 2-3. ã‚°ãƒ©ãƒ•å¯è¦–åŒ–ã®å®Ÿè£…

### å¯è¦–åŒ–é–¢æ•°ï¼ˆæ±ç”¨ç‰ˆï¼‰

```python
def visualize_graph(app, filename: str = "workflow.png", show_ascii: bool = False):
    """
    ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚°ãƒ©ãƒ•ã‚’å¯è¦–åŒ–
    
    Args:
        app: ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ã‚¢ãƒ—ãƒª
        filename: ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«å
        show_ascii: PNGå¤±æ•—æ™‚ã«ASCIIç‰ˆã‚’è¡¨ç¤º
    """
    print(f"\nğŸ“Š ã‚°ãƒ©ãƒ•ã‚’å¯è¦–åŒ–ä¸­...")
    
    try:
        # PNGç”Ÿæˆ
        png_data = app.get_graph().draw_mermaid_png()
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        with open(filename, "wb") as f:
            f.write(png_data)
        
        print(f"âœ… '{filename}' ã«ä¿å­˜ã—ã¾ã—ãŸ")
        
    except Exception as e:
        print(f"âš ï¸ PNGä¿å­˜å¤±æ•—: {e}")
        
        if show_ascii:
            print("\nğŸ“ ASCIIç‰ˆã‚°ãƒ©ãƒ•:")
            print(app.get_graph().draw_ascii())
```

### ä½¿ç”¨ä¾‹

```python
# åŸºæœ¬ä½¿ç”¨
visualize_graph(app)

# ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚¡ã‚¤ãƒ«å
visualize_graph(app, filename="my_workflow.png")

# ASCIIç‰ˆã‚‚è¡¨ç¤º
visualize_graph(app, show_ascii=True)
```

### ã‚°ãƒ©ãƒ•æ§‹é€ ã®ç†è§£

```python
import os
from langgraph.graph import StateGraph, START, END
from typing import TypedDict

os.environ["GEMINI_API_KEY"] = "your-api-key"

class State(TypedDict):
    data: str

workflow = StateGraph(State)
workflow.add_node("process", lambda s: {"data": "å‡¦ç†æ¸ˆã¿"})
workflow.add_edge(START, "process")
workflow.add_edge("process", END)
app = workflow.compile()
```

**ç”Ÿæˆã•ã‚Œã‚‹ã‚°ãƒ©ãƒ•:**

```
__start__ â†’ process â†’ __end__
```

**é‡è¦ãƒã‚¤ãƒ³ãƒˆ:**
- `START` â†’ ã‚°ãƒ©ãƒ•ã§ã¯ `__start__`
- `"process"` â†’ ã‚°ãƒ©ãƒ•ã§ã‚‚ `process`
- `END` â†’ ã‚°ãƒ©ãƒ•ã§ã¯ `__end__`

---

# 2-4. ã‚ˆãã‚ã‚‹è³ªå•41é¸ã€LangGraphå®Œå…¨ç‰ˆã€‘

---

### åŸºæœ¬æ¦‚å¿µï¼ˆQ1-Q10ï¼‰

#### Q1: LangGraphã¨LangChainã®é•ã„ã¯ï¼Ÿ

**ç°¡æ½”ãªå›ç­”**:
- **LangChain**: ç·šå½¢ã®å‡¦ç†ãƒã‚§ãƒ¼ãƒ³ï¼ˆA â†’ B â†’ Cï¼‰
- **LangGraph**: ã‚°ãƒ©ãƒ•æ§‹é€ ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ï¼ˆæ¡ä»¶åˆ†å²ã€ãƒ«ãƒ¼ãƒ—ã€ä¸¦åˆ—å®Ÿè¡ŒãŒå¯èƒ½ï¼‰

**è©³ç´°**:

LangChainã¯ç·šå½¢å‡¦ç†ã«æœ€é©ã§ã™ãŒã€è¤‡é›‘ãªåˆ¶å¾¡ãƒ•ãƒ­ãƒ¼ã«ã¯å‘ã„ã¦ã„ã¾ã›ã‚“ã€‚

```python
# âŒ LangChainã§ã¯æ¡ä»¶åˆ†å²ãŒå›°é›£
chain = prompt | llm | output_parser
result = chain.invoke({"input": "è³ªå•"})
# å¸¸ã«åŒã˜ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œ

# âœ… LangGraphãªã‚‰æ¡ä»¶åˆ†å²ãŒç°¡å˜
workflow.add_conditional_edges("classify", router, {
    "route_a": "handler_a",
    "route_b": "handler_b"
})
```

**ã„ã¤LangGraphã‚’é¸ã¶ã¹ãã‹**:
- âœ… ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã§å‡¦ç†ã‚’å¤‰ãˆã‚‹
- âœ… æº€è¶³ã™ã‚‹ã¾ã§ãƒ«ãƒ¼ãƒ—ã—ãŸã„
- âœ… è¤‡æ•°ã‚¿ã‚¹ã‚¯ã‚’ä¸¦åˆ—å®Ÿè¡Œã—ãŸã„
- âœ… è¤‡é›‘ãªçŠ¶æ…‹ç®¡ç†ãŒå¿…è¦

---

#### Q2: StateGraphã¯å¿…é ˆï¼Ÿ

**å›ç­”**: **ã¯ã„ã€çµ¶å¯¾å¿…é ˆã§ã™ã€‚**

```python
# âŒ ã‚¨ãƒ©ãƒ¼ï¼ˆStateGraphãªã—ï¼‰
workflow.add_node("process", func)  # NameError

# âœ… å¿…ãšStateGraphã‹ã‚‰å§‹ã‚ã‚‹
from langgraph.graph import StateGraph

workflow = StateGraph(State)
workflow.add_node("process", func)
app = workflow.compile()
```

**StateGraphã®å½¹å‰²**:
1. ãƒãƒ¼ãƒ‰ã¨ã‚¨ãƒƒã‚¸ã®ç®¡ç†
2. å®Ÿè¡Œé †åºã®æ±ºå®š
3. Stateæ›´æ–°ã®è‡ªå‹•å‡¦ç†
4. ã‚°ãƒ©ãƒ•ã®å¯è¦–åŒ–

---

#### Q3: TypedDictã¯å¿…é ˆï¼Ÿ

**å›ç­”**: æŠ€è¡“çš„ã«ã¯ä¸è¦ã§ã™ãŒã€**99.9%ä½¿ã†ã¹ã**ã§ã™ã€‚

**TypedDictã‚’ä½¿ã‚ãªã„å ´åˆã®å•é¡Œ**:
- å‹å®‰å…¨æ€§ãŒãªã„
- IDEã®è£œå®ŒãŒåŠ¹ã‹ãªã„
- ãƒã‚°ã®æ—©æœŸç™ºè¦‹ãŒå›°é›£

**æ¨å¥¨ï¼šTypedDictä½¿ç”¨**:
```python
from typing import TypedDict

class State(TypedDict):
    user_input: str
    llm_response: str
    confidence: float

def my_node(state: State) -> dict:
    user_text = state["user_input"]  # âœ… IDEãŒè£œå®Œ
    return {"llm_response": "å›ç­”"}
```

---

#### Q4: Stateã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ•°ã«åˆ¶é™ã¯ï¼Ÿ

**å›ç­”**: **åˆ¶é™ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚** 1å€‹ã§ã‚‚100å€‹ã§ã‚‚OKã§ã™ã€‚

```python
# âœ… ãƒŸãƒ‹ãƒãƒ 
class MinimalState(TypedDict):
    input: str
    output: str

# âœ… è¤‡é›‘
class ComplexState(TypedDict):
    user_id: str
    query: str
    intent: str
    search_results: list
    final_response: str
    # ... å¥½ããªã ã‘è¿½åŠ å¯èƒ½
```

**ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹**: é–¢é€£ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–

---

#### Q5: STARTã¨ENDã¯çœç•¥ã§ãã‚‹ï¼Ÿ

**å›ç­”**: æŠ€è¡“çš„ã«ã¯å¯èƒ½ã§ã™ãŒã€**å®Ÿç”¨ä¸Šã¯å¿…ãšä½¿ã„ã¾ã™ã€‚**

```python
# âœ… æ˜ç¢ºã§å®Ÿç”¨çš„
from langgraph.graph import START, END

workflow.add_edge(START, "node1")  # é–‹å§‹ç‚¹ãŒæ˜ç¢º
workflow.add_edge("node2", END)    # çµ‚äº†ç‚¹ãŒæ˜ç¢º
```

**ä½¿ã†ç†ç”±**:
- é–‹å§‹ç‚¹ãƒ»çµ‚äº†ç‚¹ãŒæ˜ç¢º
- å¯èª­æ€§ãŒé«˜ã„
- ãƒ‡ãƒãƒƒã‚°ã—ã‚„ã™ã„

---

#### Q6: __start__ã¨__end__ã‚’ç›´æ¥ä½¿ãˆã‚‹ï¼Ÿ

**å›ç­”**: æŠ€è¡“çš„ã«ã¯å¯èƒ½ã§ã™ãŒã€**çµ¶å¯¾ã«ä½¿ã‚ãªã„ã§ãã ã•ã„ã€‚**

```python
# âŒ å‹•ããŒéæ¨å¥¨
workflow.add_edge("__start__", "node1")

# âœ… å¿…ãšã“ã†æ›¸ã
from langgraph.graph import START, END
workflow.add_edge(START, "node1")
```

**ç†ç”±**: å¯èª­æ€§ã€å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ã®ä¸€è²«æ€§ã€å°†æ¥ã®äº’æ›æ€§

---

#### Q7: ãƒãƒ¼ãƒ‰åã¨ã—ã¦__start__ã‚„__end__ã¯ä½¿ãˆã‚‹ï¼Ÿ

**å›ç­”**: **ä½¿ãˆã¾ã›ã‚“ï¼ˆäºˆç´„èªï¼‰**ã€‚

```python
# âŒ ã‚¨ãƒ©ãƒ¼
workflow.add_node("__start__", func)  # ValueError

# âœ… OK
workflow.add_node("start_process", func)
workflow.add_node("initialize", func)
```

---

#### Q8: ãƒãƒ¼ãƒ‰åã¯æ—¥æœ¬èªã§ã‚‚OKï¼Ÿ

**å›ç­”**: **å®Œå…¨ã«OKã§ã™ï¼**

```python
# âœ… æ—¥æœ¬èªãƒãƒ¼ãƒ‰å
workflow.add_node("è³ªå•å—ä»˜", receive)
workflow.add_node("LLMå‡¦ç†", process)
workflow.add_node("çµæœæ•´å½¢", format)
```

**æ¨å¥¨**: è‹±èª + ã‚³ãƒ¡ãƒ³ãƒˆãŒå›½éš›çš„
```python
workflow.add_node("classify_intent", classify)  # æ„å›³åˆ†é¡
```

---

#### Q9: ãƒãƒ¼ãƒ‰ã®è¿½åŠ é †åºã¯é‡è¦ï¼Ÿ

**å›ç­”**: **ã„ã„ãˆã€ç„¡é–¢ä¿‚ã§ã™ã€‚** å®Ÿè¡Œé †åºã¯ã‚¨ãƒƒã‚¸ã§æ±ºã¾ã‚Šã¾ã™ã€‚

```python
# âœ… ã“ã‚Œã‚‰ã¯å…¨ã¦åŒã˜å‹•ä½œ
# ãƒ‘ã‚¿ãƒ¼ãƒ³1: é †ç•ªé€šã‚Š
workflow.add_node("step1", func1)
workflow.add_node("step2", func2)

# ãƒ‘ã‚¿ãƒ¼ãƒ³2: é€†é †
workflow.add_node("step2", func2)
workflow.add_node("step1", func1)

# å®Ÿè¡Œé †åºã¯ã‚¨ãƒƒã‚¸ã§æ±ºã¾ã‚‹
workflow.add_edge(START, "step1")
workflow.add_edge("step1", "step2")
```

---

#### Q10: ã‚¨ãƒƒã‚¸ã®è¿½åŠ é †åºã¯ï¼Ÿ

**å›ç­”**: **ç„¡é–¢ä¿‚ã§ã™ã€‚** ã‚°ãƒ©ãƒ•ã®æ¥ç¶šé–¢ä¿‚ã®ã¿ãŒé‡è¦ã€‚

---

### ãƒãƒ¼ãƒ‰é–¢æ•°ï¼ˆQ11-Q20ï¼‰

#### Q11: ãƒãƒ¼ãƒ‰é–¢æ•°ã®å¼•æ•°åã¯stateå›ºå®šï¼Ÿ

**å›ç­”**: **ã„ã„ãˆã€ä»»æ„ã®åå‰ã§OK**ï¼ˆæ…£ä¾‹ã¯`state`ï¼‰ã€‚

```python
# âœ… ã™ã¹ã¦æœ‰åŠ¹
def node1(state: State) -> dict: ...
def node2(s: State) -> dict: ...
def node3(data: State) -> dict: ...
```

**é‡è¦**: å‹ãƒ’ãƒ³ãƒˆï¼ˆ`: State`ï¼‰ã¯å¿…é ˆ

---

#### Q12: æˆ»ã‚Šå€¤ã¯å¿…ãšdictï¼Ÿ

**å›ç­”**: **ã¯ã„ã€å¿…ãšdict**ã§ã™ã€‚

```python
# âœ… OK
def node(state: State) -> dict:
    return {"output": "çµæœ"}

# âŒ NG
def bad_node(state: State) -> str:
    return "çµæœ"  # ã‚¨ãƒ©ãƒ¼
```

---

#### Q13: ç©ºã®dictã‚’è¿”ã—ã¦ã‚‚ã„ã„ï¼Ÿ

**å›ç­”**: **ã¯ã„ã€OKã§ã™ã€‚** Stateã‚’æ›´æ–°ã—ãªã„å ´åˆã«ä½¿ã„ã¾ã™ã€‚

```python
def log_only(state: State) -> dict:
    print(f"ãƒ­ã‚°: {state}")
    return {}  # ä½•ã‚‚æ›´æ–°ã—ãªã„
```

---

#### Q14: ã™ã¹ã¦ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿”ã™å¿…è¦ã¯ï¼Ÿ

**å›ç­”**: **ã„ã„ãˆã€æ›´æ–°åˆ†ã ã‘**è¿”ã›ã°OKã§ã™ã€‚

```python
class State(TypedDict):
    a: str
    b: str
    c: str

# âœ… è‰¯ã„ä¾‹: aã ã‘æ›´æ–°
def node(state: State) -> dict:
    return {"a": "æ–°å€¤"}
# bãƒ»cã¯ãã®ã¾ã¾ä¿æŒã•ã‚Œã‚‹
```

---

#### Q15: lambdaå¼ã¨defé–¢æ•°ã®ä½¿ã„åˆ†ã‘ã¯ï¼Ÿ

**å›ç­”**: **1è¡Œãªã‚‰lambdaã€ãã‚Œä»¥å¤–ã¯defé–¢æ•°**ã€‚

```python
# âœ… lambdaå‘ã
workflow.add_node("inc", lambda s: {"count": s["count"] + 1})

# âœ… defå‘ã
def process(state: State) -> dict:
    result = complex_logic(state["input"])
    return {"output": result}
```

---

#### Q16: lambdaå¼ã§è¤‡æ•°è¡Œã¯ã§ãã‚‹ï¼Ÿ

**å›ç­”**: æŠ€è¡“çš„ã«ã¯å¯èƒ½ã§ã™ãŒã€**çµ¶å¯¾ã«æ¨å¥¨ã—ã¾ã›ã‚“ã€‚**

å¯èª­æ€§ãŒæœ€æ‚ªã«ãªã‚‹ãŸã‚ã€è¤‡æ•°è¡Œãªã‚‰å¿…ãšdefé–¢æ•°ã‚’ä½¿ã„ã¾ã—ã‚‡ã†ã€‚

---

#### Q17: ãƒãƒ¼ãƒ‰é–¢æ•°å†…ã§ã‚¨ãƒ©ãƒ¼ãŒèµ·ããŸã‚‰ï¼Ÿ

**å›ç­”**: **ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å…¨ä½“ãŒåœæ­¢**ã—ã¾ã™ã€‚

**å¯¾ç­–**: try-exceptã§ã‚­ãƒ£ãƒƒãƒ
```python
def safe_node(state: State) -> dict:
    try:
        result = risky_operation(state["input"])
        return {"output": result, "error": None}
    except Exception as e:
        return {"output": None, "error": str(e)}
```

---

#### Q18: ãƒãƒ¼ãƒ‰é–¢æ•°ã§printãƒ‡ãƒãƒƒã‚°ã¯ã§ãã‚‹ï¼Ÿ

**å›ç­”**: **ã¯ã„ã€å®Œå…¨ã«ã§ãã¾ã™ã€‚**

```python
def debug_node(state: State) -> dict:
    print(f"å…¥åŠ›: {state['input']}")
    result = process(state["input"])
    print(f"å‡ºåŠ›: {result}")
    return {"output": result}
```

---

#### Q19: ãƒãƒ¼ãƒ‰é–¢æ•°ã¯éåŒæœŸï¼ˆasyncï¼‰ã«ã§ãã‚‹ï¼Ÿ

**å›ç­”**: **ã¯ã„ã€ã§ãã¾ã™ã€‚**

```python
async def async_node(state: State) -> dict:
    result = await async_api_call(state["input"])
    return {"output": result}

# éåŒæœŸå®Ÿè¡Œ
result = await app.ainvoke({"input": "test"})
```

---

#### Q20: åŒã˜ãƒãƒ¼ãƒ‰ã‚’è¤‡æ•°å›è¿½åŠ ã§ãã‚‹ï¼Ÿ

**å›ç­”**: **ã„ã„ãˆã€ã‚¨ãƒ©ãƒ¼**ã«ãªã‚Šã¾ã™ã€‚ãƒãƒ¼ãƒ‰åã¯ä¸€æ„ã€‚

```python
# âŒ ã‚¨ãƒ©ãƒ¼
workflow.add_node("process", func)
workflow.add_node("process", func)  # ValueError

# âœ… ç•°ãªã‚‹åå‰ã§
workflow.add_node("process_1", func)
workflow.add_node("process_2", func)
```

---

### å®Ÿè¡Œã¨ãƒ‡ãƒãƒƒã‚°ï¼ˆQ21-Q30ï¼‰

#### Q21: invoke()ã®å¼•æ•°ã¯ï¼Ÿ

**å›ç­”**: **Stateã®åˆæœŸå€¤ã‚’dictã§æ¸¡ã—ã¾ã™ã€‚**

```python
# âœ… æœ€å°é™
result = app.invoke({"input": "è³ªå•"})

# âœ… è©³ç´°
result = app.invoke({
    "input": "è³ªå•",
    "output": "",
    "count": 0
})
```

---

#### Q22: å®Ÿè¡Œçµæœã¯ä½•ãŒè¿”ã‚‹ï¼Ÿ

**å›ç­”**: **æœ€çµ‚çš„ãªStateå…¨ä½“ãŒdict**ã¨ã—ã¦è¿”ã‚Šã¾ã™ã€‚

```python
result = app.invoke({"input": "ãƒ†ã‚¹ãƒˆ"})

print(type(result))  # 
print(result)
# {
#     "input": "ãƒ†ã‚¹ãƒˆ",
#     "output": "å‡¦ç†æ¸ˆã¿: ãƒ†ã‚¹ãƒˆ",
#     "count": 1
# }
```

---

#### Q23: é€”ä¸­çµŒéã‚’å–å¾—ã§ãã‚‹ï¼Ÿ

**å›ç­”**: **ã¯ã„ã€stream()ã§å¯èƒ½**ã§ã™ã€‚

```python
for chunk in app.stream({"input": "ãƒ†ã‚¹ãƒˆ"}):
    print(chunk)
# å„ãƒãƒ¼ãƒ‰ã®å®Ÿè¡Œã”ã¨ã«å‡ºåŠ›
```

---

#### Q24: å®Ÿè¡Œæ™‚é–“ã‚’è¨ˆæ¸¬ã—ãŸã„

**å›ç­”**: `time`ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§è¨ˆæ¸¬ã§ãã¾ã™ã€‚

```python
import time

start = time.time()
result = app.invoke({"input": "è³ªå•"})
print(f"å®Ÿè¡Œæ™‚é–“: {time.time() - start:.2f}ç§’")
```

---

#### Q25: ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¯ï¼Ÿ

**å›ç­”**: **try-exceptã§ãƒ©ãƒƒãƒ—**ã—ã¾ã™ã€‚

```python
try:
    result = app.invoke({"input": "ãƒ‡ãƒ¼ã‚¿"})
    print(f"æˆåŠŸ: {result['output']}")
except Exception as e:
    print(f"ã‚¨ãƒ©ãƒ¼: {e}")
```

---

#### Q26: ã‚°ãƒ©ãƒ•ã®å®Ÿè¡Œé †åºã‚’ç¢ºèªã—ãŸã„

**å›ç­”**: **stream()ã§ç¢ºèª**ã§ãã¾ã™ã€‚

```python
for i, chunk in enumerate(app.stream({"input": "ãƒ‡ãƒ¼ã‚¿"}), 1):
    node_name = list(chunk.keys())[0]
    print(f"{i}. {node_name}")
```

---

#### Q27: ç‰¹å®šã®ãƒãƒ¼ãƒ‰ã ã‘å®Ÿè¡Œã§ãã‚‹ï¼Ÿ

**å›ç­”**: **ã§ãã¾ã›ã‚“ã€‚** å¸¸ã«STARTã‹ã‚‰ENDã¾ã§å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚

**ä»£æ›¿æ¡ˆ**: æ¡ä»¶åˆ†å²ã§ã‚¹ã‚­ãƒƒãƒ—ã€ã¾ãŸã¯æœ€å°é™ã®ã‚°ãƒ©ãƒ•ã‚’ä½œã‚‹

---

#### Q28: å®Ÿè¡Œã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã§ãã‚‹ï¼Ÿ

**å›ç­”**: **å›°é›£**ã§ã™ã€‚ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šã‚„æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°åˆ¶é™ã§å¯¾å¿œã€‚

```python
# æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’åˆ¶é™
result = app.invoke(
    {"input": "ãƒ‡ãƒ¼ã‚¿"},
    config={"recursion_limit": 10}
)
```

---

#### Q29: ä¸¦åˆ—å®Ÿè¡Œã¯å¯èƒ½ï¼Ÿ

**å›ç­”**: **ã¯ã„ã€å¯èƒ½ã§ã™ï¼**

```python
# ä¸¦åˆ—é–‹å§‹
workflow.add_edge(START, "task1")
workflow.add_edge(START, "task2")
workflow.add_edge(START, "task3")

# çµåˆ
workflow.add_edge("task1", "merge")
workflow.add_edge("task2", "merge")
workflow.add_edge("task3", "merge")
```

---

#### Q30: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Ÿè¡Œã¨ã¯ï¼Ÿ

**å›ç­”**: **çµæœã‚’é€æ¬¡å–å¾—**ã™ã‚‹å®Ÿè¡Œæ–¹æ³•ã§ã™ã€‚

```python
# invoke: å…¨ã¦å®Œäº†å¾Œã«è¿”ã™
result = app.invoke({"input": "è³ªå•"})

# stream: å„ãƒãƒ¼ãƒ‰ã”ã¨ã«è¿”ã™
for chunk in app.stream({"input": "è³ªå•"}):
    print(chunk)  # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤º
```

---

### ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ä½¿ã„æ–¹ï¼ˆQ31ï¼‰

#### Q31: HumanMessageã¨SystemMessageã®é•ã„ã¯ï¼Ÿ

**å›ç­”**: **å½¹å‰²ã‚’å®šç¾©ã™ã‚‹é‡è¦ãªè¦ç´ **ã§ã™ã€‚LangGraphã§é »ç¹ã«ä½¿ã„ã¾ã™ã€‚

**åŸºæœ¬**:
```python
from langchain_core.messages import SystemMessage, HumanMessage

# SystemMessage: LLMã«å½¹å‰²ã‚’æŒ‡ç¤º
system = SystemMessage(content="ã‚ãªãŸã¯è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™")

# HumanMessage: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
human = HumanMessage(content="ã“ã‚“ã«ã¡ã¯")
```

**LangGraphã®ãƒãƒ¼ãƒ‰å†…ã§ä½¿ç”¨**:
```python
def process_node(state: State) -> dict:
    messages = [
        SystemMessage(content="ç°¡æ½”ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚"),
        HumanMessage(content=state["user_input"])
    ]
    response = llm.invoke(messages)
    return {"output": response.content}
```

**SystemMessageã®åŠ¹æœ**:
```python
# SystemMessageãªã— â†’ é•·ã„èª¬æ˜
response1 = llm.invoke([HumanMessage("LangGraphã¨ã¯ï¼Ÿ")])

# SystemMessageã‚ã‚Š â†’ 1æ–‡ã§å›ç­”
response2 = llm.invoke([
    SystemMessage(content="1æ–‡ã§ç­”ãˆã¦"),
    HumanMessage(content="LangGraphã¨ã¯ï¼Ÿ")
])
```

---

### ã‚°ãƒ©ãƒ•æ§‹é€ ï¼ˆQ32-Q41ï¼‰

#### Q32: ã‚°ãƒ©ãƒ•ã«å¾ªç’°ï¼ˆãƒ«ãƒ¼ãƒ—ï¼‰ã‚’ä½œã‚Œã‚‹ï¼Ÿ

**å›ç­”**: **ã¯ã„ã€ã§ãã¾ã™ã€‚**

```python
workflow.add_edge("node1", "node2")
workflow.add_edge("node2", "node1")  # ãƒ«ãƒ¼ãƒ—

# æ³¨æ„: ç„¡é™ãƒ«ãƒ¼ãƒ—ã«ãªã‚‰ãªã„ã‚ˆã†æ¡ä»¶åˆ†å²ãŒå¿…è¦
workflow.add_conditional_edges("node2", should_continue, {
    "continue": "node1",
    "end": END
})
```

---

#### Q33: 1ã¤ã®ãƒãƒ¼ãƒ‰ã‹ã‚‰è¤‡æ•°ã®ãƒãƒ¼ãƒ‰ã«æ¥ç¶šã§ãã‚‹ï¼Ÿ

**å›ç­”**: **ã¯ã„ã€æ¡ä»¶åˆ†å²ã§å¯èƒ½**ã§ã™ã€‚

```python
workflow.add_conditional_edges("classify", route, {
    "route_a": "node_a",
    "route_b": "node_b",
    "route_c": "node_c"
})
```

---

#### Q34: è¤‡æ•°ã®ãƒãƒ¼ãƒ‰ã‹ã‚‰1ã¤ã®ãƒãƒ¼ãƒ‰ã«æ¥ç¶šã§ãã‚‹ï¼Ÿ

**å›ç­”**: **ã¯ã„ã€ã§ãã¾ã™ã€‚**

```python
workflow.add_edge("node1", "merge")
workflow.add_edge("node2", "merge")
workflow.add_edge("node3", "merge")
```

---

#### Q35: ã‚°ãƒ©ãƒ•ã‚’å‹•çš„ã«å¤‰æ›´ã§ãã‚‹ï¼Ÿ

**å›ç­”**: **compile()å‰ãªã‚‰å¯èƒ½ã€compile()å¾Œã¯ä¸å¯**ã€‚

```python
workflow.add_node("node1", func1)
workflow.add_node("node2", func2)  # ã“ã“ã¾ã§OK

app = workflow.compile()  # ã“ã®å¾Œã¯å¤‰æ›´ä¸å¯
```

---

#### Q36: ã‚°ãƒ©ãƒ•ã®æ·±ã•ã«åˆ¶é™ã¯ï¼Ÿ

**å›ç­”**: **å®Ÿè³ªçš„ãªåˆ¶é™ã¯ã‚ã‚Šã¾ã›ã‚“**ãŒã€æ·±ã™ãã‚‹ã¨å®Ÿè¡Œæ™‚é–“ãŒé•·ããªã‚Šã¾ã™ã€‚

---

#### Q37: ä¸¦åˆ—ãƒãƒ¼ãƒ‰ã¯ä½œã‚Œã‚‹ï¼Ÿ

**å›ç­”**: **ã¯ã„ã€ã§ãã¾ã™ã€‚**

```python
# ä¸¦åˆ—é–‹å§‹
workflow.add_edge(START, "parallel1")
workflow.add_edge(START, "parallel2")
workflow.add_edge(START, "parallel3")

# çµ±åˆ
workflow.add_edge("parallel1", "merge")
workflow.add_edge("parallel2", "merge")
workflow.add_edge("parallel3", "merge")
```

---

#### Q38: ã‚°ãƒ©ãƒ•å¯è¦–åŒ–ã¯å¿…é ˆï¼Ÿ

**å›ç­”**: **ã„ã„ãˆã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³**ã§ã™ãŒã€è¤‡é›‘ãªã‚°ãƒ©ãƒ•ã§ã¯å¼·ãæ¨å¥¨ã—ã¾ã™ã€‚

```python
# PNGä¿å­˜
png_data = app.get_graph().draw_mermaid_png()
with open("graph.png", "wb") as f:
    f.write(png_data)

# ASCIIç‰ˆ
print(app.get_graph().draw_ascii())
```

---

#### Q39: PNGãŒç”Ÿæˆã§ããªã„å ´åˆã¯ï¼Ÿ

**å›ç­”**: **ASCIIç‰ˆã§è¡¨ç¤º**ã§ãã¾ã™ã€‚

```python
print(app.get_graph().draw_ascii())
```

---

#### Q40: ã‚°ãƒ©ãƒ•ã‚’Jupyter Notebookã§è¡¨ç¤ºã—ãŸã„

**å›ç­”**: IPythonã®Imageæ©Ÿèƒ½ã‚’ä½¿ã„ã¾ã™ã€‚

```python
from IPython.display import Image, display

png_data = app.get_graph().draw_mermaid_png()
display(Image(png_data))
```

---

#### Q41: ã‚°ãƒ©ãƒ•ã®å®Ÿè¡Œé †åºã¯ã©ã†æ±ºã¾ã‚‹ï¼Ÿ

**å›ç­”**: **ã‚¨ãƒƒã‚¸ã§å®šç¾©ã—ãŸæ¥ç¶šé †åº**ã«å¾“ã„ã¾ã™ã€‚

STARTã‹ã‚‰è¾¿ã‚Œã‚‹ãƒãƒ¼ãƒ‰ã‚’æ¢ã—ã€ã‚¨ãƒƒã‚¸ã«å¾“ã£ã¦æ¬¡ã®ãƒãƒ¼ãƒ‰ã¸é€²ã¿ã€ENDã«åˆ°é”ã™ã‚‹ã¾ã§ç¶šã‘ã¾ã™ã€‚

```python
# ã“ã®æ¥ç¶š
workflow.add_edge(START, "a")
workflow.add_edge("a", "b")
workflow.add_edge("b", "c")
workflow.add_edge("c", END)

# ã“ã®å®Ÿè¡Œé †åº
# START â†’ a â†’ b â†’ c â†’ END
```

---

## ğŸ‰ Q&Aå®Œèµ°ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼

ã“ã®41å•ã§ã€LangGraphã®**æœ¬è³ªçš„ãªä½¿ã„æ–¹**ã‚’å®Œå…¨ã«ãƒã‚¹ã‚¿ãƒ¼ã—ã¾ã—ãŸã€‚

---

### å®Œå…¨ãªå®Ÿè£…ä¾‹ï¼ˆç¬¬2ç« ï¼‰

```python
"""
ç¬¬2ç« å®Œå…¨å®Ÿè£… - ã‚³ãƒ”ãƒ¼&ãƒšãƒ¼ã‚¹ãƒˆã§å‹•ãã¾ã™
"""

import os
from typing import TypedDict
from langgraph.graph import StateGraph, START, END
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage

# ============================================
# APIã‚­ãƒ¼è¨­å®š
# ============================================

os.environ["GEMINI_API_KEY"] = "your-actual-api-key"  # â† å®Ÿéš›ã®ã‚­ãƒ¼ã«å¤‰æ›´

if os.environ.get("GEMINI_API_KEY") == "your-actual-api-key":
    raise ValueError("âŒ APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„")

# ============================================
# Stateå®šç¾©
# ============================================

class State(TypedDict):
    input: str
    output: str

# ============================================
# LLMåˆæœŸåŒ–
# ============================================

llm = ChatGoogleGenerativeAI(
    google_api_key=os.environ["GEMINI_API_KEY"],
    model="gemini-2.0-flash-exp",
    temperature=0.7,
    max_tokens=1024
)

# ============================================
# ãƒãƒ¼ãƒ‰é–¢æ•°
# ============================================

def call_gemini(state: State) -> dict:
    """Gemini APIã‚’å‘¼ã³å‡ºã™"""
    print(f"ğŸ“ å…¥åŠ›: {state['input']}")
    
    response = llm.invoke([HumanMessage(content=state["input"])])
    
    print(f"âœ… å¿œç­”å—ä¿¡å®Œäº†")
    return {"output": response.content}

# ============================================
# ã‚°ãƒ©ãƒ•æ§‹ç¯‰
# ============================================

workflow = StateGraph(State)
workflow.add_node("gemini", call_gemini)
workflow.add_edge(START, "gemini")
workflow.add_edge("gemini", END)

app = workflow.compile()

print("âœ… ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æ§‹ç¯‰å®Œäº†")

# ============================================
# ã‚°ãƒ©ãƒ•å¯è¦–åŒ–
# ============================================

def visualize_graph(app, filename="workflow.png"):
    """ã‚°ãƒ©ãƒ•ã‚’PNGä¿å­˜"""
    try:
        png_data = app.get_graph().draw_mermaid_png()
        with open(filename, "wb") as f:
            f.write(png_data)
        print(f"âœ… ã‚°ãƒ©ãƒ•ã‚’ '{filename}' ã«ä¿å­˜")
    except Exception as e:
        print(f"âš ï¸ PNGä¿å­˜å¤±æ•—: {e}")
        print(app.get_graph().draw_ascii())

visualize_graph(app, "ch2_workflow.png")

# ============================================
# å®Ÿè¡Œé–¢æ•°
# ============================================

def run_workflow(app, user_input: str):
    """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œ"""
    print("\n" + "=" * 60)
    print("ğŸš€ å®Ÿè¡Œé–‹å§‹")
    print("=" * 60)
    
    result = app.invoke({"input": user_input, "output": ""})
    
    print("=" * 60)
    print("âœ… å®Œäº†")
    print("=" * 60)
    print(f"\nğŸ“¤ çµæœ:\n{result['output']}\n")
    
    return result

# ============================================
# å®Ÿè¡Œ
# ============================================

if __name__ == "__main__":
    run_workflow(app, "LangGraphã®ä¸»ãªç‰¹å¾´ã‚’3ã¤æ•™ãˆã¦ãã ã•ã„")
```

---

## ç¬¬3ç« : æ¡ä»¶åˆ†å²ã®å®Ÿè£…ã€å®Œå…¨è§£èª¬ã€‘

### 3-1. æ¡ä»¶åˆ†å²ã®åŸºæœ¬æ¦‚å¿µ

#### ãªãœæ¡ä»¶åˆ†å²ãŒå¿…è¦ãªã®ã‹

å®Ÿéš›ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ã€**å…¥åŠ›å†…å®¹ã«ã‚ˆã£ã¦å‡¦ç†ã‚’å¤‰ãˆã‚‹å¿…è¦**ãŒã‚ã‚Šã¾ã™ã€‚

**ä¾‹: ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒœãƒƒãƒˆ**
```
ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã€Œãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å¿˜ã‚Œã¾ã—ãŸã€
  â†“
æ„å›³åˆ†é¡ã€Œãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãƒªã‚»ãƒƒãƒˆã€
  â†“
FAQæ¤œç´¢ â†’ è‡ªå‹•å›ç­”

ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã€Œå•†å“ãŒå£Šã‚Œã¦ã„ã¾ã™ï¼ã€
  â†“
æ„å›³åˆ†é¡ã€Œã‚¯ãƒ¬ãƒ¼ãƒ ã€
  â†“
ç·Šæ€¥ãƒã‚±ãƒƒãƒˆä½œæˆ â†’ äººé–“ã«ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
```

**ç·šå½¢ãƒ•ãƒ­ãƒ¼ã§ã¯ä¸å¯èƒ½**
```python
# âŒ ã“ã‚Œã§ã¯ã™ã¹ã¦åŒã˜å‡¦ç†ã«ãªã£ã¦ã—ã¾ã†
workflow.add_edge(START, "process")
workflow.add_edge("process", END)
```

**æ¡ä»¶åˆ†å²ã§å®Ÿç¾**
```python
# âœ… å…¥åŠ›ã«å¿œã˜ã¦å‡¦ç†ã‚’å¤‰ãˆã‚‹
workflow.add_conditional_edges("classify", router, {
    "password_reset": "faq_node",
    "complaint": "escalate_node",
    "question": "answer_node"
})
```

---

#### add_conditional_edges() ã®å®Œå…¨ç†è§£

**æ§‹æ–‡ã®åˆ†è§£**
```python
workflow.add_conditional_edges(
    source="åˆ¤å®šã‚’è¡Œã†ãƒãƒ¼ãƒ‰å",        # â‘ 
    path=ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é–¢æ•°,              # â‘¡
    path_map={                         # â‘¢
        "ãƒ«ãƒ¼ãƒˆåA": "é·ç§»å…ˆãƒãƒ¼ãƒ‰A",
        "ãƒ«ãƒ¼ãƒˆåB": "é·ç§»å…ˆãƒãƒ¼ãƒ‰B",
        "ãƒ«ãƒ¼ãƒˆåC": END
    }
)
```

**â‘  sourceï¼ˆåˆ¤å®šã‚’è¡Œã†ãƒãƒ¼ãƒ‰ï¼‰**
- ã“ã®ãƒãƒ¼ãƒ‰ã®**å®Ÿè¡Œå¾Œ**ã«æ¡ä»¶åˆ†å²ãŒç™ºç”Ÿ
- æ–‡å­—åˆ—ã§ãƒãƒ¼ãƒ‰åã‚’æŒ‡å®š

**â‘¡ pathï¼ˆãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é–¢æ•°ï¼‰**
- Stateã‚’å—ã‘å–ã‚Šã€**æ¬¡ã«é€²ã‚€ãƒ«ãƒ¼ãƒˆåã‚’è¿”ã™é–¢æ•°**
- å¿…ãšæ–‡å­—åˆ—ã‚’è¿”ã™
- ã“ã®æˆ»ã‚Šå€¤ãŒpath_mapã®ã‚­ãƒ¼ã¨ä¸€è‡´ã™ã‚‹å¿…è¦ãŒã‚ã‚‹

**â‘¢ path_mapï¼ˆãƒ«ãƒ¼ãƒˆãƒãƒƒãƒ”ãƒ³ã‚°ï¼‰**
- ã‚­ãƒ¼: ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é–¢æ•°ãŒè¿”ã™æ–‡å­—åˆ—
- å€¤: æ¬¡ã«å®Ÿè¡Œã™ã‚‹ãƒãƒ¼ãƒ‰åï¼ˆã¾ãŸã¯ENDï¼‰

---

### 3-2. æœ€å°ã®æ¡ä»¶åˆ†å²å®Ÿè£…ï¼ˆã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ï¼‰

#### ã‚¹ãƒ†ãƒƒãƒ—1: è¦ä»¶å®šç¾©

**ç›®æ¨™**: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’ã€ŒæŒ¨æ‹¶ã€ã€Œè³ªå•ã€ã€Œãã®ä»–ã€ã«åˆ†é¡ã—ã¦ã€ãã‚Œãã‚Œç•°ãªã‚‹å‡¦ç†ã‚’è¡Œã†

**ãƒ•ãƒ­ãƒ¼è¨­è¨ˆ**
```
START
  â†“
classifyï¼ˆåˆ†é¡ãƒãƒ¼ãƒ‰ï¼‰
  â†“
æ¡ä»¶åˆ†å²
  â”œâ”€ "greeting" â†’ greeting_node â†’ END
  â”œâ”€ "question" â†’ question_node â†’ END
  â””â”€ "other" â†’ other_node â†’ END
```

---

#### ã‚¹ãƒ†ãƒƒãƒ—2: Stateè¨­è¨ˆ

```python
from typing import TypedDict

class State(TypedDict):
    input: str       # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
    category: str    # åˆ†é¡çµæœï¼ˆgreeting/question/otherï¼‰
    output: str      # æœ€çµ‚çš„ãªå¿œç­”
```

**é‡è¦ãƒã‚¤ãƒ³ãƒˆ**:
- `category`ã¯åˆ†å²åˆ¤å®šã«ä½¿ç”¨
- ãƒãƒ¼ãƒ‰é–¢æ•°ã§`category`ã‚’æ›´æ–°ã—ã€ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é–¢æ•°ã§èª­ã¿å–ã‚‹

---

#### ã‚¹ãƒ†ãƒƒãƒ—3: åˆ†é¡ãƒãƒ¼ãƒ‰å®Ÿè£…

```python
import os
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, SystemMessage

os.environ["GEMINI_API_KEY"] = "your-api-key"

llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-exp",
    temperature=0.0  # åˆ†é¡ã¯æ±ºå®šçš„ã«ã™ã‚‹
)

def classify(state: State) -> dict:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’åˆ†é¡ã™ã‚‹"""
    
    # LLMã«åˆ†é¡ã‚’ä¾é ¼
    messages = [
        SystemMessage(content="""
ã‚ãªãŸã¯å…¥åŠ›åˆ†é¡ã®å°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦åˆ†é¡ã—ã¦ãã ã•ã„:

- ã€Œã“ã‚“ã«ã¡ã¯ã€ã€ŒãŠã¯ã‚ˆã†ã€ãªã© â†’ greeting
- ç–‘å•æ–‡ã‚„èª¬æ˜ã‚’æ±‚ã‚ã‚‹å†…å®¹ â†’ question  
- ä¸Šè¨˜ä»¥å¤– â†’ other

å¿…ãšã€Œgreetingã€ã€Œquestionã€ã€Œotherã€ã®ã„ãšã‚Œã‹1ã¤ã ã‘ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
ä½™è¨ˆãªèª¬æ˜ã¯ä¸è¦ã§ã™ã€‚
        """),
        HumanMessage(content=f"åˆ†é¡å¯¾è±¡: {state['input']}")
    ]
    
    response = llm.invoke(messages)
    category = response.content.strip().lower()
    
    # äºˆæœŸã—ãªã„å€¤ã®å ´åˆã¯otherã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    if category not in ["greeting", "question", "other"]:
        category = "other"
    
    return {"category": category}
```

**ãƒã‚¤ãƒ³ãƒˆè§£èª¬**:
1. **temperature=0.0**: åˆ†é¡ã¯ä¸€è²«æ€§ãŒé‡è¦ãªã®ã§æ±ºå®šçš„ã«ã™ã‚‹
2. **SystemMessage**: LLMã«æ˜ç¢ºãªæŒ‡ç¤ºã‚’ä¸ãˆã‚‹
3. **ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†**: äºˆæœŸã—ãªã„å€¤ã¸ã®å¯¾ç­–ï¼ˆé‡è¦ï¼ï¼‰

---

#### ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é–¢æ•°ã®å®Ÿè£…

```python
from typing import Literal

def route_by_category(state: State) -> Literal["greeting", "question", "other"]:
    """åˆ†é¡çµæœã«åŸºã¥ã„ã¦ãƒ«ãƒ¼ãƒˆã‚’æ±ºå®š"""
    return state["category"]
```

**é‡è¦ãªç†è§£**:
- ã“ã®é–¢æ•°ã¯**ã‚·ãƒ³ãƒ—ãƒ«ã§OK**
- è¤‡é›‘ãªãƒ­ã‚¸ãƒƒã‚¯ã¯åˆ†é¡ãƒãƒ¼ãƒ‰ã§å®Ÿè£…æ¸ˆã¿
- ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é–¢æ•°ã¯å˜ã«çµæœã‚’è¿”ã™ã ã‘

**Literalã®å½¹å‰²**:
```python
# âœ… å‹å®‰å…¨ï¼ˆIDEãŒè£œå®Œã—ã¦ãã‚Œã‚‹ï¼‰
def route(state: State) -> Literal["greeting", "question", "other"]:
    return state["category"]

# âŒ å‹ãƒ’ãƒ³ãƒˆãªã—ï¼ˆå‹•ããŒæ¨å¥¨ã—ãªã„ï¼‰
def route(state: State):
    return state["category"]
```

---

#### ã‚¹ãƒ†ãƒƒãƒ—5: å„ãƒ«ãƒ¼ãƒˆã®å‡¦ç†ãƒãƒ¼ãƒ‰å®Ÿè£…

```python
def handle_greeting(state: State) -> dict:
    """æŒ¨æ‹¶ã¸ã®å¿œç­”"""
    return {
        "output": "ã“ã‚“ã«ã¡ã¯ï¼ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"
    }

def handle_question(state: State) -> dict:
    """è³ªå•ã¸ã®å¿œç­”"""
    # LLMã‚’ä½¿ã£ã¦è³ªå•ã«å›ç­”
    response = llm.invoke([
        SystemMessage(content="è¦ªåˆ‡ã§åˆ†ã‹ã‚Šã‚„ã™ãå›ç­”ã—ã¦ãã ã•ã„ã€‚"),
        HumanMessage(content=state["input"])
    ])
    return {"output": response.content}

def handle_other(state: State) -> dict:
    """ãã®ä»–ã¸ã®å¿œç­”"""
    return {
        "output": "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€ç†è§£ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠé¡˜ã„ã—ã¾ã™ã€‚"
    }
```

**è¨­è¨ˆã®ãƒã‚¤ãƒ³ãƒˆ**:
- `handle_greeting`: ã‚·ãƒ³ãƒ—ãƒ«ãªå®šå‹å¿œç­”ï¼ˆLLMä¸è¦ï¼‰
- `handle_question`: LLMã§å‹•çš„ã«å›ç­”ç”Ÿæˆ
- `handle_other`: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†

---

#### ã‚¹ãƒ†ãƒƒãƒ—6: ã‚°ãƒ©ãƒ•ã®çµ„ã¿ç«‹ã¦

```python
from langgraph.graph import StateGraph, START, END

# ã‚°ãƒ©ãƒ•ä½œæˆ
workflow = StateGraph(State)

# ãƒãƒ¼ãƒ‰è¿½åŠ 
workflow.add_node("classify", classify)
workflow.add_node("greeting", handle_greeting)
workflow.add_node("question", handle_question)
workflow.add_node("other", handle_other)

# ã‚¨ãƒƒã‚¸è¿½åŠ 
workflow.add_edge(START, "classify")  # é–‹å§‹ç‚¹

# æ¡ä»¶åˆ†å²ï¼ˆé‡è¦ï¼ï¼‰
workflow.add_conditional_edges(
    source="classify",          # åˆ†é¡ãƒãƒ¼ãƒ‰ã®å¾Œã§åˆ†å²
    path=route_by_category,     # ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é–¢æ•°
    path_map={                  # ãƒ«ãƒ¼ãƒˆãƒãƒƒãƒ”ãƒ³ã‚°
        "greeting": "greeting",  # ã‚­ãƒ¼=é–¢æ•°ã®æˆ»ã‚Šå€¤, å€¤=ãƒãƒ¼ãƒ‰å
        "question": "question",
        "other": "other"
    }
)

# å„ãƒ«ãƒ¼ãƒˆã®çµ‚äº†
workflow.add_edge("greeting", END)
workflow.add_edge("question", END)
workflow.add_edge("other", END)

# ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
app = workflow.compile()
```

**ã‚°ãƒ©ãƒ•æ§‹é€ ã®å¯è¦–åŒ–**:
```
__start__
    â†“
classify
    â†“
  [æ¡ä»¶åˆ†å²]
    â”œâ”€ greeting â†’ __end__
    â”œâ”€ question â†’ __end__
    â””â”€ other â†’ __end__
```

---

#### ã‚¹ãƒ†ãƒƒãƒ—7: ãƒ†ã‚¹ãƒˆã¨å®Ÿè¡Œ

```python
# ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
test_inputs = [
    "ã“ã‚“ã«ã¡ã¯",
    "LangGraphã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
    "ã‚ã„ã†ãˆãŠ12345",
    "ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™",
    "æ¡ä»¶åˆ†å²ã®ä»•çµ„ã¿ã‚’æ•™ãˆã¦"
]

for user_input in test_inputs:
    result = app.invoke({
        "input": user_input,
        "category": "",  # åˆæœŸå€¤
        "output": ""     # åˆæœŸå€¤
    })
    
    print(f"å…¥åŠ›: {user_input}")
    print(f"åˆ†é¡: {result['category']}")
    print(f"å¿œç­”: {result['output']}")
    print("-" * 60)
```

**å®Ÿè¡Œçµæœä¾‹**:
```
å…¥åŠ›: ã“ã‚“ã«ã¡ã¯
åˆ†é¡: greeting
å¿œç­”: ã“ã‚“ã«ã¡ã¯ï¼ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ
------------------------------------------------------------
å…¥åŠ›: LangGraphã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ
åˆ†é¡: question
å¿œç­”: LangGraphã¯ã€LangChainã®ä¸Šã«æ§‹ç¯‰ã•ã‚ŒãŸ...ï¼ˆLLMã®å›ç­”ï¼‰
------------------------------------------------------------
å…¥åŠ›: ã‚ã„ã†ãˆãŠ12345
åˆ†é¡: other
å¿œç­”: ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€ç†è§£ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠé¡˜ã„ã—ã¾ã™ã€‚
------------------------------------------------------------
```

---

#### å®Œå…¨ãªã‚³ãƒ¼ãƒ‰ï¼ˆã‚³ãƒ”ãƒšå¯èƒ½ï¼‰

```python
import os
from typing import TypedDict, Literal
from langgraph.graph import StateGraph, START, END
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, SystemMessage

# APIã‚­ãƒ¼è¨­å®š
os.environ["GEMINI_API_KEY"] = "your-api-key"

# Stateå®šç¾©
class State(TypedDict):
    input: str
    category: str
    output: str

# LLMåˆæœŸåŒ–
llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash-exp", temperature=0.0)

# ãƒãƒ¼ãƒ‰1: åˆ†é¡
def classify(state: State) -> dict:
    messages = [
        SystemMessage(content="""
å…¥åŠ›ã‚’åˆ†é¡ã—ã¦ãã ã•ã„:
- greeting: æŒ¨æ‹¶
- question: è³ªå•
- other: ãã®ä»–
1å˜èªã®ã¿è¿”ã—ã¦ãã ã•ã„ã€‚
        """),
        HumanMessage(content=state["input"])
    ]
    response = llm.invoke(messages)
    category = response.content.strip().lower()
    if category not in ["greeting", "question", "other"]:
        category = "other"
    return {"category": category}

# ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é–¢æ•°
def route_by_category(state: State) -> Literal["greeting", "question", "other"]:
    return state["category"]

# ãƒãƒ¼ãƒ‰2a: æŒ¨æ‹¶å‡¦ç†
def handle_greeting(state: State) -> dict:
    return {"output": "ã“ã‚“ã«ã¡ã¯ï¼ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"}

# ãƒãƒ¼ãƒ‰2b: è³ªå•å‡¦ç†
def handle_question(state: State) -> dict:
    response = llm.invoke([
        SystemMessage(content="è¦ªåˆ‡ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚"),
        HumanMessage(content=state["input"])
    ])
    return {"output": response.content}

# ãƒãƒ¼ãƒ‰2c: ãã®ä»–
def handle_other(state: State) -> dict:
    return {"output": "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€ç†è§£ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"}

# ã‚°ãƒ©ãƒ•æ§‹ç¯‰
workflow = StateGraph(State)
workflow.add_node("classify", classify)
workflow.add_node("greeting", handle_greeting)
workflow.add_node("question", handle_question)
workflow.add_node("other", handle_other)

workflow.add_edge(START, "classify")
workflow.add_conditional_edges("classify", route_by_category, {
    "greeting": "greeting",
    "question": "question",
    "other": "other"
})
workflow.add_edge("greeting", END)
workflow.add_edge("question", END)
workflow.add_edge("other", END)

app = workflow.compile()

# ãƒ†ã‚¹ãƒˆ
test_inputs = ["ã“ã‚“ã«ã¡ã¯", "LangGraphã¨ã¯?", "12345"]
for text in test_inputs:
    result = app.invoke({"input": text, "category": "", "output": ""})
    print(f"å…¥åŠ›: {text} | åˆ†é¡: {result['category']} | å¿œç­”: {result['output'][:50]}...")
```

---

### 3-3. ã‚ˆãã‚ã‚‹é–“é•ã„ã¨ãƒ‡ãƒãƒƒã‚°

#### é–“é•ã„1: ãƒ«ãƒ¼ãƒˆãƒãƒƒãƒ”ãƒ³ã‚°ã®ä¸ä¸€è‡´

```python
# âŒ ã‚¨ãƒ©ãƒ¼ä¾‹
def route(state: State) -> Literal["greet", "quest", "other"]:
    if state["category"] == "greeting":
        return "greet"  # ã“ã“ã§"greet"ã‚’è¿”ã™
    elif state["category"] == "question":
        return "quest"
    return "other"

workflow.add_conditional_edges("classify", route, {
    "greeting": "greeting_node",  # ã‚­ãƒ¼ãŒ"greeting"ã ãŒãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é–¢æ•°ã¯"greet"ã‚’è¿”ã™
    "question": "question_node",  # ã‚­ãƒ¼ãŒ"question"ã ãŒãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é–¢æ•°ã¯"quest"ã‚’è¿”ã™
    "other": "other_node"
})
# â†’ KeyError: 'greet' ãŒç™ºç”Ÿ
```

**ä¿®æ­£æ–¹æ³•**:
```python
# âœ… æ­£ã—ã„å®Ÿè£…
def route(state: State) -> Literal["greeting", "question", "other"]:
    return state["category"]  # Stateã®categoryã‚’ãã®ã¾ã¾è¿”ã™

workflow.add_conditional_edges("classify", route, {
    "greeting": "greeting_node",  # ã‚­ãƒ¼ã¨æˆ»ã‚Šå€¤ãŒä¸€è‡´
    "question": "question_node",
    "other": "other_node"
})
```

---

#### é–“é•ã„2: ã™ã¹ã¦ã®ã‚±ãƒ¼ã‚¹ã‚’ã‚«ãƒãƒ¼ã—ã¦ã„ãªã„

```python
# âŒ ã‚¨ãƒ©ãƒ¼ä¾‹
def route(state: State) -> Literal["greeting", "question", "other"]:
    return state["category"]

workflow.add_conditional_edges("classify", route, {
    "greeting": "greeting_node",
    "question": "question_node"
    # "other"ã®ã‚±ãƒ¼ã‚¹ãŒæŠœã‘ã¦ã„ã‚‹
})
# â†’ state["category"]ãŒ"other"ã®ã¨ãKeyError
```

**ä¿®æ­£æ–¹æ³•**:
```python
# âœ… ã™ã¹ã¦ã®ã‚±ãƒ¼ã‚¹ã‚’ã‚«ãƒãƒ¼
workflow.add_conditional_edges("classify", route, {
    "greeting": "greeting_node",
    "question": "question_node",
    "other": "other_node"  # å¿˜ã‚Œãšã«è¿½åŠ 
})
```

---

#### é–“é•ã„3: ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é–¢æ•°ã§ç›´æ¥åˆ†å²ãƒ­ã‚¸ãƒƒã‚¯ã‚’æ›¸ã

```python
# âŒ ã‚¢ãƒ³ãƒãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå‹•ããŒä¿å®ˆæ€§ãŒæ‚ªã„ï¼‰
def route(state: State) -> Literal["route_a", "route_b"]:
    user_input = state["input"]
    
    # ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é–¢æ•°å†…ã§è¤‡é›‘ãªãƒ­ã‚¸ãƒƒã‚¯
    if "ã“ã‚“ã«ã¡ã¯" in user_input or "ãŠã¯ã‚ˆã†" in user_input:
        return "route_a"
    elif "ï¼Ÿ" in user_input or "æ•™ãˆã¦" in user_input:
        return "route_b"
    else:
        return "route_a"
```

**æ¨å¥¨ãƒ‘ã‚¿ãƒ¼ãƒ³**:
```python
# âœ… ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
# 1. åˆ†é¡ãƒãƒ¼ãƒ‰ã§åˆ¤å®š
def classify(state: State) -> dict:
    # ã“ã“ã§è¤‡é›‘ãªãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…
    category = complex_classification_logic(state["input"])
    return {"category": category}

# 2. ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é–¢æ•°ã¯ã‚·ãƒ³ãƒ—ãƒ«ã«
def route(state: State) -> Literal["route_a", "route_b"]:
    return state["category"]
```

**ç†ç”±**:
- åˆ†é¡ãƒ­ã‚¸ãƒƒã‚¯ã®å†åˆ©ç”¨ãŒå®¹æ˜“
- ãƒ†ã‚¹ãƒˆãŒã—ã‚„ã™ã„
- ã‚³ãƒ¼ãƒ‰ã®å¯èª­æ€§ãŒé«˜ã„

---

### 3-4. è¤‡æ•°æ¡ä»¶ã§ã®é«˜åº¦ãªãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

#### ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ã®åˆ†å²

**ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹**: ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã«å¿œã˜ã¦å‡¦ç†ã‚’å¤‰ãˆã‚‹

```python
from typing import TypedDict, Literal
from langgraph.graph import StateGraph, START, END
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, SystemMessage
import os

os.environ["GEMINI_API_KEY"] = "your-api-key"

class State(TypedDict):
    query: str
    confidence_score: float  # 0.0ã€œ1.0
    answer: str

llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash-exp", temperature=0.3)

# ã‚¹ãƒ†ãƒƒãƒ—1: ä¿¡é ¼åº¦ã‚’è©•ä¾¡
def evaluate_confidence(state: State) -> dict:
    """è³ªå•ã®é›£æ˜“åº¦ã‚’è©•ä¾¡ã—ã¦ã‚¹ã‚³ã‚¢ã‚’ç®—å‡º"""
    
    messages = [
        SystemMessage(content="""
ã‚ãªãŸã¯è³ªå•ã®é›£æ˜“åº¦è©•ä¾¡ã®å°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®åŸºæº–ã§ã‚¹ã‚³ã‚¢ã‚’0.0ã€œ1.0ã§è©•ä¾¡ã—ã¦ãã ã•ã„:

0.0-0.3: ç°¡å˜ï¼ˆä¸€èˆ¬å¸¸è­˜ã§ç­”ãˆã‚‰ã‚Œã‚‹ï¼‰
  ä¾‹: "ã“ã‚“ã«ã¡ã¯", "ä»Šæ—¥ã®å¤©æ°—ã¯?"
  
0.4-0.7: ä¸­ç¨‹åº¦ï¼ˆã‚ã‚‹ç¨‹åº¦ã®çŸ¥è­˜ãŒå¿…è¦ï¼‰
  ä¾‹: "LangGraphã®åŸºæœ¬çš„ãªä½¿ã„æ–¹ã¯?", "Pythonã§ãƒªã‚¹ãƒˆã‚’ã‚½ãƒ¼ãƒˆã™ã‚‹æ–¹æ³•ã¯?"
  
0.8-1.0: é›£ã—ã„ï¼ˆå°‚é–€çŸ¥è­˜ãŒå¿…è¦ï¼‰
  ä¾‹: "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®åŸç†ã‚’èª¬æ˜ã—ã¦", "transformerãƒ¢ãƒ‡ãƒ«ã®attentionæ©Ÿæ§‹ã®æ•°å¼ã‚’å°å‡ºã—ã¦"

å¿…ãš0.0ã€œ1.0ã®æ•°å€¤ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚èª¬æ˜ã¯ä¸è¦ã§ã™ã€‚
        """),
        HumanMessage(content=f"è©•ä¾¡å¯¾è±¡: {state['query']}")
    ]
    
    response = llm.invoke(messages)
    
    try:
        score = float(response.content.strip())
        # ç¯„å›²ãƒã‚§ãƒƒã‚¯
        score = max(0.0, min(1.0, score))
    except ValueError:
        # ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆã¯ä¸­é–“å€¤
        score = 0.5
    
    return {"confidence_score": score}

# ã‚¹ãƒ†ãƒƒãƒ—2: ã‚¹ã‚³ã‚¢ã«åŸºã¥ã„ã¦ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
def route_by_confidence(state: State) -> Literal["simple", "moderate", "complex"]:
    """ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã«åŸºã¥ã„ã¦ãƒ«ãƒ¼ãƒˆã‚’æ±ºå®š"""
    score = state["confidence_score"]
    
    if score < 0.4:
        return "simple"
    elif score < 0.8:
        return "moderate"
    else:
        return "complex"

# ã‚¹ãƒ†ãƒƒãƒ—3: å„é›£æ˜“åº¦ã«å¿œã˜ãŸå‡¦ç†
def handle_simple(state: State) -> dict:
    """ç°¡å˜ãªè³ªå•ã¸ã®å¿œç­”"""
    llm_simple = ChatGoogleGenerativeAI(
        model="gemini-1.5-flash",  # è»½é‡ãƒ¢ãƒ‡ãƒ«
        temperature=0.7,
        max_tokens=256  # çŸ­ã„å¿œç­”
    )
    
    response = llm_simple.invoke([
        SystemMessage(content="ç°¡æ½”ã«1-2æ–‡ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"),
        HumanMessage(content=state["query"])
    ])
    
    return {"answer": f"[ç°¡æ˜“å›ç­”] {response.content}"}

def handle_moderate(state: State) -> dict:
    """ä¸­ç¨‹åº¦ã®è³ªå•ã¸ã®å¿œç­”"""
    response = llm.invoke([
        SystemMessage(content="åˆ†ã‹ã‚Šã‚„ã™ãã€é©åº¦ãªè©³ã—ã•ã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"),
        HumanMessage(content=state["query"])
    ])
    
    return {"answer": f"[æ¨™æº–å›ç­”] {response.content}"}

def handle_complex(state: State) -> dict:
    """é›£ã—ã„è³ªå•ã¸ã®å¿œç­”"""
    llm_advanced = ChatGoogleGenerativeAI(
        model="gemini-2.0-flash-exp",  # é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«
        temperature=0.3,
        max_tokens=2048  # è©³ç´°ãªå¿œç­”
    )
    
    response = llm_advanced.invoke([
        SystemMessage(content="""
å°‚é–€çš„ã‹ã¤åŒ…æ‹¬çš„ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
å¿…è¦ã«å¿œã˜ã¦ä»¥ä¸‹ã‚’å«ã‚ã¦ãã ã•ã„:
- èƒŒæ™¯çŸ¥è­˜
- è©³ç´°ãªèª¬æ˜
- å…·ä½“ä¾‹
- é–¢é€£ã™ã‚‹æ¦‚å¿µ
        """),
        HumanMessage(content=state["query"])
    ])
    
    return {"answer": f"[è©³ç´°å›ç­”] {response.content}"}

# ã‚°ãƒ©ãƒ•æ§‹ç¯‰
workflow = StateGraph(State)

workflow.add_node("evaluate", evaluate_confidence)
workflow.add_node("simple", handle_simple)
workflow.add_node("moderate", handle_moderate)
workflow.add_node("complex", handle_complex)

workflow.add_edge(START, "evaluate")

workflow.add_conditional_edges(
    source="evaluate",
    path=route_by_confidence,
    path_map={
        "simple": "simple",
        "moderate": "moderate",
        "complex": "complex"
    }
)

workflow.add_edge("simple", END)
workflow.add_edge("moderate", END)
workflow.add_edge("complex", END)

app = workflow.compile()

# ãƒ†ã‚¹ãƒˆ
test_queries = [
    "ã“ã‚“ã«ã¡ã¯",
    "LangGraphã®åŸºæœ¬çš„ãªä½¿ã„æ–¹ã‚’æ•™ãˆã¦",
    "é‡å­ã‚‚ã¤ã‚Œã®æ•°å­¦çš„å®šå¼åŒ–ã¨ãƒ™ãƒ«ä¸ç­‰å¼ã®å°å‡ºéç¨‹ã‚’èª¬æ˜ã—ã¦ãã ã•ã„"
]

for query in test_queries:
    result = app.invoke({
        "query": query,
        "confidence_score": 0.0,
        "answer": ""
    })
    
    print(f"\n{'='*70}")
    print(f"è³ªå•: {query}")
    print(f"é›£æ˜“åº¦ã‚¹ã‚³ã‚¢: {result['confidence_score']:.2f}")
    print(f"å›ç­”: {result['answer'][:100]}...")
```

**å®Ÿè¡Œçµæœä¾‹**:
```
======================================================================
è³ªå•: ã“ã‚“ã«ã¡ã¯
é›£æ˜“åº¦ã‚¹ã‚³ã‚¢: 0.15
å›ç­”: [ç°¡æ˜“å›ç­”] ã“ã‚“ã«ã¡ã¯ï¼ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ
======================================================================
è³ªå•: LangGraphã®åŸºæœ¬çš„ãªä½¿ã„æ–¹ã‚’æ•™ãˆã¦
é›£æ˜“åº¦ã‚¹ã‚³ã‚¢: 0.55
å›ç­”: [æ¨™æº–å›ç­”] LangGraphã¯ã€è¤‡é›‘ãªAIãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ã‚°ãƒ©ãƒ•æ§‹é€ ã§è¡¨ç¾ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™...
======================================================================
è³ªå•: é‡å­ã‚‚ã¤ã‚Œã®æ•°å­¦çš„å®šå¼åŒ–ã¨ãƒ™ãƒ«ä¸ç­‰å¼ã®å°å‡ºéç¨‹ã‚’èª¬æ˜ã—ã¦ãã ã•ã„
é›£æ˜“åº¦ã‚¹ã‚³ã‚¢: 0.95
å›ç­”: [è©³ç´°å›ç­”] é‡å­ã‚‚ã¤ã‚Œ(quantum entanglement)ã¯ã€2ã¤ä»¥ä¸Šã®é‡å­ç³»ãŒåˆ†é›¢ä¸å¯èƒ½ãªçŠ¶æ…‹...
```

---

### 3-5. å®Ÿè·µä¾‹: ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒœãƒƒãƒˆï¼ˆå®Œå…¨å®Ÿè£…ï¼‰

#### è¦ä»¶å®šç¾©

**æ©Ÿèƒ½è¦ä»¶**:
1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ„å›³ã‚’åˆ†é¡ï¼ˆå•ã„åˆã‚ã›/ã‚¯ãƒ¬ãƒ¼ãƒ /è¦æœ›/ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ï¼‰
2. æ„Ÿæƒ…åˆ†æï¼ˆãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«/ãƒã‚¬ãƒ†ã‚£ãƒ–ï¼‰
3. å„ªå…ˆåº¦åˆ¤å®šï¼ˆé«˜/ä¸­/ä½ï¼‰
4. å„ªå…ˆåº¦ã«å¿œã˜ãŸå¿œç­”ç”Ÿæˆ

**ãƒ•ãƒ­ãƒ¼è¨­è¨ˆ**:
```
START â†’ analyze â†’ 
  â”œâ”€ high priority â†’ urgent_response â†’ END
  â”œâ”€ medium priority â†’ standard_response â†’ END
  â””â”€ low priority â†’ basic_response â†’ END
```

---

#### å®Œå…¨å®Ÿè£…

```python
import os
from typing import TypedDict, Literal
from langgraph.graph import StateGraph, START, END
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, SystemMessage

os.environ["GEMINI_API_KEY"] = "your-api-key"

# Stateå®šç¾©
class SupportState(TypedDict):
    message: str        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    intent: str         # æ„å›³ï¼ˆinquiry/complaint/request/feedbackï¼‰
    sentiment: str      # æ„Ÿæƒ…ï¼ˆpositive/neutral/negativeï¼‰
    priority: str       # å„ªå…ˆåº¦ï¼ˆhigh/medium/lowï¼‰
    response: str       # æœ€çµ‚å¿œç­”
    analysis_detail: str  # åˆ†æã®è©³ç´°ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰

llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash-exp", temperature=0.3)

# ãƒãƒ¼ãƒ‰1: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸åˆ†æ
def analyze_message(state: SupportState) -> dict:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å¤šè§’çš„ã«åˆ†æ"""
    
    message = state["message"]
    
    # æ„å›³åˆ†é¡
    intent_prompt = """
ä»¥ä¸‹ã®ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡ã—ã¦ãã ã•ã„:
- inquiry: è£½å“ã‚„ã‚µãƒ¼ãƒ“ã‚¹ã«ã¤ã„ã¦ã®å•ã„åˆã‚ã›
- complaint: ä¸æº€ã‚„ã‚¯ãƒ¬ãƒ¼ãƒ 
- request: æ–°æ©Ÿèƒ½ã‚„æ”¹å–„ã®è¦æœ›
- feedback: ä¸€èˆ¬çš„ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯

å…¥åŠ›: {message}

å¿…ãšä¸Šè¨˜4ã¤ã®ã†ã¡1ã¤ã ã‘ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
    """.format(message=message)
    
    intent_response = llm.invoke([HumanMessage(content=intent_prompt)])
    intent = intent_response.content.strip().lower()
    
    # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    valid_intents = ["inquiry", "complaint", "request", "feedback"]
    if intent not in valid_intents:
        intent = "inquiry"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    # æ„Ÿæƒ…åˆ†æ
    sentiment_prompt = """
ä»¥ä¸‹ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ„Ÿæƒ…ã‚’åˆ†æã—ã¦ãã ã•ã„:
- positive: ãƒã‚¸ãƒ†ã‚£ãƒ–ãªå†…å®¹
- neutral: ä¸­ç«‹çš„ãªå†…å®¹
- negative: ãƒã‚¬ãƒ†ã‚£ãƒ–ãªå†…å®¹

å…¥åŠ›: {message}

å¿…ãšä¸Šè¨˜3ã¤ã®ã†ã¡1ã¤ã ã‘ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
    """.format(message=message)
    
    sentiment_response = llm.invoke([HumanMessage(content=sentiment_prompt)])
    sentiment = sentiment_response.content.strip().lower()
    
    # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    valid_sentiments = ["positive", "neutral", "negative"]
    if sentiment not in valid_sentiments:
        sentiment = "neutral"
    
    # å„ªå…ˆåº¦åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯
    if intent == "complaint" or sentiment == "negative":
        priority = "high"
    elif intent == "inquiry":
        priority = "medium"
    else:
        priority = "low"
    
    # åˆ†æè©³ç´°ã‚’è¨˜éŒ²ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
    analysis_detail = f"æ„å›³={intent}, æ„Ÿæƒ…={sentiment}, å„ªå…ˆåº¦={priority}"
    
    return {
        "intent": intent,
        "sentiment": sentiment,
        "priority": priority,
        "analysis_detail": analysis_detail
    }

# ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é–¢æ•°
def route_by_priority(state: SupportState) -> Literal["high", "medium", "low"]:
    """å„ªå…ˆåº¦ã«åŸºã¥ã„ã¦ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°"""
    return state["priority"]

# é«˜å„ªå…ˆåº¦å¯¾å¿œ
def handle_high_priority(state: SupportState) -> dict:
    """ç·Šæ€¥å¯¾å¿œãŒå¿…è¦ãªã‚±ãƒ¼ã‚¹"""
    
    messages = [
        SystemMessage(content="""
ã‚ãªãŸã¯çµŒé¨“è±Šå¯Œãªã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆã®ã‚·ãƒ‹ã‚¢ã‚¹ã‚¿ãƒƒãƒ•ã§ã™ã€‚
ã‚¯ãƒ¬ãƒ¼ãƒ ã‚„ç·Šæ€¥ã®å•ã„åˆã‚ã›ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚

ä»¥ä¸‹ã®ç‚¹ã«æ³¨æ„ã—ã¦å¿œç­”ã—ã¦ãã ã•ã„:
1. èª å®Ÿãªè¬ç½ªã‹ã‚‰å§‹ã‚ã‚‹
2. å•é¡Œã‚’ç†è§£ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã™
3. å…·ä½“çš„ãªè§£æ±ºç­–ã‚’æç¤º
4. è¿…é€Ÿãªå¯¾å¿œã‚’ç´„æŸ
5. ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®æ‰‹é †ã‚’æ¡ˆå†…
        """),
        HumanMessage(content=state["message"])
    ]
    
    response = llm.invoke(messages)
    
    return {
        "response": f"ã€ç·Šæ€¥å¯¾å¿œã€‘\n{response.content}\n\nâ€»ã“ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯å„ªå…ˆçš„ã«å‡¦ç†ã•ã‚Œã€æ‹…å½“è€…ã«å³åº§ã«ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã•ã‚Œã¾ã—ãŸã€‚"
    }

# ä¸­å„ªå…ˆåº¦å¯¾å¿œ
def handle_medium_priority(state: SupportState) -> dict:
    """æ¨™æº–çš„ãªå•ã„åˆã‚ã›å¯¾å¿œ"""
    
    messages = [
        SystemMessage(content="""
ã‚ãªãŸã¯è¦ªåˆ‡ã§çŸ¥è­˜è±Šå¯Œãªã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆã‚¹ã‚¿ãƒƒãƒ•ã§ã™ã€‚
ä¸€èˆ¬çš„ãªå•ã„åˆã‚ã›ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚

ä»¥ä¸‹ã®ç‚¹ã«æ³¨æ„ã—ã¦å¿œç­”ã—ã¦ãã ã•ã„:
1. è¦ªã—ã¿ã‚„ã™ãä¸å¯§ãªå£èª¿
2. è³ªå•ã«æ­£ç¢ºã«å›ç­”
3. å¿…è¦ã«å¿œã˜ã¦è¿½åŠ æƒ…å ±ã‚’æä¾›
4. ã•ã‚‰ãªã‚‹è³ªå•ã‚’ä¿ƒã™
        """),
        HumanMessage(content=state["message"])
    ]
    
    response = llm.invoke(messages)
    
    return {
        "response": f"ã€æ¨™æº–å¯¾å¿œã€‘\n{response.content}\n\nã•ã‚‰ã«ã”ä¸æ˜ãªç‚¹ãŒã”ã–ã„ã¾ã—ãŸã‚‰ã€ãŠæ°—è»½ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚"
    }

# ä½å„ªå…ˆåº¦å¯¾å¿œ
def handle_low_priority(state: SupportState) -> dict:
    """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚„è¦æœ›ã¸ã®å¯¾å¿œ"""
    
    messages = [
        SystemMessage(content="""
ã‚ãªãŸã¯ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ãªã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆã‚¹ã‚¿ãƒƒãƒ•ã§ã™ã€‚
ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚„è¦æœ›ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚

ä»¥ä¸‹ã®ç‚¹ã«æ³¨æ„ã—ã¦å¿œç­”ã—ã¦ãã ã•ã„:
1. æ„Ÿè¬ã®è¨€è‘‰ã‹ã‚‰å§‹ã‚ã‚‹
2. ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’çœŸæ‘¯ã«å—ã‘æ­¢ã‚ã‚‹å§¿å‹¢
3. ç°¡æ½”ã‹ã¤ãƒã‚¸ãƒ†ã‚£ãƒ–ãªå¿œç­”
        """),
        HumanMessage(content=state["message"])
    ]
    
    response = llm.invoke(messages)
    
    return {
        "response": f"ã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å¯¾å¿œã€‘\n{response.content}\n\nè²´é‡ãªã”æ„è¦‹ã‚’ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ï¼"
    }

# ã‚°ãƒ©ãƒ•æ§‹ç¯‰
workflow = StateGraph(SupportState)

# ãƒãƒ¼ãƒ‰ç™»éŒ²
workflow.add_node("analyze", analyze_message)
workflow.add_node("high", handle_high_priority)
workflow.add_node("medium", handle_medium_priority)
workflow.add_node("low", handle_low_priority)

# ãƒ•ãƒ­ãƒ¼å®šç¾©
workflow.add_edge(START, "analyze")

workflow.add_conditional_edges(
    source="analyze",
    path=route_by_priority,
    path_map={
        "high": "high",
        "medium": "medium",
        "low": "low"
    }
)

workflow.add_edge("high", END)
workflow.add_edge("medium", END)
workflow.add_edge("low", END)

app = workflow.compile()

# ã‚°ãƒ©ãƒ•å¯è¦–åŒ–
try:
    png_data = app.get_graph().draw_mermaid_png()
    with open("support_bot_graph.png", "wb") as f:
        f.write(png_data)
    print("âœ… ã‚°ãƒ©ãƒ•ã‚’ 'support_bot_graph.png' ã«ä¿å­˜ã—ã¾ã—ãŸ")
except Exception as e:
    print(f"âš ï¸ PNGä¿å­˜å¤±æ•—: {e}")
    print("\nASCIIç‰ˆã‚°ãƒ©ãƒ•:")
    print(app.get_graph().draw_ascii())

# ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
test_messages = [
    "å•†å“ãŒ2é€±é–“çµŒã£ã¦ã‚‚å±Šãã¾ã›ã‚“ï¼æ³¨æ–‡ç•ªå·12345ã§ã™ã€‚ã™ãã«ç¢ºèªã—ã¦ãã ã•ã„ï¼",
    "è£½å“Aã®è¨­å®šæ–¹æ³•ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚",
    "ã“ã®ã‚µãƒ¼ãƒ“ã‚¹ã€æœ¬å½“ã«ä¾¿åˆ©ã§ã™ï¼ã„ã¤ã‚‚ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚",
    "ãƒ€ãƒ¼ã‚¯ãƒ¢ãƒ¼ãƒ‰æ©Ÿèƒ½ã‚’è¿½åŠ ã—ã¦ã»ã—ã„ã§ã™ã€‚",
    "ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãƒªã‚»ãƒƒãƒˆã®æ–¹æ³•ãŒã‚ã‹ã‚Šã¾ã›ã‚“ã€‚"
]

print("\n" + "="*80)
print("ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒœãƒƒãƒˆ - ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
print("="*80)

for i, msg in enumerate(test_messages, 1):
    result = app.invoke({
        "message": msg,
        "intent": "",
        "sentiment": "",
        "priority": "",
        "response": "",
        "analysis_detail": ""
    })
    
    print(f"\nã€ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ {i}ã€‘")
    print(f"ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {msg}")
    print(f"åˆ†æçµæœ: {result['analysis_detail']}")
    print(f"\nå¿œç­”:\n{result['response']}")
    print("-" * 80)
```

**å®Ÿè¡Œçµæœä¾‹**:
```
================================================================================
ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒœãƒƒãƒˆ - ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
================================================================================

ã€ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ 1ã€‘
ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: å•†å“ãŒ2é€±é–“çµŒã£ã¦ã‚‚å±Šãã¾ã›ã‚“ï¼æ³¨æ–‡ç•ªå·12345ã§ã™ã€‚ã™ãã«ç¢ºèªã—ã¦ãã ã•ã„ï¼
åˆ†æçµæœ: æ„å›³=complaint, æ„Ÿæƒ…=negative, å„ªå…ˆåº¦=high

å¿œç­”:
ã€ç·Šæ€¥å¯¾å¿œã€‘
å¤§å¤‰ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ã”æ³¨æ–‡ã®å•†å“ãŒã¾ã ãŠæ‰‹å…ƒã«å±Šã„ã¦ã„ãªã„ã¨ã®ã“ã¨ã€å¿ƒã‚ˆã‚ŠãŠè©«ã³ç”³ã—ä¸Šã’ã¾ã™ã€‚

æ³¨æ–‡ç•ªå·12345ã«ã¤ã„ã¦ã€ç›´ã¡ã«é…é€çŠ¶æ³ã‚’ç¢ºèªã•ã›ã¦ã„ãŸã ãã¾ã™...
ï¼ˆçœç•¥ï¼‰

â€»ã“ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯å„ªå…ˆçš„ã«å‡¦ç†ã•ã‚Œã€æ‹…å½“è€…ã«å³åº§ã«ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã•ã‚Œã¾ã—ãŸã€‚
--------------------------------------------------------------------------------

ã€ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ 2ã€‘
ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: è£½å“Aã®è¨­å®šæ–¹æ³•ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚
åˆ†æçµæœ: æ„å›³=inquiry, æ„Ÿæƒ…=neutral, å„ªå…ˆåº¦=medium

å¿œç­”:
ã€æ¨™æº–å¯¾å¿œã€‘
è£½å“Aã®è¨­å®šæ–¹æ³•ã«ã¤ã„ã¦ã”æ¡ˆå†…ã„ãŸã—ã¾ã™...
ï¼ˆçœç•¥ï¼‰

ã•ã‚‰ã«ã”ä¸æ˜ãªç‚¹ãŒã”ã–ã„ã¾ã—ãŸã‚‰ã€ãŠæ°—è»½ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚
--------------------------------------------------------------------------------
```

---

### 3-6. æ¡ä»¶åˆ†å²ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

#### âœ… DOï¼ˆæ¨å¥¨ï¼‰

**1. ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é–¢æ•°ã¯ã‚·ãƒ³ãƒ—ãƒ«ã«**
```python
# âœ… è‰¯ã„ä¾‹
def route(state: State) -> Literal["a", "b", "c"]:
    return state["category"]  # Stateã‹ã‚‰èª­ã¿å–ã‚‹ã ã‘
```

**2. å‹ãƒ’ãƒ³ãƒˆã‚’å¿…ãšä½¿ã†**
```python
# âœ… è‰¯ã„ä¾‹
def route(state: State) -> Literal["route_a", "route_b"]:
    return state["route"]

# âŒ æ‚ªã„ä¾‹
def route(state):  # å‹ãƒ’ãƒ³ãƒˆãªã—
    return state["route"]
```

**3. ã™ã¹ã¦ã®ã‚±ãƒ¼ã‚¹ã‚’ã‚«ãƒãƒ¼**
```python
# âœ… è‰¯ã„ä¾‹
workflow.add_conditional_edges("node", route, {
    "case_a": "handler_a",
    "case_b": "handler_b",
    "case_c": "handler_c"  # ã™ã¹ã¦ã®ã‚±ãƒ¼ã‚¹
})
```

**4. ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚’å®Ÿè£…**
```python
# âœ… è‰¯ã„ä¾‹
def classify(state: State) -> dict:
    response = llm.invoke([...])
    category = response.content.strip().lower()
    
    # äºˆæœŸã—ãªã„å€¤ã¸ã®å¯¾ç­–
    if category not in ["a", "b", "c"]:
        category = "default"
    
    return {"category": category}
```

---

#### âŒ DON'Tï¼ˆéæ¨å¥¨ï¼‰

**1. ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é–¢æ•°ã§è¤‡é›‘ãªãƒ­ã‚¸ãƒƒã‚¯**
```python
# âŒ æ‚ªã„ä¾‹
def route(state: State) -> Literal["a", "b"]:
    # ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é–¢æ•°å†…ã§è¤‡é›‘ãªå‡¦ç†
    result = complex_analysis(state["input"])
    if result > 0.5:
        return "a"
    return "b"
```

**2. ãƒãƒƒãƒ”ãƒ³ã‚°ã‚­ãƒ¼ã¨æˆ»ã‚Šå€¤ã®ä¸ä¸€è‡´**
```python
# âŒ æ‚ªã„ä¾‹
def route(state: State) -> Literal["yes", "no"]:
    return "yes" if state["flag"] else "no"

workflow.add_conditional_edges("node", route, {
    "true": "node_a",   # ã‚­ãƒ¼ãŒåˆã‚ãªã„
    "false": "node_b"
})
```

**3. ä¸€éƒ¨ã®ã‚±ãƒ¼ã‚¹ãŒæŠœã‘ã¦ã„ã‚‹**
```python
# âŒ æ‚ªã„ä¾‹
def route(state: State) -> Literal["a", "b", "c"]:
    return state["category"]

workflow.add_conditional_edges("node", route, {
    "a": "handler_a",
    "b": "handler_b"
    # "c"ãŒæŠœã‘ã¦ã„ã‚‹ â†’ å®Ÿè¡Œæ™‚ã‚¨ãƒ©ãƒ¼
})
```

---

## ç¬¬4ç« : ãƒ„ãƒ¼ãƒ«çµ±åˆã€å®Œå…¨è§£èª¬ã€‘

### 4-1. Tavilyæ¤œç´¢APIã®å®Œå…¨ç†è§£

#### Tavilyæ¤œç´¢ã¨ã¯ä½•ã‹

**å…¬å¼èª¬æ˜**:
Tavilyã¯ã€AIå‘ã‘ã«æœ€é©åŒ–ã•ã‚ŒãŸæ¤œç´¢APIã§ã™ã€‚é€šå¸¸ã®æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã¨ç•°ãªã‚Šã€LLMãŒç†è§£ã—ã‚„ã™ã„ã‚¯ãƒªãƒ¼ãƒ³ã§æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã—ã¾ã™ã€‚

**ç‰¹å¾´**:
- ğŸš€ é«˜é€Ÿ: ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ãŒçŸ­ã„ï¼ˆé€šå¸¸1-2ç§’ï¼‰
- ğŸ¯ æ­£ç¢º: AIå‘ã‘ã«æœ€é©åŒ–ã•ã‚ŒãŸé–¢é€£æ€§ã®é«˜ã„çµæœ
- ğŸ’° ç„¡æ–™æ : æœˆé–“1,000ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¾ã§ç„¡æ–™
- ğŸ”§ ç°¡å˜: LangChainã¨å®Œå…¨çµ±åˆ

**ä»–ã®æ¤œç´¢APIã¨ã®æ¯”è¼ƒ**:
| é …ç›® | Tavily | Google Search API | Bing Search API |
|------|--------|-------------------|-----------------|
| ç„¡æ–™æ  | 1,000req/æœˆ | ãªã— | ãªã— |
| AIæœ€é©åŒ– | â­â­â­ | â­ | â­ |
| LangChainçµ±åˆ | ãƒã‚¤ãƒ†ã‚£ãƒ– | è¦ã‚«ã‚¹ã‚¿ãƒ  | è¦ã‚«ã‚¹ã‚¿ãƒ  |

---

#### Tavily APIã‚­ãƒ¼ã®å–å¾—ï¼ˆè©³ç´°æ‰‹é †ï¼‰

**ã‚¹ãƒ†ãƒƒãƒ—1**: https://tavily.com/ ã«ã‚¢ã‚¯ã‚»ã‚¹

**ã‚¹ãƒ†ãƒƒãƒ—2**: ã€ŒGet Startedã€ã¾ãŸã¯ã€ŒSign Upã€ã‚’ã‚¯ãƒªãƒƒã‚¯

**ã‚¹ãƒ†ãƒƒãƒ—3**: Googleã‚¢ã‚«ã‚¦ãƒ³ãƒˆã¾ãŸã¯ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã§ç™»éŒ²

**ã‚¹ãƒ†ãƒƒãƒ—4**: ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸAPIã‚­ãƒ¼ã‚’ã‚³ãƒ”ãƒ¼
- ã‚­ãƒ¼ã¯ `tvly-` ã§å§‹ã¾ã‚‹æ–‡å­—åˆ—
- ä¾‹: `tvly-Abc123XyZ789...`

**ã‚¹ãƒ†ãƒƒãƒ—5**: APIã‚­ãƒ¼ã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®š
```python
import os
os.environ["TAVILY_API_KEY"] = "tvly-your-actual-key"
```

ã¾ãŸã¯`.env`ãƒ•ã‚¡ã‚¤ãƒ«ã«:
```
TAVILY_API_KEY=tvly-your-actual-key
```

---

#### Tavilyæ¤œç´¢ã®åŸºæœ¬çš„ãªä½¿ã„æ–¹

**æœ€å°å®Ÿè£…**:
```python
import os
from langchain_community.tools.tavily_search import TavilySearchResults

# APIã‚­ãƒ¼è¨­å®š
os.environ["TAVILY_API_KEY"] = "tvly-your-key"

# æ¤œç´¢ãƒ„ãƒ¼ãƒ«åˆæœŸåŒ–
search = TavilySearchResults(
    max_results=3  # å–å¾—ã™ã‚‹çµæœã®æ•°
)

# æ¤œç´¢å®Ÿè¡Œ
results = search.invoke("LangGraphã¨ã¯")

# çµæœè¡¨ç¤º
for i, result in enumerate(results, 1):
    print(f"\nçµæœ {i}:")
    print(f"URL: {result['url']}")
    print(f"å†…å®¹: {result['content'][:100]}...")
    print(f"ã‚¹ã‚³ã‚¢: {result.get('score', 'N/A')}")
```

**å®Ÿè¡Œçµæœã®æ§‹é€ **:
```python
[
    {
        'url': 'https://example.com/langgraph-intro',
        'content': 'LangGraphã¯ã€LangChainã®ä¸Šã«æ§‹ç¯‰ã•ã‚ŒãŸãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§...',
        'score': 0.95  # é–¢é€£æ€§ã‚¹ã‚³ã‚¢ï¼ˆ0.0ã€œ1.0ï¼‰
    },
    {
        'url': 'https://another-site.com/tutorial',
        'content': 'LangGraphã‚’ä½¿ã†ã“ã¨ã§ã€è¤‡é›‘ãªAIãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼...',
        'score': 0.87
    },
    ...
]
```

---

#### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è©³ç´°è§£èª¬

```python
search = TavilySearchResults(
    max_results=5,           # çµæœæ•°ï¼ˆ1ã€œ10æ¨å¥¨ï¼‰
    search_depth="basic",    # "basic" or "advanced"
    include_domains=[],      # ç‰¹å®šãƒ‰ãƒ¡ã‚¤ãƒ³ã®ã¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    exclude_domains=[],      # é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    include_answer=False,    # è¦ç´„ã‚’å«ã‚ã‚‹ã‹
    include_raw_content=False  # ç”Ÿã®HTMLã‚’å«ã‚ã‚‹ã‹
)
```

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è©³ç´°**:

1. **max_results**:
   - å–å¾—ã™ã‚‹æ¤œç´¢çµæœã®æ•°
   - æ¨å¥¨: 3ã€œ5ï¼ˆãƒãƒ©ãƒ³ã‚¹é‡è¦–ï¼‰
   - å¤šã™ãã‚‹ã¨LLMã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’åœ§è¿«

2. **search_depth**:
   - `"basic"`: é«˜é€Ÿã€è¡¨é¢çš„ãªæƒ…å ±
   - `"advanced"`: è©³ç´°ã€æ·±ã„åˆ†æï¼ˆé…ã„ï¼‰
   - æ¨å¥¨: é€šå¸¸ã¯`"basic"`ã§ååˆ†

3. **include_domains** (ä¾‹):
   ```python
   search = TavilySearchResults(
       max_results=3,
       include_domains=["wikipedia.org", "github.com"]
   )
   ```

4. **exclude_domains** (ä¾‹):
   ```python
   search = TavilySearchResults(
       max_results=3,
       exclude_domains=["ads-site.com", "spam.com"]
   )
   ```

---

### 4-2. LLMã¨ãƒ„ãƒ¼ãƒ«ã®çµ±åˆï¼ˆbind_toolsï¼‰

#### bind_toolsã®ä»•çµ„ã¿

**æ¦‚å¿µ**:
`bind_tools()`ã¯ã€LLMã«ã€Œã“ã®ãƒ„ãƒ¼ãƒ«ãŒä½¿ãˆã¾ã™ã‚ˆã€ã¨æ•™ãˆã‚‹æ©Ÿèƒ½ã§ã™ã€‚LLMã¯å¿…è¦ã«å¿œã˜ã¦ãƒ„ãƒ¼ãƒ«ã®ä½¿ç”¨ã‚’åˆ¤æ–­ã—ã¾ã™ã€‚

**ãƒ•ãƒ­ãƒ¼**:
```
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€Œ2024å¹´ã®ãƒãƒ¼ãƒ™ãƒ«è³å—è³è€…ã¯ï¼Ÿã€
  â†“
LLMã€Œã“ã‚Œã¯æœ€æ–°æƒ…å ±ãªã®ã§æ¤œç´¢ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ãŠã†ã€
  â†“
tool_calls ã‚’ç”Ÿæˆ
  â†“
ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œï¼ˆTavilyæ¤œç´¢ï¼‰
  â†“
çµæœã‚’LLMã«æ¸¡ã™
  â†“
LLMã€Œæ¤œç´¢çµæœã‚’ã‚‚ã¨ã«å›ç­”ã‚’ç”Ÿæˆã€
```

---

#### åŸºæœ¬çš„ãªbind_toolsã®ä½¿ã„æ–¹

```python
import os
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.messages import HumanMessage

# APIã‚­ãƒ¼è¨­å®š
os.environ["GEMINI_API_KEY"] = "your-key"
os.environ["TAVILY_API_KEY"] = "your-key"

# LLMåˆæœŸåŒ–
llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-exp",
    temperature=0
)

# ãƒ„ãƒ¼ãƒ«åˆæœŸåŒ–
search_tool = TavilySearchResults(max_results=3)

# ãƒ„ãƒ¼ãƒ«ã‚’LLMã«ãƒã‚¤ãƒ³ãƒ‰
llm_with_tools = llm.bind_tools([search_tool])

# å®Ÿè¡Œ
response = llm_with_tools.invoke([
    HumanMessage(content="2024å¹´ã®ãƒãƒ¼ãƒ™ãƒ«ç‰©ç†å­¦è³å—è³è€…ã¯ï¼Ÿ")
])

# ãƒ„ãƒ¼ãƒ«ä½¿ç”¨åˆ¤å®š
if hasattr(response, "tool_calls") and response.tool_calls:
    print("âœ… LLMãŒãƒ„ãƒ¼ãƒ«ä½¿ç”¨ã‚’æ±ºå®šã—ã¾ã—ãŸ")
    print(f"ãƒ„ãƒ¼ãƒ«å: {response.tool_calls[0]['name']}")
    print(f"å¼•æ•°: {response.tool_calls[0]['args']}")
else:
    print("âŒ LLMã¯ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã›ã‚“ã§ã—ãŸ")
    print(f"ç›´æ¥å›ç­”: {response.content}")
```

**é‡è¦ãªç†è§£**:
- `bind_tools()`ã¯**ãƒªã‚¹ãƒˆã‚’å—ã‘å–ã‚‹**ï¼ˆè¤‡æ•°ãƒ„ãƒ¼ãƒ«å¯¾å¿œï¼‰
- LLMãŒè‡ªå‹•çš„ã«ãƒ„ãƒ¼ãƒ«ã®ä½¿ç”¨ã‚’åˆ¤æ–­
- `tool_calls`å±æ€§ã®æœ‰ç„¡ã§ãƒ„ãƒ¼ãƒ«ä½¿ç”¨ã‚’ç¢ºèª

---

#### tool_callsã®è©³ç´°

```python
# tool_callsã®æ§‹é€ 
response.tool_calls = [
    {
        'name': 'tavily_search_results_json',
        'args': {'query': '2024 ãƒãƒ¼ãƒ™ãƒ«ç‰©ç†å­¦è³'},
        'id': 'call_abc123'
    }
]
```

**ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰è§£èª¬**:
- `name`: ä½¿ç”¨ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã®åå‰
- `args`: ãƒ„ãƒ¼ãƒ«ã«æ¸¡ã™å¼•æ•°ï¼ˆdictå½¢å¼ï¼‰
- `id`: ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã®ä¸€æ„ID

---

### 4-3. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã®å®Œå…¨å®Ÿè£…

#### ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã¨ã¯

**å®šç¾©**:
LLMã¨ãƒ„ãƒ¼ãƒ«ãŒ**å”èª¿å‹•ä½œ**ã—ã¦ã€ã‚¿ã‚¹ã‚¯ã‚’å®Œé‚ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã™ã€‚

**ãƒ•ãƒ­ãƒ¼å›³**:
```
START
  â†“
agent (LLMãŒãƒ„ãƒ¼ãƒ«ä½¿ç”¨ã‚’åˆ¤æ–­)
  â†“
ãƒ„ãƒ¼ãƒ«å¿…è¦? 
  â”œâ”€ YES â†’ tools (æ¤œç´¢å®Ÿè¡Œ) â†’ agent (çµæœã§å†è©•ä¾¡) â†’ ...
  â””â”€ NO  â†’ END (æœ€çµ‚å›ç­”)
```

**é‡è¦ãªæ¦‚å¿µ**:
- **ãƒ«ãƒ¼ãƒ—**: `tools â†’ agent` ã®ç¹°ã‚Šè¿”ã—
- **çµ‚äº†æ¡ä»¶**: `tool_calls`ãŒãªã„ = ã‚¿ã‚¹ã‚¯å®Œäº†

---

#### messagesãƒ™ãƒ¼ã‚¹ã®Stateè¨­è¨ˆ

**ãªãœmessagesãƒ™ãƒ¼ã‚¹?**
ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã¯ã€LLMã¨ãƒ„ãƒ¼ãƒ«ã®å¾€å¾©ãŒç™ºç”Ÿã—ã¾ã™ã€‚ä¼šè©±å±¥æ­´ã‚’ä¿æŒã™ã‚‹ãŸã‚ã«`messages`ã‚’Stateã«æŒã¡ã¾ã™ã€‚

```python
from typing import TypedDict, Annotated
import operator

class AgentState(TypedDict):
    messages: Annotated[list, operator.add]
```

**Annotatedã®æ„å‘³**:
```python
messages: Annotated[list, operator.add]
#         ^^^^^^^^^^^^^^^^^^^^^^^^
#         å‹ãƒ’ãƒ³ãƒˆ + ãƒãƒ¼ã‚¸æ–¹æ³•ã®æŒ‡å®š
```

- `list`: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ
- `operator.add`: æ–°ã—ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’**è¿½åŠ **ï¼ˆä¸Šæ›¸ãã§ã¯ãªã„ï¼‰

**å‹•ä½œä¾‹**:
```python
# åˆæœŸState
{"messages": [HumanMessage("è³ªå•")]}

# ãƒãƒ¼ãƒ‰AãŒå®Ÿè¡Œ
return {"messages": [AIMessage("å›ç­”A")]}

# ãƒãƒ¼ã‚¸å¾Œï¼ˆaddãªã®ã§è¿½åŠ ï¼‰
{"messages": [HumanMessage("è³ªå•"), AIMessage("å›ç­”A")]}

# ãƒãƒ¼ãƒ‰BãŒå®Ÿè¡Œ
return {"messages": [ToolMessage("æ¤œç´¢çµæœ")]}

# ãƒãƒ¼ã‚¸å¾Œ
{"messages": [HumanMessage("è³ªå•"), AIMessage("å›ç­”A"), ToolMessage("æ¤œç´¢çµæœ")]}
```

---

#### ToolNodeã®ç†è§£

**ToolNodeã¨ã¯**:
LangGraphãŒæä¾›ã™ã‚‹**çµ„ã¿è¾¼ã¿ãƒãƒ¼ãƒ‰**ã§ã€ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã‚’è‡ªå‹•å‡¦ç†ã—ã¾ã™ã€‚

```python
from langgraph.prebuilt import ToolNode

tools = [search_tool]
tool_node = ToolNode(tools)
```

**ToolNodeã®å‹•ä½œ**:
1. `messages`ã‹ã‚‰æœ€æ–°ã®`tool_calls`ã‚’å–å¾—
2. è©²å½“ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè¡Œ
3. çµæœã‚’`ToolMessage`ã¨ã—ã¦è¿”ã™

**åˆ©ç‚¹**:
- æ‰‹å‹•ã§ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã‚³ãƒ¼ãƒ‰ã‚’æ›¸ãå¿…è¦ãŒãªã„
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãŒè‡ªå‹•
- è¤‡æ•°ãƒ„ãƒ¼ãƒ«ã«å¯¾å¿œ

---

#### å®Œå…¨ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£…ï¼ˆã‚³ãƒ”ãƒšå¯èƒ½ï¼‰

```python
import os
from typing import TypedDict, Annotated, Literal
import operator
from langgraph.graph import StateGraph, START, END
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.messages import HumanMessage
from langgraph.prebuilt import ToolNode

# APIã‚­ãƒ¼è¨­å®š
os.environ["GEMINI_API_KEY"] = "your-key"
os.environ["TAVILY_API_KEY"] = "your-key"

# Stateå®šç¾©
class AgentState(TypedDict):
    messages: Annotated[list, operator.add]

# ãƒ„ãƒ¼ãƒ«åˆæœŸåŒ–
search_tool = TavilySearchResults(
    max_results=3,
    search_depth="advanced"  # è©³ç´°ãªæ¤œç´¢
)

tools = [search_tool]

# LLMåˆæœŸåŒ–ã¨ãƒ„ãƒ¼ãƒ«ãƒã‚¤ãƒ³ãƒ‰
llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-exp",
    temperature=0  # æ¤œç´¢ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯æ±ºå®šçš„ã«
)
llm_with_tools = llm.bind_tools(tools)

# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒãƒ¼ãƒ‰
def agent_node(state: AgentState) -> dict:
    """LLMãŒãƒ„ãƒ¼ãƒ«ä½¿ç”¨ã‚’åˆ¤æ–­"""
    messages = state["messages"]
    response = llm_with_tools.invoke(messages)
    return {"messages": [response]}

# ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é–¢æ•°
def should_continue(state: AgentState) -> Literal["tools", "end"]:
    """ãƒ„ãƒ¼ãƒ«ä½¿ç”¨ãŒå¿…è¦ã‹åˆ¤å®š"""
    last_message = state["messages"][-1]
    
    # tool_callsãŒã‚ã‚Œã°ãƒ„ãƒ¼ãƒ«ãƒãƒ¼ãƒ‰ã¸
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tools"
    
    # ãªã‘ã‚Œã°çµ‚äº†
    return "end"

# ã‚°ãƒ©ãƒ•æ§‹ç¯‰
workflow = StateGraph(AgentState)

# ãƒãƒ¼ãƒ‰è¿½åŠ 
workflow.add_node("agent", agent_node)
workflow.add_node("tools", ToolNode(tools))

# ã‚¨ãƒƒã‚¸è¿½åŠ 
workflow.add_edge(START, "agent")

# æ¡ä»¶åˆ†å²
workflow.add_conditional_edges(
    source="agent",
    path=should_continue,
    path_map={
        "tools": "tools",
        "end": END
    }
)

# ãƒ„ãƒ¼ãƒ« â†’ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ«ãƒ¼ãƒ—
workflow.add_edge("tools", "agent")

# ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
app = workflow.compile()

# ã‚°ãƒ©ãƒ•å¯è¦–åŒ–
try:
    png_data = app.get_graph().draw_mermaid_png()
    with open("search_agent_graph.png", "wb") as f:
        f.write(png_data)
    print("âœ… ã‚°ãƒ©ãƒ•ã‚’ 'search_agent_graph.png' ã«ä¿å­˜ã—ã¾ã—ãŸ")
except:
    print("ASCIIç‰ˆã‚°ãƒ©ãƒ•:")
    print(app.get_graph().draw_ascii())

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
print("\n" + "="*80)
print("æ¤œç´¢ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ - ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
print("="*80)

test_queries = [
    "2024å¹´ã®ãƒãƒ¼ãƒ™ãƒ«ç‰©ç†å­¦è³å—è³è€…ã‚’æ•™ãˆã¦",
    "LangGraphã®æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯ï¼Ÿ",
    "ã“ã‚“ã«ã¡ã¯"  # æ¤œç´¢ä¸è¦ã®ã‚±ãƒ¼ã‚¹
]

for query in test_queries:
    print(f"\nã€è³ªå•ã€‘ {query}")
    
    result = app.invoke({
        "messages": [HumanMessage(content=query)]
    })
    
    # æœ€çµ‚å›ç­”ã‚’å–å¾—
    final_message = result["messages"][-1]
    print(f"ã€å›ç­”ã€‘ {final_message.content[:200]}...")
    
    # ãƒ„ãƒ¼ãƒ«ä½¿ç”¨çŠ¶æ³ã‚’ç¢ºèª
    tool_used = any(
        hasattr(msg, "tool_calls") and msg.tool_calls
        for msg in result["messages"]
    )
    print(f"ã€ãƒ„ãƒ¼ãƒ«ä½¿ç”¨ã€‘ {'ã‚ã‚Š' if tool_used else 'ãªã—'}")
    print("-" * 80)
```

**å®Ÿè¡Œçµæœä¾‹**:
```
================================================================================
æ¤œç´¢ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ - ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
================================================================================

ã€è³ªå•ã€‘ 2024å¹´ã®ãƒãƒ¼ãƒ™ãƒ«ç‰©ç†å­¦è³å—è³è€…ã‚’æ•™ãˆã¦
ã€å›ç­”ã€‘ 2024å¹´ã®ãƒãƒ¼ãƒ™ãƒ«ç‰©ç†å­¦è³ã¯ã€John J. Hopfieldã¨Geoffrey E. HintonãŒå—è³ã—ã¾ã—ãŸã€‚å½¼ã‚‰ã¯äººå·¥ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç”¨ã„ãŸæ©Ÿæ¢°å­¦ç¿’ã®åŸºç¤çš„ãªç™ºè¦‹ã¨ç™ºæ˜ã«ã‚ˆã‚Šå—è³ã—ã¾ã—ãŸ...
ã€ãƒ„ãƒ¼ãƒ«ä½¿ç”¨ã€‘ ã‚ã‚Š
--------------------------------------------------------------------------------

ã€è³ªå•ã€‘ LangGraphã®æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯ï¼Ÿ
ã€å›ç­”ã€‘ æ¤œç´¢çµæœã«ã‚ˆã‚‹ã¨ã€LangGraphã®æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯0.2.xã‚·ãƒªãƒ¼ã‚ºã§ã™...
ã€ãƒ„ãƒ¼ãƒ«ä½¿ç”¨ã€‘ ã‚ã‚Š
--------------------------------------------------------------------------------

ã€è³ªå•ã€‘ ã“ã‚“ã«ã¡ã¯
ã€å›ç­”ã€‘ ã“ã‚“ã«ã¡ã¯ï¼ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ
ã€ãƒ„ãƒ¼ãƒ«ä½¿ç”¨ã€‘ ãªã—
--------------------------------------------------------------------------------
```

---

#### ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã®ãƒ‡ãƒãƒƒã‚°

**ãƒ«ãƒ¼ãƒ—ã®é€²è¡Œã‚’ç¢ºèª**:
```python
# stream()ã§å„ã‚¹ãƒ†ãƒƒãƒ—ã‚’è¦³å¯Ÿ
for step in app.stream({"messages": [HumanMessage("è³ªå•")]}):
    print(step)
```

**å‡ºåŠ›ä¾‹**:
```python
{'agent': {'messages': [AIMessage(content='', tool_calls=[...])]}}
{'tools': {'messages': [ToolMessage(content='æ¤œç´¢çµæœ...')]}}
{'agent': {'messages': [AIMessage(content='æœ€çµ‚å›ç­”...')]}}
```

**ãƒ«ãƒ¼ãƒ—å›æ•°ã®åˆ¶é™**ï¼ˆç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢ï¼‰:
```python
# æœ€å¤§5ã‚¹ãƒ†ãƒƒãƒ—ã§å¼·åˆ¶çµ‚äº†
result = app.invoke(
    {"messages": [HumanMessage("è³ªå•")]},
    config={"recursion_limit": 5}
)
```

---

### 4-4. è¤‡æ•°ãƒ„ãƒ¼ãƒ«ã®çµ±åˆ

#### ã‚«ã‚¹ã‚¿ãƒ ãƒ„ãƒ¼ãƒ«ã®ä½œæˆ

LangChainã®`@tool`ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã§ç°¡å˜ã«ä½œæˆã§ãã¾ã™ã€‚

```python
from langchain_core.tools import tool

@tool
def calculate(expression: str) -> str:
    """æ•°å¼ã‚’è¨ˆç®—ã—ã¾ã™ã€‚
    
    Args:
        expression: è¨ˆç®—ã™ã‚‹æ•°å¼ï¼ˆä¾‹: "2 + 2", "10 * 5"ï¼‰
    
    Returns:
        è¨ˆç®—çµæœã®æ–‡å­—åˆ—
    """
    try:
        # æ³¨æ„: eval()ã¯å®Ÿé‹ç”¨ã§ã¯å±é™ºã€‚ã“ã“ã§ã¯ãƒ‡ãƒ¢ã®ã¿
        result = eval(expression)
        return str(result)
    except Exception as e:
        return f"è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}"

@tool
def get_current_time() -> str:
    """ç¾åœ¨ã®æ—¥æ™‚ã‚’å–å¾—ã—ã¾ã™ã€‚
    
    Returns:
        ç¾åœ¨ã®æ—¥æ™‚ï¼ˆæ—¥æœ¬æ™‚é–“ï¼‰
    """
    from datetime import datetime
    import pytz
    
    jst = pytz.timezone('Asia/Tokyo')
    now = datetime.now(jst)
    return now.strftime("%Yå¹´%mæœˆ%dæ—¥ %Hæ™‚%Måˆ†%Sç§’")
```

**é‡è¦ãƒã‚¤ãƒ³ãƒˆ**:
1. **docstringå¿…é ˆ**: LLMãŒãƒ„ãƒ¼ãƒ«ã®ç”¨é€”ã‚’ç†è§£ã™ã‚‹ãŸã‚
2. **å‹ãƒ’ãƒ³ãƒˆå¿…é ˆ**: å¼•æ•°ã¨æˆ»ã‚Šå€¤ã®å‹ã‚’æ˜ç¤º
3. **æˆ»ã‚Šå€¤ã¯æ–‡å­—åˆ—**: LLMãŒè§£é‡ˆã—ã‚„ã™ã„å½¢å¼

---

#### è¤‡æ•°ãƒ„ãƒ¼ãƒ«ã‚’æŒã¤ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

```python
import os
from typing import TypedDict, Annotated, Literal
import operator
from langgraph.graph import StateGraph, START, END
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.messages import HumanMessage
from langchain_core.tools import tool
from langgraph.prebuilt import ToolNode

os.environ["GEMINI_API_KEY"] = "your-key"
os.environ["TAVILY_API_KEY"] = "your-key"

# Stateå®šç¾©
class MultiToolState(TypedDict):
    messages: Annotated[list, operator.add]

# ãƒ„ãƒ¼ãƒ«å®šç¾©
search_tool = TavilySearchResults(max_results=3)

@tool
def calculate(expression: str) -> str:
    """æ•°å¼ã‚’è¨ˆç®—"""
    try:
        return str(eval(expression))
    except Exception as e:
        return f"ã‚¨ãƒ©ãƒ¼: {e}"

@tool
def get_weather(city: str) -> str:
    """å¤©æ°—æƒ…å ±ã‚’å–å¾—ï¼ˆãƒ‡ãƒ¢ç”¨ã®ãƒ€ãƒŸãƒ¼å®Ÿè£…ï¼‰"""
    # å®Ÿéš›ã¯APIå‘¼ã³å‡ºã—ãªã©
    return f"{city}ã®å¤©æ°—: æ™´ã‚Œã€æ°—æ¸©23åº¦"

# ã™ã¹ã¦ã®ãƒ„ãƒ¼ãƒ«ã‚’ãƒªã‚¹ãƒˆåŒ–
tools = [search_tool, calculate, get_weather]

# LLMåˆæœŸåŒ–
llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash-exp", temperature=0)
llm_with_tools = llm.bind_tools(tools)

# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒãƒ¼ãƒ‰
def agent(state: MultiToolState) -> dict:
    response = llm_with_tools.invoke(state["messages"])
    return {"messages": [response]}

# ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
def should_continue(state: MultiToolState) -> Literal["tools", "end"]:
    last_msg = state["messages"][-1]
    if hasattr(last_msg, "tool_calls") and last_msg.tool_calls:
        return "tools"
    return "end"

# ã‚°ãƒ©ãƒ•æ§‹ç¯‰
workflow = StateGraph(MultiToolState)
workflow.add_node("agent", agent)
workflow.add_node("tools", ToolNode(tools))

workflow.add_edge(START, "agent")
workflow.add_conditional_edges("agent", should_continue, {
    "tools": "tools",
    "end": END
})
workflow.add_edge("tools", "agent")

app = workflow.compile()

# ãƒ†ã‚¹ãƒˆ
test_queries = [
    "æ±äº¬ã®å¤©æ°—ã¯ï¼Ÿ",
    "125 Ã— 48ã®è¨ˆç®—ã‚’ã—ã¦",
    "Pythonã®æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’èª¿ã¹ã¦",
    "å¤§é˜ªã®å¤©æ°—ã‚’èª¿ã¹ã¦ã€ãã®æ°—æ¸©ã‚’è¯æ°ã«å¤‰æ›ã—ã¦"  # è¤‡æ•°ãƒ„ãƒ¼ãƒ«ä½¿ç”¨
]

print("="*80)
print("ãƒãƒ«ãƒãƒ„ãƒ¼ãƒ«ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ - ãƒ†ã‚¹ãƒˆ")
print("="*80)

for query in test_queries:
    print(f"\nã€è³ªå•ã€‘ {query}")
    
    result = app.invoke({"messages": [HumanMessage(content=query)]})
    
    # ä½¿ç”¨ã•ã‚ŒãŸãƒ„ãƒ¼ãƒ«ã‚’ç¢ºèª
    tools_used = []
    for msg in result["messages"]:
        if hasattr(msg, "tool_calls") and msg.tool_calls:
            for tc in msg.tool_calls:
                tools_used.append(tc["name"])
    
    print(f"ã€ä½¿ç”¨ãƒ„ãƒ¼ãƒ«ã€‘ {', '.join(set(tools_used)) if tools_used else 'ãªã—'}")
    print(f"ã€å›ç­”ã€‘ {result['messages'][-1].content[:150]}...")
    print("-" * 80)
```

**å®Ÿè¡Œçµæœä¾‹**:
```
ã€è³ªå•ã€‘ æ±äº¬ã®å¤©æ°—ã¯ï¼Ÿ
ã€ä½¿ç”¨ãƒ„ãƒ¼ãƒ«ã€‘ get_weather
ã€å›ç­”ã€‘ æ±äº¬ã®å¤©æ°—: æ™´ã‚Œã€æ°—æ¸©23åº¦

ã€è³ªå•ã€‘ 125 Ã— 48ã®è¨ˆç®—ã‚’ã—ã¦
ã€ä½¿ç”¨ãƒ„ãƒ¼ãƒ«ã€‘ calculate
ã€å›ç­”ã€‘ 125 Ã— 48 = 6000ã§ã™ã€‚

ã€è³ªå•ã€‘ Pythonã®æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’èª¿ã¹ã¦
ã€ä½¿ç”¨ãƒ„ãƒ¼ãƒ«ã€‘ tavily_search_results_json
ã€å›ç­”ã€‘ æ¤œç´¢çµæœã«ã‚ˆã‚‹ã¨ã€Pythonã®æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯3.12.xã§ã™...

ã€è³ªå•ã€‘ å¤§é˜ªã®å¤©æ°—ã‚’èª¿ã¹ã¦ã€ãã®æ°—æ¸©ã‚’è¯æ°ã«å¤‰æ›ã—ã¦
ã€ä½¿ç”¨ãƒ„ãƒ¼ãƒ«ã€‘ get_weather, calculate
ã€å›ç­”ã€‘ å¤§é˜ªã®å¤©æ°—ã¯æ™´ã‚Œã§ã€æ°—æ¸©ã¯23åº¦ï¼ˆæ‘‚æ°ï¼‰ã§ã™ã€‚ã“ã‚Œã‚’è¯æ°ã«å¤‰æ›ã™ã‚‹ã¨73.4åº¦ã«ãªã‚Šã¾ã™ã€‚
```

---

## ç¬¬5ç« : é«˜åº¦ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã€å®Œå…¨è§£èª¬ã€‘

### 5-1. ãƒ«ãƒ¼ãƒ—ã¨å†è©¦è¡Œãƒ‘ã‚¿ãƒ¼ãƒ³

#### ãªãœãƒ«ãƒ¼ãƒ—ãŒå¿…è¦ã‹

**ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹**:
1. **å“è³ªãƒã‚§ãƒƒã‚¯**: çµæœãŒæº€è¶³ã„ãã¾ã§ç¹°ã‚Šè¿”ã™
2. **ãƒªãƒˆãƒ©ã‚¤**: ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã«å†è©¦è¡Œ
3. **æ®µéšçš„æ”¹å–„**: å‰å›ã®çµæœã‚’ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã—ã¦æ”¹å–„

**ä¾‹: ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã®å“è³ªãƒã‚§ãƒƒã‚¯**
```
generate_code â†’ check_quality â†’
  â”œâ”€ åˆæ ¼ â†’ END
  â””â”€ ä¸åˆæ ¼ â†’ improve_code â†’ check_quality â†’ ...
```

---

#### åŸºæœ¬çš„ãªãƒ«ãƒ¼ãƒ—ãƒ‘ã‚¿ãƒ¼ãƒ³

```python
import os
from typing import TypedDict, Literal
from langgraph.graph import StateGraph, START, END
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, SystemMessage

os.environ["GEMINI_API_KEY"] = "your-key"

class LoopState(TypedDict):
    task: str
    result: str
    quality_score: float
    attempts: int

llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash-exp", temperature=0.7)

# ã‚¹ãƒ†ãƒƒãƒ—1: ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ
def execute_task(state: LoopState) -> dict:
    """ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ"""
    messages = [
        SystemMessage(content="ç°¡æ½”ãªPythoné–¢æ•°ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"),
        HumanMessage(content=state["task"])
    ]
    response = llm.invoke(messages)
    
    return {
        "result": response.content,
        "attempts": state["attempts"] + 1
    }

# ã‚¹ãƒ†ãƒƒãƒ—2: å“è³ªè©•ä¾¡
def evaluate_quality(state: LoopState) -> dict:
    """ç”Ÿæˆçµæœã®å“è³ªã‚’è©•ä¾¡"""
    messages = [
        SystemMessage(content="""
ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã®å“è³ªã‚’0.0ã€œ1.0ã§è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚
åŸºæº–:
- 0.8ä»¥ä¸Š: å„ªç§€
- 0.5ã€œ0.8: æ™®é€š
- 0.5æœªæº€: æ”¹å–„å¿…è¦

æ•°å€¤ã®ã¿è¿”ã—ã¦ãã ã•ã„ã€‚
        """),
        HumanMessage(content=state["result"])
    ]
    
    response = llm.invoke(messages)
    
    try:
        score = float(response.content.strip())
        score = max(0.0, min(1.0, score))
    except:
        score = 0.5
    
    return {"quality_score": score}

# ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ«ãƒ¼ãƒ—åˆ¤å®š
def should_retry(state: LoopState) -> Literal["retry", "end"]:
    """å†è©¦è¡ŒãŒå¿…è¦ã‹åˆ¤å®š"""
    
    # å“è³ªåŸºæº–ã‚’æº€ãŸã—ãŸ
    if state["quality_score"] >= 0.8:
        return "end"
    
    # æœ€å¤§è©¦è¡Œå›æ•°ã«åˆ°é”
    if state["attempts"] >= 3:
        return "end"
    
    # å†è©¦è¡Œ
    return "retry"

# ã‚¹ãƒ†ãƒƒãƒ—4: æ”¹å–„
def improve_result(state: LoopState) -> dict:
    """å‰å›ã®çµæœã‚’æ”¹å–„"""
    messages = [
        SystemMessage(content=f"""
å‰å›ç”Ÿæˆã—ãŸã‚³ãƒ¼ãƒ‰ã‚’æ”¹å–„ã—ã¦ãã ã•ã„ã€‚
å“è³ªã‚¹ã‚³ã‚¢: {state['quality_score']}

æ”¹å–„ç‚¹:
- ã‚ˆã‚Šæ˜ç¢ºãªå¤‰æ•°å
- ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ã®å‡¦ç†
- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®è¿½åŠ 
        """),
        HumanMessage(content=state["result"])
    ]
    
    response = llm.invoke(messages)
    return {"result": response.content}

# ã‚°ãƒ©ãƒ•æ§‹ç¯‰
workflow = StateGraph(LoopState)

workflow.add_node("execute", execute_task)
workflow.add_node("evaluate", evaluate_quality)
workflow.add_node("improve", improve_result)

workflow.add_edge(START, "execute")
workflow.add_edge("execute", "evaluate")

workflow.add_conditional_edges(
    source="evaluate",
    path=should_retry,
    path_map={
        "retry": "improve",
        "end": END
    }
)

# ãƒ«ãƒ¼ãƒ—ã®è¦: improve â†’ execute
workflow.add_edge("improve", "execute")

app = workflow.compile()

# ãƒ†ã‚¹ãƒˆ
result = app.invoke({
    "task": "ãƒªã‚¹ãƒˆã®é‡è¤‡ã‚’å‰Šé™¤ã™ã‚‹é–¢æ•°ã‚’ä½œæˆ",
    "result": "",
    "quality_score": 0.0,
    "attempts": 0
})

print(f"è©¦è¡Œå›æ•°: {result['attempts']}")
print(f"æœ€çµ‚ã‚¹ã‚³ã‚¢: {result['quality_score']:.2f}")
print(f"\nç”Ÿæˆã‚³ãƒ¼ãƒ‰:\n{result['result']}")
```

**ã‚°ãƒ©ãƒ•æ§‹é€ **:
```
START â†’ execute â†’ evaluate â†’
          â†‘         â†“
          |       retry?
          |         â†“
        improve â†â”€ yes
          
        no â†’ END
```

---

#### å†è©¦è¡Œã®å®Ÿè·µä¾‹: APIå‘¼ã³å‡ºã—ã®ãƒªãƒˆãƒ©ã‚¤

```python
import os
import time
from typing import TypedDict, Literal
from langgraph.graph import StateGraph, START, END

class RetryState(TypedDict):
    url: str
    response: str
    error: str
    attempts: int

def call_api(state: RetryState) -> dict:
    """APIå‘¼ã³å‡ºã—ï¼ˆå¤±æ•—ã™ã‚‹å¯èƒ½æ€§ã‚ã‚Šï¼‰"""
    try:
        # ãƒ€ãƒŸãƒ¼APIå‘¼ã³å‡ºã—
        # å®Ÿéš›ã¯requests.get(state["url"])ãªã©
        import random
        if random.random() < 0.5:  # 50%ã®ç¢ºç‡ã§å¤±æ•—
            raise Exception("APIæ¥ç¶šã‚¨ãƒ©ãƒ¼")
        
        return {
            "response": "APIå‘¼ã³å‡ºã—æˆåŠŸ",
            "error": "",
            "attempts": state["attempts"] + 1
        }
    except Exception as e:
        return {
            "response": "",
            "error": str(e),
            "attempts": state["attempts"] + 1
        }

def should_retry(state: RetryState) -> Literal["retry", "end"]:
    """ãƒªãƒˆãƒ©ã‚¤ã™ã¹ãã‹åˆ¤å®š"""
    
    # æˆåŠŸã—ãŸ
    if state["response"]:
        return "end"
    
    # æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã«åˆ°é”
    if state["attempts"] >= 5:
        return "end"
    
    # ãƒªãƒˆãƒ©ã‚¤
    return "retry"

def wait_before_retry(state: RetryState) -> dict:
    """ãƒªãƒˆãƒ©ã‚¤å‰ã«å¾…æ©Ÿï¼ˆã‚¨ã‚¯ã‚¹ãƒãƒãƒ³ã‚·ãƒ£ãƒ«ãƒãƒƒã‚¯ã‚ªãƒ•ï¼‰"""
    wait_time = 2 ** state["attempts"]  # 2, 4, 8, 16ç§’...
    print(f"ãƒªãƒˆãƒ©ã‚¤ {state['attempts']}å›ç›®ã¾ã§ {wait_time}ç§’å¾…æ©Ÿ...")
    time.sleep(wait_time)
    return {}

# ã‚°ãƒ©ãƒ•æ§‹ç¯‰
workflow = StateGraph(RetryState)
workflow.add_node("call_api", call_api)
workflow.add_node("wait", wait_before_retry)

workflow.add_edge(START, "call_api")
workflow.add_conditional_edges("call_api", should_retry, {
    "retry": "wait",
    "end": END
})
workflow.add_edge("wait", "call_api")

app = workflow.compile()

# ãƒ†ã‚¹ãƒˆ
result = app.invoke({
    "url": "https://api.example.com/data",
    "response": "",
    "error": "",
    "attempts": 0
})

if result["response"]:
    print(f"âœ… æˆåŠŸï¼ˆ{result['attempts']}å›ç›®ã®è©¦è¡Œï¼‰")
else:
    print(f"âŒ å¤±æ•—ï¼ˆ{result['attempts']}å›è©¦è¡Œå¾Œï¼‰: {result['error']}")
```

---

### 5-2. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®å®Œå…¨ã‚¬ã‚¤ãƒ‰

#### åŸºæœ¬çš„ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

```python
def safe_node(state: State) -> dict:
    """ã‚¨ãƒ©ãƒ¼ã‚’ã‚­ãƒ£ãƒƒãƒã—ã¦å®‰å…¨ã«å‡¦ç†"""
    try:
        result = risky_operation(state["input"])
        return {
            "output": result,
            "error": None,
            "success": True
        }
    except ValueError as e:
        return {
            "output": None,
            "error": f"å…¥åŠ›ã‚¨ãƒ©ãƒ¼: {e}",
            "success": False
        }
    except Exception as e:
        return {
            "output": None,
            "error": f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}",
            "success": False
        }
```

---

#### ã‚¨ãƒ©ãƒ¼æ™‚ã®åˆ†å²å‡¦ç†

```python
import os
from typing import TypedDict, Literal
from langgraph.graph import StateGraph, START, END
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage

os.environ["GEMINI_API_KEY"] = "your-key"

class ErrorHandlingState(TypedDict):
    input: str
    output: str
    error: str
    error_type: str

llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash-exp")

def process_with_error_handling(state: ErrorHandlingState) -> dict:
    """å‡¦ç†ã‚’å®Ÿè¡Œã—ã¦ã‚¨ãƒ©ãƒ¼ã‚’ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°"""
    try:
        # ãƒ€ãƒŸãƒ¼å‡¦ç†: JSONãƒ‘ãƒ¼ã‚¹
        import json
        data = json.loads(state["input"])
        
        response = llm.invoke([
            HumanMessage(content=f"ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª¬æ˜: {data}")
        ])
        
        return {
            "output": response.content,
            "error": "",
            "error_type": "none"
        }
    
    except json.JSONDecodeError as e:
        return {
            "output": "",
            "error": str(e),
            "error_type": "json_error"
        }
    
    except Exception as e:
        return {
            "output": "",
            "error": str(e),
            "error_type": "unknown_error"
        }

def route_by_error(state: ErrorHandlingState) -> Literal["success", "json_error", "unknown_error"]:
    """ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—ã§åˆ†å²"""
    return state["error_type"]

def handle_json_error(state: ErrorHandlingState) -> dict:
    """JSONè§£æã‚¨ãƒ©ãƒ¼ã¸ã®å¯¾å¿œ"""
    return {
        "output": f"å…¥åŠ›å½¢å¼ã‚¨ãƒ©ãƒ¼: {state['error']}\næ­£ã—ã„JSONå½¢å¼ã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚"
    }

def handle_unknown_error(state: ErrorHandlingState) -> dict:
    """æœªçŸ¥ã®ã‚¨ãƒ©ãƒ¼ã¸ã®å¯¾å¿œ"""
    return {
        "output": f"ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {state['error']}\nç®¡ç†è€…ã«é€£çµ¡ã—ã¦ãã ã•ã„ã€‚"
    }

def format_success(state: ErrorHandlingState) -> dict:
    """æˆåŠŸæ™‚ã®å‡¦ç†"""
    return {"output": f"âœ… å‡¦ç†æˆåŠŸ\n{state['output']}"}

# ã‚°ãƒ©ãƒ•æ§‹ç¯‰
workflow = StateGraph(ErrorHandlingState)

workflow.add_node("process", process_with_error_handling)
workflow.add_node("json_error_handler", handle_json_error)
workflow.add_node("unknown_error_handler", handle_unknown_error)
workflow.add_node("success_handler", format_success)

workflow.add_edge(START, "process")

workflow.add_conditional_edges(
    source="process",
    path=route_by_error,
    path_map={
        "none": "success_handler",
        "json_error": "json_error_handler",
        "unknown_error": "unknown_error_handler"
    }
)

workflow.add_edge("success_handler", END)
workflow.add_edge("json_error_handler", END)
workflow.add_edge("unknown_error_handler", END)

app = workflow.compile()

# ãƒ†ã‚¹ãƒˆ
test_inputs = [
    '{"name": "Alice", "age": 30}',  # æ­£å¸¸
    '{"invalid json',                 # JSONè§£æã‚¨ãƒ©ãƒ¼
    None                              # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼
]

for inp in test_inputs:
    print(f"\nå…¥åŠ›: {inp}")
    try:
        result = app.invoke({
            "input": str(inp),
            "output": "",
            "error": "",
            "error_type": ""
        })
        print(f"å‡ºåŠ›: {result['output']}")
    except Exception as e:
        print(f"å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
    print("-" * 60)
```

---

### 5-3. ä¸¦åˆ—å®Ÿè¡Œãƒ‘ã‚¿ãƒ¼ãƒ³

#### ä¸¦åˆ—å®Ÿè¡Œã®åŸºæœ¬æ¦‚å¿µ

**ç›´åˆ—å®Ÿè¡Œï¼ˆé€šå¸¸ï¼‰**:
```
task1 â†’ task2 â†’ task3  ï¼ˆæ‰€è¦æ™‚é–“: 6ç§’ï¼‰
2ç§’    2ç§’    2ç§’
```

**ä¸¦åˆ—å®Ÿè¡Œ**:
```
task1 â”€â”
task2 â”€â”¼â†’ merge  ï¼ˆæ‰€è¦æ™‚é–“: 2ç§’ï¼‰
task3 â”€â”˜
2ç§’
```

---

#### ä¸¦åˆ—ãƒãƒ¼ãƒ‰ã®å®Ÿè£…

```python
import os
from typing import TypedDict
from langgraph.graph import StateGraph, START, END
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, SystemMessage

os.environ["GEMINI_API_KEY"] = "your-key"

class ParallelState(TypedDict):
    topic: str
    summary: str
    pros: str
    cons: str
    final_report: str

llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash-exp", temperature=0.7)

# ä¸¦åˆ—ã‚¿ã‚¹ã‚¯1: è¦ç´„ç”Ÿæˆ
def generate_summary(state: ParallelState) -> dict:
    """ãƒˆãƒ”ãƒƒã‚¯ã®è¦ç´„ã‚’ç”Ÿæˆ"""
    response = llm.invoke([
        SystemMessage(content="ãƒˆãƒ”ãƒƒã‚¯ã‚’3æ–‡ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚"),
        HumanMessage(content=state["topic"])
    ])
    return {"summary": response.content}

# ä¸¦åˆ—ã‚¿ã‚¹ã‚¯2: ãƒ¡ãƒªãƒƒãƒˆæŠ½å‡º
def extract_pros(state: ParallelState) -> dict:
    """ãƒ¡ãƒªãƒƒãƒˆã‚’åˆ—æŒ™"""
    response = llm.invoke([
        SystemMessage(content="3ã¤ã®ä¸»è¦ãªãƒ¡ãƒªãƒƒãƒˆã‚’ç®‡æ¡æ›¸ãã§ã€‚"),
        HumanMessage(content=state["topic"])
    ])
    return {"pros": response.content}

# ä¸¦åˆ—ã‚¿ã‚¹ã‚¯3: ãƒ‡ãƒ¡ãƒªãƒƒãƒˆæŠ½å‡º
def extract_cons(state: ParallelState) -> dict:
    """ãƒ‡ãƒ¡ãƒªãƒƒãƒˆã‚’åˆ—æŒ™"""
    response = llm.invoke([
        SystemMessage(content="3ã¤ã®ä¸»è¦ãªãƒ‡ãƒ¡ãƒªãƒƒãƒˆã‚’ç®‡æ¡æ›¸ãã§ã€‚"),
        HumanMessage(content=state["topic"])
    ])
    return {"cons": response.content}

# çµ±åˆã‚¿ã‚¹ã‚¯
def merge_results(state: ParallelState) -> dict:
    """ä¸¦åˆ—å‡¦ç†çµæœã‚’çµ±åˆ"""
    report = f"""
=== åˆ†æãƒ¬ãƒãƒ¼ãƒˆ: {state['topic']} ===

ã€è¦ç´„ã€‘
{state['summary']}

ã€ãƒ¡ãƒªãƒƒãƒˆã€‘
{state['pros']}

ã€ãƒ‡ãƒ¡ãƒªãƒƒãƒˆã€‘
{state['cons']}
    """
    return {"final_report": report.strip()}

# ã‚°ãƒ©ãƒ•æ§‹ç¯‰
workflow = StateGraph(ParallelState)

# ãƒãƒ¼ãƒ‰ç™»éŒ²
workflow.add_node("summary", generate_summary)
workflow.add_node("pros", extract_pros)
workflow.add_node("cons", extract_cons)
workflow.add_node("merge", merge_results)

# ä¸¦åˆ—é–‹å§‹
workflow.add_edge(START, "summary")
workflow.add_edge(START, "pros")
workflow.add_edge(START, "cons")

# çµ±åˆãƒãƒ¼ãƒ‰ã¸é›†ç´„
workflow.add_edge("summary", "merge")
workflow.add_edge("pros", "merge")
workflow.add_edge("cons", "merge")

workflow.add_edge("merge", END)

app = workflow.compile()

# ã‚°ãƒ©ãƒ•ç¢ºèª
print("ã‚°ãƒ©ãƒ•æ§‹é€ :")
print(app.get_graph().draw_ascii())

# ãƒ†ã‚¹ãƒˆ
import time
start = time.time()

result = app.invoke({
    "topic": "ãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯ã®å°å…¥",
    "summary": "",
    "pros": "",
    "cons": "",
    "final_report": ""
})

elapsed = time.time() - start

print(f"\nå®Ÿè¡Œæ™‚é–“: {elapsed:.2f}ç§’")
print(result["final_report"])
```

**é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ**:
- `add_edge(START, "node")` ã‚’è¤‡æ•°å›å‘¼ã¶ã“ã¨ã§ä¸¦åˆ—é–‹å§‹
- ã™ã¹ã¦ã®ä¸¦åˆ—ãƒãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¦ã‹ã‚‰`merge`ãŒå®Ÿè¡Œã•ã‚Œã‚‹
- å®Ÿè¡Œé †åºã¯ä¿è¨¼ã•ã‚Œãªã„ï¼ˆéåŒæœŸå®Ÿè¡Œï¼‰

---

### 5-4. ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Ÿè¡Œ

#### stream()ã®åŸºæœ¬

**é€šå¸¸ã®invoke()**:
```python
result = app.invoke({"input": "è³ªå•"})
print(result)  # å…¨ã¦å®Œäº†å¾Œã«ä¸€åº¦ã ã‘è¡¨ç¤º
```

**stream()ã‚’ä½¿ç”¨**:
```python
for chunk in app.stream({"input": "è³ªå•"}):
    print(chunk)  # å„ãƒãƒ¼ãƒ‰ã®å®Ÿè¡Œã”ã¨ã«è¡¨ç¤º
```

---

#### å®Ÿè·µä¾‹: ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤º

```python
import os
from typing import TypedDict
from langgraph.graph import StateGraph, START, END
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage
import time

os.environ["GEMINI_API_KEY"] = "your-key"

class StreamState(TypedDict):
    input: str
    step1_result: str
    step2_result: str
    final_output: str

llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash-exp")

def step1_process(state: StreamState) -> dict:
    """ã‚¹ãƒ†ãƒƒãƒ—1: å…¥åŠ›ã‚’åˆ†æ"""
    print("  [ã‚¹ãƒ†ãƒƒãƒ—1] å…¥åŠ›ã‚’åˆ†æä¸­...")
    time.sleep(1)  # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    
    response = llm.invoke([
        HumanMessage(content=f"æ¬¡ã®æ–‡ã‚’åˆ†æ: {state['input']}")
    ])
    
    return {"step1_result": response.content}

def step2_process(state: StreamState) -> dict:
    """ã‚¹ãƒ†ãƒƒãƒ—2: è©³ç´°åˆ†æ"""
    print("  [ã‚¹ãƒ†ãƒƒãƒ—2] è©³ç´°åˆ†æä¸­...")
    time.sleep(1)
    
    response = llm.invoke([
        HumanMessage(content=f"æ¬¡ã®åˆ†æã‚’æ·±ã‚ã‚‹: {state['step1_result']}")
    ])
    
    return {"step2_result": response.content}

def finalize(state: StreamState) -> dict:
    """ã‚¹ãƒ†ãƒƒãƒ—3: æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ"""
    print("  [ã‚¹ãƒ†ãƒƒãƒ—3] ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")
    time.sleep(1)
    
    final = f"""
åˆ†æçµæœ:
- åˆæœŸåˆ†æ: {state['step1_result'][:50]}...
- è©³ç´°åˆ†æ: {state['step2_result'][:50]}...
    """
    return {"final_output": final}

# ã‚°ãƒ©ãƒ•æ§‹ç¯‰
workflow = StateGraph(StreamState)
workflow.add_node("step1", step1_process)
workflow.add_node("step2", step2_process)
workflow.add_node("finalize", finalize)

workflow.add_edge(START, "step1")
workflow.add_edge("step1", "step2")
workflow.add_edge("step2", "finalize")
workflow.add_edge("finalize", END)

app = workflow.compile()

# ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Ÿè¡Œ
print("="*60)
print("ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Ÿè¡Œé–‹å§‹")
print("="*60)

for i, chunk in enumerate(app.stream({
    "input": "AIã®æœªæ¥ã«ã¤ã„ã¦",
    "step1_result": "",
    "step2_result": "",
    "final_output": ""
}), 1):
    node_name = list(chunk.keys())[0]
    print(f"\n[ãƒãƒ£ãƒ³ã‚¯ {i}] ãƒãƒ¼ãƒ‰ '{node_name}' å®Œäº†")
    # print(f"  çµæœ: {chunk}")  # è©³ç´°è¡¨ç¤º

print("\n" + "="*60)
print("âœ… ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
print("="*60)
```

**å‡ºåŠ›ä¾‹**:
```
============================================================
ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Ÿè¡Œé–‹å§‹
============================================================
  [ã‚¹ãƒ†ãƒƒãƒ—1] å…¥åŠ›ã‚’åˆ†æä¸­...

[ãƒãƒ£ãƒ³ã‚¯ 1] ãƒãƒ¼ãƒ‰ 'step1' å®Œäº†
  [ã‚¹ãƒ†ãƒƒãƒ—2] è©³ç´°åˆ†æä¸­...

[ãƒãƒ£ãƒ³ã‚¯ 2] ãƒãƒ¼ãƒ‰ 'step2' å®Œäº†
  [ã‚¹ãƒ†ãƒƒãƒ—3] ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...

[ãƒãƒ£ãƒ³ã‚¯ 3] ãƒãƒ¼ãƒ‰ 'finalize' å®Œäº†

============================================================
âœ… ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ
============================================================
```

---

## ä»˜éŒ²A: ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°å®Œå…¨ã‚¬ã‚¤ãƒ‰

### ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã¨è§£æ±ºæ³•

#### ã‚¨ãƒ©ãƒ¼1: KeyError: 'route_name'

**ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸**:
```
KeyError: 'high_priority'
```

**åŸå› **:
ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é–¢æ•°ãŒè¿”ã™å€¤ãŒpath_mapã«å­˜åœ¨ã—ãªã„

**è§£æ±ºæ³•**:
```python
# âŒ å•é¡Œã®ã‚ã‚‹ã‚³ãƒ¼ãƒ‰
def route(state):
    return "high_priority"  # ã“ã®å€¤ãŒ

workflow.add_conditional_edges("node", route, {
    "high": "handler",  # ã“ã“ã«ãªã„
    "low": "handler2"
})

# âœ… ä¿®æ­£ç‰ˆ
def route(state):
    return "high"  # å€¤ã‚’ä¸€è‡´ã•ã›ã‚‹

workflow.add_conditional_edges("node", route, {
    "high": "handler",
    "low": "handler2"
})
```

---

#### ã‚¨ãƒ©ãƒ¼2: "Node '__start__' already exists"

**ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸**:
```
ValueError: Node '__start__' already exists
```

**åŸå› **:
äºˆç´„èª`__start__`ã‚„`__end__`ã‚’ãƒãƒ¼ãƒ‰åã¨ã—ã¦ä½¿ç”¨

**è§£æ±ºæ³•**:
```python
# âŒ å•é¡Œã®ã‚ã‚‹ã‚³ãƒ¼ãƒ‰
workflow.add_node("__start__", func)

# âœ… ä¿®æ­£ç‰ˆ
workflow.add_node("start_process", func)
```

---

#### ã‚¨ãƒ©ãƒ¼3: APIèªè¨¼ã‚¨ãƒ©ãƒ¼

**ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸**:
```
AuthenticationError: Invalid API key
```

**åŸå› **:
- APIã‚­ãƒ¼ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ãªã„
- APIã‚­ãƒ¼ãŒç„¡åŠ¹

**è§£æ±ºæ³•**:
```python
import os

# ãƒ‡ãƒãƒƒã‚°: ã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
print(os.environ.get("GEMINI_API_KEY"))
print(os.environ.get("TAVILY_API_KEY"))

# æ­£ã—ãè¨­å®š
os.environ["GEMINI_API_KEY"] = "AIza..."  # å®Ÿéš›ã®ã‚­ãƒ¼
os.environ["TAVILY_API_KEY"] = "tvly-..."  # å®Ÿéš›ã®ã‚­ãƒ¼
```

---

#### ã‚¨ãƒ©ãƒ¼4: RecursionError

**ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸**:
```
RecursionError: maximum recursion depth exceeded
```

**åŸå› **:
ç„¡é™ãƒ«ãƒ¼ãƒ—ï¼ˆçµ‚äº†æ¡ä»¶ãŒãªã„ï¼‰

**è§£æ±ºæ³•**:
```python
# âŒ å•é¡Œã®ã‚ã‚‹ã‚³ãƒ¼ãƒ‰ï¼ˆçµ‚äº†æ¡ä»¶ãªã—ï¼‰
def route(state):
    return "retry"  # å¸¸ã«retry

workflow.add_conditional_edges("node", route, {
    "retry": "node"  # ç„¡é™ãƒ«ãƒ¼ãƒ—
})

# âœ… ä¿®æ­£ç‰ˆï¼ˆçµ‚äº†æ¡ä»¶ã‚’è¿½åŠ ï¼‰
def route(state):
    if state["attempts"] >= 5:
        return "end"
    return "retry"

workflow.add_conditional_edges("node", route, {
    "retry": "node",
    "end": END
})
```

**ã¾ãŸã¯æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’åˆ¶é™**:
```python
result = app.invoke(
    {"input": "è³ªå•"},
    config={"recursion_limit": 10}  # æœ€å¤§10ã‚¹ãƒ†ãƒƒãƒ—
)
```

---

## ä»˜éŒ²B: ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹é›†

### 1. Stateè¨­è¨ˆã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

#### âœ… è‰¯ã„è¨­è¨ˆ

```python
from typing import TypedDict

class State(TypedDict):
    # æ˜ç¢ºã§å…·ä½“çš„ãªåå‰
    user_query: str
    search_results: list
    final_answer: str
    confidence_score: float
    error_message: str
```

#### âŒ æ‚ªã„è¨­è¨ˆ

```python
class State(TypedDict):
    # æ›–æ˜§ãªåå‰
    data: str
    result: str
    value: float
    msg: str
```

---

### 2. ãƒãƒ¼ãƒ‰é–¢æ•°ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

#### âœ… è‰¯ã„å®Ÿè£…

```python
def process_node(state: State) -> dict:
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒªã‚’å‡¦ç†ã—ã¦LLMã§å›ç­”ã‚’ç”Ÿæˆ
    
    Args:
        state: ç¾åœ¨ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çŠ¶æ…‹
    
    Returns:
        æ›´æ–°ã™ã‚‹Stateãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
    """
    try:
        # æ˜ç¢ºãªãƒ­ã‚¸ãƒƒã‚¯
        user_input = state["user_query"]
        response = llm.invoke([HumanMessage(content=user_input)])
        
        return {
            "final_answer": response.content,
            "error_message": ""
        }
    
    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
        return {
            "final_answer": "",
            "error_message": str(e)
        }
```

#### âŒ æ‚ªã„å®Ÿè£…

```python
def process_node(state):  # å‹ãƒ’ãƒ³ãƒˆãªã—
    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãªã—
    # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãªã—
    return {"result": llm.invoke([HumanMessage(state["input"])]).content}
```

---

### 3. ã‚°ãƒ©ãƒ•æ§‹é€ ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

#### âœ… è‰¯ã„æ§‹é€ 

```python
# æ˜ç¢ºãªå‘½å
workflow.add_node("classify_intent", classify)
workflow.add_node("search_knowledge_base", search)
workflow.add_node("generate_response", generate)

# è«–ç†çš„ãªãƒ•ãƒ­ãƒ¼
workflow.add_edge(START, "classify_intent")
workflow.add_conditional_edges("classify_intent", route, {
    "needs_search": "search_knowledge_base",
    "direct_answer": "generate_response"
})
```

#### âŒ æ‚ªã„æ§‹é€ 

```python
# æ›–æ˜§ãªå‘½å
workflow.add_node("node1", func1)
workflow.add_node("node2", func2)
workflow.add_node("process", func3)

# è¤‡é›‘ã™ãã‚‹ãƒ•ãƒ­ãƒ¼ï¼ˆå¯èª­æ€§ãŒä½ã„ï¼‰
```

---

## ä»˜éŒ²C: å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

### ãƒ‘ã‚¿ãƒ¼ãƒ³1: ã‚·ãƒ³ãƒ—ãƒ«Q&A

```python
import os
from typing import TypedDict
from langgraph.graph import StateGraph, START, END
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage

os.environ["GEMINI_API_KEY"] = "your-key"

class QAState(TypedDict):
    question: str
    answer: str

llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash-exp")

workflow = StateGraph(QAState)
workflow.add_node("answer", lambda s: {
    "answer": llm.invoke([HumanMessage(s["question"])]).content
})
workflow.add_edge(START, "answer")
workflow.add_edge("answer", END)

app = workflow.compile()

result = app.invoke({"question": "LangGraphã¨ã¯ï¼Ÿ", "answer": ""})
print(result["answer"])
```

---

### ãƒ‘ã‚¿ãƒ¼ãƒ³2: æ¤œç´¢ä»˜ãQ&A

```python
# ç¬¬4ç« ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã‚’å‚ç…§
```

---

### ãƒ‘ã‚¿ãƒ¼ãƒ³3: å¤šæ®µéšå‡¦ç†

```python
workflow.add_edge(START, "analyze")
workflow.add_edge("analyze", "transform")
workflow.add_edge("transform", "validate")
workflow.add_edge("validate", "format")
workflow.add_edge("format", END)
```

---

### ãƒ‘ã‚¿ãƒ¼ãƒ³4: æ¡ä»¶åˆ†å² + ãƒ«ãƒ¼ãƒ—

```python
workflow.add_edge(START, "process")
workflow.add_conditional_edges("process", check_quality, {
    "good": END,
    "retry": "improve"
})
workflow.add_edge("improve", "process")  # ãƒ«ãƒ¼ãƒ—
```

---

## ğŸ‰ å®Œèµ°ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼

ã“ã®ç¬¬3ç« ä»¥é™ã®è³‡æ–™ã§ã€ã‚ãªãŸã¯ä»¥ä¸‹ã‚’ãƒã‚¹ã‚¿ãƒ¼ã—ã¾ã—ãŸ:

âœ… **æ¡ä»¶åˆ†å²**: è¤‡é›‘ãªãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯ã®å®Ÿè£…
âœ… **ãƒ„ãƒ¼ãƒ«çµ±åˆ**: Tavilyæ¤œç´¢ã¨ã‚«ã‚¹ã‚¿ãƒ ãƒ„ãƒ¼ãƒ«ã®çµ±åˆ
âœ… **ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—**: LLMã¨ãƒ„ãƒ¼ãƒ«ã®å”èª¿å‹•ä½œ
âœ… **é«˜åº¦ãªãƒ‘ã‚¿ãƒ¼ãƒ³**: ãƒ«ãƒ¼ãƒ—ã€ä¸¦åˆ—å®Ÿè¡Œã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
âœ… **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: å …ç‰¢ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼è¨­è¨ˆ
âœ… **ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹**: ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³å“è³ªã®ã‚³ãƒ¼ãƒ‰

---

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. **å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«æŒ‘æˆ¦**:
   - ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒœãƒƒãƒˆã‚’æ§‹ç¯‰
   - ãƒªã‚µãƒ¼ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½œæˆ
   - ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè£…

2. **å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§æ·±æ˜ã‚Š**:
   - [LangGraphå…¬å¼](https://langchain-ai.github.io/langgraph/)
   - [LangChainçµ±åˆ](https://python.langchain.com/)

3. **ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£å‚åŠ **:
   - GitHub Discussionsã§è³ªå•
   - è‡ªåˆ†ã®å®Ÿè£…ã‚’å…±æœ‰

---

## å‚è€ƒãƒªãƒ³ã‚¯

- ğŸ“š [LangGraphå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://langchain-ai.github.io/langgraph/)
- ğŸ”‘ [Google AI Studio](https://aistudio.google.com/)
- ğŸ” [Tavily Search API](https://tavily.com/)
- ğŸ’¬ [LangChain Discord](https://discord.gg/langchain)

**Happy Coding! ğŸš€**

ã‚ãªãŸã¯ä»Šã€LangGraphã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚å®Ÿéš›ã®ã‚µãƒ¼ãƒ“ã‚¹ã‚’æ§‹ç¯‰ã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸï¼


