# LangGraph å®Œå…¨ãƒã‚¹ã‚¿ãƒ¼ã‚¬ã‚¤ãƒ‰ ğŸš€

**Python é–‹ç™ºè€…ã®ãŸã‚ã®å®Ÿè·µçš„å…¥é–€æ›¸**

---

## ğŸ“š ç›®æ¬¡

- [ç¬¬ 1 ç« : æœ€å°æ§‹æˆã®å®Ÿè£…](#ç¬¬1ç« -æœ€å°æ§‹æˆã®å®Ÿè£…)
- [ç¬¬ 2 ç« : æ¡ä»¶åˆ†å²ã¨ãƒ„ãƒ¼ãƒ«çµ±åˆ](#ç¬¬2ç« -æ¡ä»¶åˆ†å²ã¨ãƒ„ãƒ¼ãƒ«çµ±åˆ)
- [ç¬¬ 3 ç« : ãƒ«ãƒ¼ãƒ—å‡¦ç†ã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°](#ç¬¬3ç« -ãƒ«ãƒ¼ãƒ—å‡¦ç†ã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°)
- [ç¬¬ 4 ç« : ä¸¦åˆ—å‡¦ç†ã¨éåŒæœŸå®Ÿè¡Œ](#ç¬¬4ç« -ä¸¦åˆ—å‡¦ç†ã¨éåŒæœŸå®Ÿè¡Œ)
- [ç¬¬ 5 ç« : è¤‡é›‘ãªåˆ†å²ã¨å®Ÿè·µçš„ã‚·ã‚¹ãƒ†ãƒ ](#ç¬¬5ç« -è¤‡é›‘ãªåˆ†å²ã¨å®Ÿè·µçš„ã‚·ã‚¹ãƒ†ãƒ )
- [ä»˜éŒ²: ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°](#ä»˜éŒ²-ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°å®Œå…¨ç‰ˆ)

---

# ç¬¬ 1 ç« : æœ€å°æ§‹æˆã®å®Ÿè£…

## ğŸ¯ å­¦ç¿’ç›®æ¨™

- StateGraph ã®åŸºæœ¬æ§‹é€ ã‚’ç†è§£
- Start â†’ Process â†’ End ã®æœ€å°ãƒ•ãƒ­ãƒ¼ã‚’æ§‹ç¯‰
- ã‚°ãƒ©ãƒ•å¯è¦–åŒ–ã¨å®Ÿè¡Œãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚¹ã‚¿ãƒ¼

## ğŸ“¦ ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# åŸºæœ¬ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
pip install langgraph langchain-google-genai langchain-community

# å¯è¦–åŒ–ç”¨
pip install pygraphviz pillow
# pygraphvizãŒå‹•ã‹ãªã„å ´åˆ
pip install grandalf

# æ¤œç´¢ç”¨ï¼ˆç¬¬2ç« ä»¥é™ï¼‰
pip install tavily-python

# éåŒæœŸå‡¦ç†ç”¨ï¼ˆç¬¬4ç« ä»¥é™ï¼‰
pip install aiohttp

# ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ç”¨
pip install tenacity
```

### API ã‚­ãƒ¼è¨­å®š

```python
import os

# Google AI Studio API
os.environ["GOOGLE_API_KEY"] = "your-google-api-key"

# Tavily Search APIï¼ˆç¬¬2ç« ä»¥é™ï¼‰
os.environ["TAVILY_API_KEY"] = "your-tavily-api-key"
```

## ğŸ—ï¸ æœ€å°æ§‹æˆã®å®Ÿè£…

### ã‚¹ãƒ†ãƒƒãƒ— 1: å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

```python
from typing import TypedDict
from langgraph.graph import StateGraph, START, END
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage
```

**å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å½¹å‰²**:

- `TypedDict`: ã‚¹ãƒ†ãƒ¼ãƒˆã®å‹å®šç¾©ï¼ˆå‹å®‰å…¨æ€§ï¼‰
- `StateGraph`: ã‚°ãƒ©ãƒ•ã®ä½œæˆ
- `START, END`: ã‚°ãƒ©ãƒ•ã®é–‹å§‹ãƒ»çµ‚äº†ãƒãƒ¼ãƒ‰
- `ChatGoogleGenerativeAI`: Gemini API æ¥ç¶š
- `HumanMessage`: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ§‹é€ åŒ–

### ã‚¹ãƒ†ãƒƒãƒ— 2: ã‚¹ãƒ†ãƒ¼ãƒˆå®šç¾©

```python
class State(TypedDict):
    """
    ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å…¨ä½“ã§å…±æœ‰ã•ã‚Œã‚‹çŠ¶æ…‹

    Attributes:
        input: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
        output: LLMã‹ã‚‰ã®å‡ºåŠ›ãƒ†ã‚­ã‚¹ãƒˆ
    """
    input: str
    output: str
```

**ã‚¹ãƒ†ãƒ¼ãƒˆè¨­è¨ˆã®åŸå‰‡**:

- âœ… å¿…è¦æœ€å°é™ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã¿å®šç¾©
- âœ… å‹ãƒ’ãƒ³ãƒˆã‚’æ˜ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ãŒå®¹æ˜“ï¼‰
- âœ… ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ–‡å­—åˆ—ã§å½¹å‰²ã‚’è¨˜è¿°

### ã‚¹ãƒ†ãƒƒãƒ— 3: LLM åˆæœŸåŒ–

```python
# Gemini LLMã®åˆæœŸåŒ–
llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-exp",
    temperature=0.7,
    max_tokens=1024
)
```

### ã‚¹ãƒ†ãƒƒãƒ— 4: ãƒãƒ¼ãƒ‰é–¢æ•°ã®å®šç¾©

```python
def call_gemini(state: State) -> dict:
    """Gemini APIã‚’å‘¼ã³å‡ºã™ãƒãƒ¼ãƒ‰é–¢æ•°"""
    user_input = state["input"]
    response = llm.invoke([HumanMessage(content=user_input)])
    return {"output": response.content}
```

### ã‚¹ãƒ†ãƒƒãƒ— 5: ã‚°ãƒ©ãƒ•ã®æ§‹ç¯‰

```python
# StateGraphã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
workflow = StateGraph(State)

# ãƒãƒ¼ãƒ‰ã‚’è¿½åŠ 
workflow.add_node("gemini", call_gemini)

# ãƒ•ãƒ­ãƒ¼ã‚’å®šç¾©
workflow.add_edge(START, "gemini")
workflow.add_edge("gemini", END)

# ã‚°ãƒ©ãƒ•ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
app = workflow.compile()
```

## ğŸ¨ ã‚°ãƒ©ãƒ•å¯è¦–åŒ–

### å¯è¦–åŒ–ã®å®Ÿè£…

```python
from IPython.display import Image, display

def visualize_and_save_graph(app, filename="workflow_graph.png"):
    """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚°ãƒ©ãƒ•ã‚’å¯è¦–åŒ–ã—ã¦PNGä¿å­˜"""
    try:
        png_data = app.get_graph().draw_mermaid_png()
        with open(filename, "wb") as f:
            f.write(png_data)
        print(f"âœ… ã‚°ãƒ©ãƒ•ã‚’ '{filename}' ã«ä¿å­˜ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"â„¹ï¸ PNGä¿å­˜å¤±æ•—: {e}")
        print("ğŸ“ ASCIIç‰ˆã‚°ãƒ©ãƒ•:")
        print(app.get_graph().draw_ascii())
```

### ã‚°ãƒ©ãƒ•æ§‹é€ 

```mermaid
flowchart TD
    START(["START"]) --> gemini["Gemini APIå‘¼ã³å‡ºã—"]
    gemini --> END(["END"])
```

## â–¶ï¸ å®Ÿè¡Œæ–¹æ³•

### æ±ç”¨å®Ÿè¡Œé–¢æ•°

```python
def run_workflow(app, initial_state: dict, verbose=True):
    """LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œï¼ˆå…¨ç« å…±é€šï¼‰"""
    if verbose:
        print("=" * 60)
        print("ğŸš€ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œé–‹å§‹")
        print("=" * 60)
        print(f"ğŸ“¥ åˆæœŸå…¥åŠ›: {initial_state}")

    result = app.invoke(initial_state)

    if verbose:
        print("=" * 60)
        print("âœ… ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Œäº†")
        print("=" * 60)
        print(f"ğŸ“¤ çµæœ:\n{result['output']}")

    return result
```

## ğŸ¯ å®Œå…¨ãªã‚³ãƒ¼ãƒ‰ä¾‹

```python
"""
LangGraph æœ€å°æ§‹æˆ - å®Œå…¨å®Ÿè£…
Start â†’ Gemini â†’ Output
"""

import os
from typing import TypedDict
from langgraph.graph import StateGraph, START, END
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage

# APIã‚­ãƒ¼è¨­å®š
os.environ["GOOGLE_API_KEY"] = "your-key-here"

# ã‚¹ãƒ†ãƒ¼ãƒˆå®šç¾©
class State(TypedDict):
    input: str
    output: str

# LLMåˆæœŸåŒ–
llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-exp",
    temperature=0.7
)

# ãƒãƒ¼ãƒ‰é–¢æ•°
def call_gemini(state: State) -> dict:
    user_input = state["input"]
    response = llm.invoke([HumanMessage(content=user_input)])
    return {"output": response.content}

# ã‚°ãƒ©ãƒ•æ§‹ç¯‰
workflow = StateGraph(State)
workflow.add_node("gemini", call_gemini)
workflow.add_edge(START, "gemini")
workflow.add_edge("gemini", END)
app = workflow.compile()

# å®Ÿè¡Œ
if __name__ == "__main__":
    result = run_workflow(app, {
        "input": "LangGraphã®ç‰¹å¾´ã‚’3ã¤æ•™ãˆã¦ãã ã•ã„"
    })
```

---

# ç¬¬ 2 ç« : æ¡ä»¶åˆ†å²ã¨ãƒ„ãƒ¼ãƒ«çµ±åˆ

## ğŸ¯ å­¦ç¿’ç›®æ¨™

- æ¡ä»¶åˆ†å²ï¼ˆ`add_conditional_edges`ï¼‰ã®å®Ÿè£…
- å¤–éƒ¨ãƒ„ãƒ¼ãƒ«ï¼ˆTavily æ¤œç´¢ï¼‰ã®çµ±åˆ
- ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç†è§£

## ğŸ› ï¸ ãƒ„ãƒ¼ãƒ«çµ±åˆã®åŸºç¤

### ã‚¹ãƒ†ãƒ¼ãƒˆå®šç¾©ï¼ˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ™ãƒ¼ã‚¹ï¼‰

```python
from typing import Annotated
import operator

class AgentState(TypedDict):
    messages: Annotated[list, operator.add]
```

### ãƒ„ãƒ¼ãƒ«è¨­å®š

```python
from langchain_community.tools.tavily_search import TavilySearchResults
from langgraph.prebuilt import ToolNode

search_tool = TavilySearchResults(
    max_results=3,
    search_depth="advanced"
)

tools = [search_tool]

llm_with_tools = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-exp",
    temperature=0
).bind_tools(tools)
```

### ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒãƒ¼ãƒ‰ã®å®Ÿè£…

```python
def agent_node(state: AgentState) -> dict:
    """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ: æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ±ºå®š"""
    messages = state["messages"]
    response = llm_with_tools.invoke(messages)
    return {"messages": [response]}

def should_continue(state: AgentState) -> str:
    """æ¡ä»¶åˆ†å²: ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ or çµ‚äº†"""
    last_message = state["messages"][-1]

    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tools"

    return END
```

### ã‚°ãƒ©ãƒ•æ§‹ç¯‰

```python
workflow = StateGraph(AgentState)

workflow.add_node("agent", agent_node)
workflow.add_node("tools", ToolNode(tools))

workflow.add_edge(START, "agent")

workflow.add_conditional_edges(
    "agent",
    should_continue,
    {
        "tools": "tools",
        END: END
    }
)

workflow.add_edge("tools", "agent")

app = workflow.compile()
```

### ã‚°ãƒ©ãƒ•æ§‹é€ 

```mermaid
flowchart TD
    START(["START"]) --> agent["ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"]
    agent --> decision{"ãƒ„ãƒ¼ãƒ«å¿…è¦?"}
    decision -- "Yes" --> tools["ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ"]
    tools --> agent
    decision -- "No" --> END(["END"])
```

---

# ç¬¬ 3 ç« : ãƒ«ãƒ¼ãƒ—å‡¦ç†ã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

## ğŸ¯ å­¦ç¿’ç›®æ¨™

- ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ã®å®Ÿè£…
- 429 ã‚¨ãƒ©ãƒ¼ï¼ˆRate Limitï¼‰å¯¾ç­–
- è¤‡æ•° LLM ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

## ğŸ›¡ï¸ ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æˆ¦ç•¥

### æ‹¡å¼µã‚¹ãƒ†ãƒ¼ãƒˆå®šç¾©

```python
class RobustState(TypedDict):
    messages: Annotated[list, operator.add]
    retry_count: int
    error_log: list
    current_llm: str
```

### è¤‡æ•° LLM è¨­å®š

```python
llm_primary = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-exp",
    temperature=0.7
)

llm_backup = ChatGoogleGenerativeAI(
    model="gemini-1.5-pro",
    temperature=0.7
)
```

### ãƒªãƒˆãƒ©ã‚¤ä»˜ããƒãƒ¼ãƒ‰

```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10)
)
def call_llm_with_retry(messages: list, llm):
    return llm.invoke(messages)

def robust_agent_node(state: RobustState) -> dict:
    retry_count = state.get("retry_count", 0)
    current_llm = state.get("current_llm", "primary")

    try:
        llm = llm_primary if current_llm == "primary" else llm_backup
        response = call_llm_with_retry(state["messages"], llm)
        return {
            "messages": [response],
            "retry_count": 0
        }
    except Exception as e:
        if retry_count < 2:
            return {
                "retry_count": retry_count + 1,
                "current_llm": "backup"
            }
        else:
            error_msg = AIMessage(content="ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
            return {"messages": [error_msg]}
```

### ã‚°ãƒ©ãƒ•æ§‹é€ 

```mermaid
flowchart TD
    START(["START"]) --> agent["ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"]
    agent --> check{"æˆåŠŸ?"}
    check -- "Yes" --> END(["END"])
    check -- "No" --> retry{"ãƒªãƒˆãƒ©ã‚¤å¯èƒ½?"}
    retry -- "Yes" --> switch["LLMåˆ‡æ›¿"]
    switch --> agent
    retry -- "No" --> error["ã‚¨ãƒ©ãƒ¼çµ‚äº†"]
    error --> END
```

---

# ç¬¬ 4 ç« : ä¸¦åˆ—å‡¦ç†ã¨éåŒæœŸå®Ÿè¡Œ

## ğŸ¯ å­¦ç¿’ç›®æ¨™

- ä¸¦åˆ—ãƒãƒ¼ãƒ‰å®Ÿè¡Œ
- éåŒæœŸå‡¦ç†ã«ã‚ˆã‚‹é«˜é€ŸåŒ–
- è¤‡æ•° API å‘¼ã³å‡ºã—ã®æœ€é©åŒ–

## ğŸš€ ä¸¦åˆ—å‡¦ç†ã®å®Ÿè£…

### ä¸¦åˆ—å‡¦ç†ç”¨ã‚¹ãƒ†ãƒ¼ãƒˆ

```python
import asyncio

class ParallelState(TypedDict):
    query: str
    search_results: list
    summary_results: list
    analysis_results: list
    final_output: str
```

### ä¸¦åˆ—å®Ÿè¡Œãƒãƒ¼ãƒ‰

```python
async def search_task(state: ParallelState) -> dict:
    """æ¤œç´¢ã‚¿ã‚¹ã‚¯"""
    await asyncio.sleep(1)
    search_tool = TavilySearchResults(max_results=3)
    results = search_tool.invoke({"query": state["query"]})
    return {"search_results": results}

async def summary_task(state: ParallelState) -> dict:
    """è¦ç´„ã‚¿ã‚¹ã‚¯"""
    await asyncio.sleep(1)
    prompt = f"æ¬¡ã®ã‚¯ã‚¨ãƒªã‚’è¦ç´„: {state['query']}"
    response = await llm_primary.ainvoke([HumanMessage(content=prompt)])
    return {"summary_results": [response.content]}

async def analysis_task(state: ParallelState) -> dict:
    """åˆ†æã‚¿ã‚¹ã‚¯"""
    await asyncio.sleep(1)
    prompt = f"æ¬¡ã®ã‚¯ã‚¨ãƒªã‚’åˆ†æ: {state['query']}"
    response = await llm_primary.ainvoke([HumanMessage(content=prompt)])
    return {"analysis_results": [response.content]}
```

### ã‚°ãƒ©ãƒ•æ§‹ç¯‰ï¼ˆä¸¦åˆ—ï¼‰

```python
workflow = StateGraph(ParallelState)

workflow.add_node("search", search_task)
workflow.add_node("summary", summary_task)
workflow.add_node("analysis", analysis_task)
workflow.add_node("aggregate", aggregate_results)

# ä¸¦åˆ—ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
workflow.add_edge(START, "search")
workflow.add_edge(START, "summary")
workflow.add_edge(START, "analysis")

# é›†ç´„ãƒãƒ¼ãƒ‰ã¸
workflow.add_edge("search", "aggregate")
workflow.add_edge("summary", "aggregate")
workflow.add_edge("analysis", "aggregate")

workflow.add_edge("aggregate", END)

app = workflow.compile()
```

### ã‚°ãƒ©ãƒ•æ§‹é€ 

```mermaid
flowchart TD
    START(["START"]) --> search["æ¤œç´¢ã‚¿ã‚¹ã‚¯"]
    START --> summary["è¦ç´„ã‚¿ã‚¹ã‚¯"]
    START --> analysis["åˆ†æã‚¿ã‚¹ã‚¯"]
    search --> aggregate["çµæœé›†ç´„"]
    summary --> aggregate
    analysis --> aggregate
    aggregate --> END(["END"])
```

### ä¸¦åˆ—å‡¦ç†ã®åŠ¹æœ

- **é€æ¬¡å®Ÿè¡Œ**: 3 ç§’ï¼ˆ1 ç§’ Ã— 3 ã‚¿ã‚¹ã‚¯ï¼‰
- **ä¸¦åˆ—å®Ÿè¡Œ**: 1.2 ç§’ï¼ˆæœ€ã‚‚é…ã„ã‚¿ã‚¹ã‚¯ + ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ï¼‰
- **é€Ÿåº¦å‘ä¸Š**: ç´„ 2.5 å€

---

# ç¬¬ 5 ç« : è¤‡é›‘ãªåˆ†å²ã¨å®Ÿè·µçš„ã‚·ã‚¹ãƒ†ãƒ 

## ğŸ¯ å­¦ç¿’ç›®æ¨™

- 10 ä»¥ä¸Šã®æ¡ä»¶åˆ†å²ã‚’æŒã¤è¤‡é›‘ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
- ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ 
- å®Ÿè·µçš„ãªãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–

## ğŸ—ï¸ è¤‡é›‘ãªã‚¹ãƒ†ãƒ¼ãƒˆå®šç¾©

```python
from enum import Enum
from dataclasses import dataclass

class WorkflowStage(Enum):
    RESEARCH = "research"
    CHART = "chart"
    REVIEW = "review"
    COMPLETE = "complete"
    ERROR = "error"

class ComplexState(TypedDict):
    # å…¥åŠ›
    topic: str
    requirements: dict

    # å‡¦ç†çŠ¶æ…‹
    stage: str
    iteration: int
    max_iterations: int

    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆçµæœ
    research_data: list
    chart_data: dict
    review_feedback: list

    # ã‚¨ãƒ©ãƒ¼ç®¡ç†
    errors: list
    retry_count: int

    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
    last_api_call: float
    api_call_count: int
    current_llm_index: int

    # æœ€çµ‚çµæœ
    final_report: str
    messages: Annotated[list, operator.add]
```

## ğŸ›¡ï¸ ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼

```python
import time

class RateLimitManager:
    """APIãƒ¬ãƒ¼ãƒˆåˆ¶é™ç®¡ç†"""

    def __init__(self, calls_per_minute=10, llms=None):
        self.calls_per_minute = calls_per_minute
        self.min_interval = 60.0 / calls_per_minute
        self.llms = llms or [llm_primary, llm_backup]
        self.current_llm_index = 0

    async def wait_if_needed(self, last_call_time: float):
        """å¿…è¦ã«å¿œã˜ã¦å¾…æ©Ÿ"""
        if last_call_time > 0:
            elapsed = time.time() - last_call_time
            if elapsed < self.min_interval:
                wait_time = self.min_interval - elapsed
                await asyncio.sleep(wait_time)

    def get_llm(self, error_count: int = 0):
        """ã‚¨ãƒ©ãƒ¼å›æ•°ã«å¿œã˜ã¦LLMã‚’åˆ‡ã‚Šæ›¿ãˆ"""
        if error_count > 2:
            self.current_llm_index = (self.current_llm_index + 1) % len(self.llms)
        return self.llms[self.current_llm_index]

rate_limiter = RateLimitManager(calls_per_minute=15)
```

## ğŸ“Š ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒãƒ¼ãƒ‰ã®å®Ÿè£…

### Researcher ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

```python
async def researcher_node(state: ComplexState) -> dict:
    """ãƒªã‚µãƒ¼ãƒãƒ£ãƒ¼: ãƒˆãƒ”ãƒƒã‚¯ã‚’èª¿æŸ»"""
    await rate_limiter.wait_if_needed(state.get("last_api_call", 0))

    try:
        search_tool = TavilySearchResults(max_results=3)
        search_results = search_tool.invoke({"query": state["topic"]})

        llm = rate_limiter.get_llm(state.get("retry_count", 0))

        prompt = f"""
        ãƒˆãƒ”ãƒƒã‚¯: {state['topic']}
        æ¤œç´¢çµæœ: {search_results}

        ä¸Šè¨˜ã‚’åˆ†æã—ã€3ã¤ã®é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚
        """

        response = await llm.ainvoke([HumanMessage(content=prompt)])

        research_data = state.get("research_data", [])
        research_data.append({
            "iteration": state.get("iteration", 0),
            "content": response.content
        })

        return {
            "research_data": research_data,
            "stage": WorkflowStage.RESEARCH.value,
            "last_api_call": time.time(),
            "api_call_count": state.get("api_call_count", 0) + 1,
            "messages": [response]
        }
    except Exception as e:
        return {
            "errors": state.get("errors", []) + [str(e)],
            "retry_count": state.get("retry_count", 0) + 1,
            "stage": WorkflowStage.ERROR.value
        }
```

### Router ãƒãƒ¼ãƒ‰

```python
def researcher_router(state: ComplexState) -> str:
    """ãƒªã‚µãƒ¼ãƒå¾Œã®ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°åˆ¤å®š"""
    iteration = state.get("iteration", 0)
    max_iterations = state.get("max_iterations", 3)
    errors = state.get("errors", [])

    if len(errors) > 0:
        retry_count = state.get("retry_count", 0)
        if retry_count < 3:
            return "researcher"
        else:
            return "error_handler"

    if iteration < max_iterations:
        return "researcher"

    return "chart_generator"
```

## ğŸ¨ ã‚°ãƒ©ãƒ•æ§‹é€ ï¼ˆå…¨ä½“åƒï¼‰

```mermaid
flowchart TD
    START(["START"]) --> researcher["Researcher"]

    researcher --> r_router{"Routeråˆ¤å®š"}
    r_router -- "ç¶™ç¶š" --> researcher
    r_router -- "å®Œäº†" --> chart["Chart Generator"]
    r_router -- "ã‚¨ãƒ©ãƒ¼" --> error["Error Handler"]

    chart --> c_router{"Chartåˆ¤å®š"}
    c_router -- "å†ç”Ÿæˆ" --> chart
    c_router -- "ãƒ‡ãƒ¼ã‚¿ä¸è¶³" --> researcher
    c_router -- "å®Œäº†" --> reviewer["Reviewer"]
    c_router -- "ã‚¨ãƒ©ãƒ¼" --> error

    reviewer --> rev_router{"Reviewåˆ¤å®š"}
    rev_router -- "å†ãƒ¬ãƒ“ãƒ¥ãƒ¼" --> reviewer
    rev_router -- "ãƒªã‚µãƒ¼ãƒæˆ»ã—" --> researcher
    rev_router -- "ãƒãƒ£ãƒ¼ãƒˆæˆ»ã—" --> chart
    rev_router -- "æ‰¿èª" --> final["Final Report"]

    final --> END(["END"])
    error --> END
```

## ğŸ”€ æ¡ä»¶åˆ†å²ã®è©³ç´°

**10 ä»¥ä¸Šã®åˆ†å²ãƒã‚¤ãƒ³ãƒˆ**:

1. **Researcher Router**

   - ç¶™ç¶š â†’ Researcher
   - å®Œäº† â†’ Chart Generator
   - ã‚¨ãƒ©ãƒ¼ â†’ Error Handler

2. **Chart Router**

   - å†ç”Ÿæˆ â†’ Chart Generator
   - ãƒ‡ãƒ¼ã‚¿ä¸è¶³ â†’ Researcher
   - å®Œäº† â†’ Reviewer
   - ã‚¨ãƒ©ãƒ¼ â†’ Error Handler

3. **Review Router**

   - å†ãƒ¬ãƒ“ãƒ¥ãƒ¼ â†’ Reviewer
   - ãƒªã‚µãƒ¼ãƒè¦æ±‚ â†’ Researcher
   - ãƒãƒ£ãƒ¼ãƒˆä¿®æ­£ â†’ Chart Generator
   - æ‰¿èª â†’ Final Report

4. **ã‚¨ãƒ©ãƒ¼åˆ¤å®š**ï¼ˆå„ãƒãƒ¼ãƒ‰ã§ï¼‰
   - ãƒªãƒˆãƒ©ã‚¤å¯èƒ½ â†’ è©²å½“ãƒãƒ¼ãƒ‰
   - ãƒªãƒˆãƒ©ã‚¤ä¸Šé™ â†’ Error Handler

**åˆè¨ˆ**: 15 ä»¥ä¸Šã®æ¡ä»¶åˆ†å²çµŒè·¯

---

# ä»˜éŒ²: ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°å®Œå…¨ç‰ˆ

## âŒ ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã¨è§£æ±ºç­–

### 1. 429 Rate Limit Error

**ã‚¨ãƒ©ãƒ¼**: `google.api_core.exceptions.ResourceExhausted: 429`

**è§£æ±ºç­–**:

```python
# è§£æ±ºç­–1: ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
rate_limiter = RateLimitManager(calls_per_minute=10)

# è§£æ±ºç­–2: æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
@retry(
    wait=wait_exponential(multiplier=1, min=4, max=60),
    stop=stop_after_attempt(5)
)
def call_api():
    return llm.invoke(messages)

# è§£æ±ºç­–3: è¤‡æ•°APIã‚­ãƒ¼
api_keys = ["key1", "key2", "key3"]
current_key_index = 0

def get_llm():
    global current_key_index
    os.environ["GOOGLE_API_KEY"] = api_keys[current_key_index]
    current_key_index = (current_key_index + 1) % len(api_keys)
    return ChatGoogleGenerativeAI(model="gemini-2.0-flash-exp")
```

### 2. ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼

**ã‚¨ãƒ©ãƒ¼**: `asyncio.TimeoutError`

**è§£æ±ºç­–**:

```python
llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-exp",
    request_timeout=60,
    max_retries=3
)

async def call_with_timeout(llm, messages, timeout=30):
    try:
        return await asyncio.wait_for(
            llm.ainvoke(messages),
            timeout=timeout
        )
    except asyncio.TimeoutError:
        return await call_with_timeout(llm, messages, timeout * 1.5)
```

### 3. ãƒ¡ãƒ¢ãƒªä¸è¶³

**è§£æ±ºç­–**:

```python
def cleanup_state(state: dict) -> dict:
    """å¤ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‰Šé™¤"""
    if len(state.get("messages", [])) > 50:
        state["messages"] = state["messages"][-20:]
    return state
```

### 4. ã‚°ãƒ©ãƒ•å¯è¦–åŒ–ã‚¨ãƒ©ãƒ¼

**è§£æ±ºç­–**:

```python
# ä»£æ›¿æ‰‹æ®µ1: grandalf
pip install grandalf

# ä»£æ›¿æ‰‹æ®µ2: ASCIIè¡¨ç¤º
print(app.get_graph().draw_ascii())

# ä»£æ›¿æ‰‹æ®µ3: Mermaid
mermaid_code = app.get_graph().draw_mermaid()
print(mermaid_code)
```

## ğŸ’¡ ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### ã‚¹ãƒ†ãƒ¼ãƒˆè¨­è¨ˆ

```python
# âœ… è‰¯ã„ä¾‹
class State(TypedDict):
    user_input: str
    llm_output: str
    error_count: int

# âŒ æ‚ªã„ä¾‹
class State(TypedDict):
    data: dict
    stuff: list
```

### ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

```python
def safe_node(state: State) -> dict:
    try:
        result = risky_operation(state)
        return {"output": result}
    except Exception as e:
        return {
            "errors": state.get("errors", []) + [str(e)],
            "retry_count": state.get("retry_count", 0) + 1
        }
```

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°

```python
from functools import lru_cache

@lru_cache(maxsize=100)
def cached_llm_call(prompt: str) -> str:
    return llm.invoke([HumanMessage(content=prompt)]).content
```

### ãƒãƒƒãƒå‡¦ç†

```python
inputs = [{"input": f"è³ªå•{i}"} for i in range(10)]
results = await asyncio.gather(*[
    app.ainvoke(inp) for inp in inputs
])
```

---

## ğŸ“ ã¾ã¨ã‚

### ç¿’å¾—ã—ãŸå†…å®¹

âœ… **ç¬¬ 1 ç« **: æœ€å°æ§‹æˆ - Start â†’ Process â†’ End  
âœ… **ç¬¬ 2 ç« **: æ¡ä»¶åˆ†å²ã¨ãƒ„ãƒ¼ãƒ«çµ±åˆ  
âœ… **ç¬¬ 3 ç« **: ãƒ«ãƒ¼ãƒ—ã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°  
âœ… **ç¬¬ 4 ç« **: ä¸¦åˆ—å‡¦ç†ã¨éåŒæœŸå®Ÿè¡Œ  
âœ… **ç¬¬ 5 ç« **: 10 ä»¥ä¸Šã®åˆ†å²ã‚’æŒã¤è¤‡é›‘ãªã‚·ã‚¹ãƒ†ãƒ 

### å®Ÿè·µçš„ã‚¹ã‚­ãƒ«

- âœ… ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ï¼ˆ429 ã‚¨ãƒ©ãƒ¼å›é¿ï¼‰
- âœ… è¤‡æ•° LLM ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
- âœ… éåŒæœŸä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚‹é«˜é€ŸåŒ–
- âœ… è¤‡é›‘ãªæ¡ä»¶åˆ†å²ã¨ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
- âœ… ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå”èª¿ã‚·ã‚¹ãƒ†ãƒ 

**ã“ã‚Œã§ã€LangGraph ã®å®Œå…¨ãƒã‚¹ã‚¿ãƒ¼å®Œäº†ã§ã™ï¼Perfect! ğŸ‰**
