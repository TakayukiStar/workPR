# LangGraphå®Œå…¨ãƒã‚¹ã‚¿ãƒ¼ã‚¬ã‚¤ãƒ‰ 2025

**åˆå¿ƒè€…ã‹ã‚‰å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã¾ã§ - ã“ã®ä¸€å†Šã§å®Œçµã™ã‚‹æ±ºå®šç‰ˆã‚¬ã‚¤ãƒ‰**

---

## ğŸ“– æœ¬æ›¸ã®ä½¿ã„æ–¹

### ã“ã®ã‚¬ã‚¤ãƒ‰ã§é”æˆã§ãã‚‹ã“ã¨

âœ… LangGraphã®åŸºç¤ã‹ã‚‰å®Ÿè·µã¾ã§å®Œå…¨ç¿’å¾—  
âœ… Google AI Studio (Gemini) ã¨ã®çµ±åˆã‚’ãƒã‚¹ã‚¿ãƒ¼  
âœ… Tavilyæ¤œç´¢APIã‚’æ´»ç”¨ã—ãŸå®Ÿç”¨çš„ãªAIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ§‹ç¯‰  
âœ… å®Ÿã‚µãƒ¼ãƒ“ã‚¹ã§ä½¿ãˆã‚‹æœ¬æ ¼çš„ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè£…  
âœ… ã‚¨ãƒ©ãƒ¼å‡¦ç†ã€ä¸¦åˆ—å®Ÿè¡Œã€è¤‡é›‘ãªåˆ†å²ã®å®Ÿè£…

### å¯¾è±¡èª­è€…

- LangGraphæœªçµŒé¨“è€…
- PythonåŸºç¤çŸ¥è­˜ãŒã‚ã‚‹æ–¹
- å®Ÿå‹™ã§AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æ§‹ç¯‰ã—ãŸã„æ–¹

### å­¦ç¿’ã®é€²ã‚æ–¹

1. **ç¬¬1ç« ã¯å¿…èª­** - å…¨ã¦ã®åŸºç¤ã¨ãªã‚Šã¾ã™
2. **ã‚³ãƒ¼ãƒ‰ã¯å¿…ãšå®Ÿè¡Œ** - èª­ã‚€ã ã‘ã§ãªãå‹•ã‹ã—ã¦ç†è§£
3. **APIã‚­ãƒ¼ã‚’äº‹å‰æº–å‚™** - å¾Œè¿°ã®æ‰‹é †ã§å–å¾—ã—ã¦ãã ã•ã„

---

## ğŸ¯ ç›®æ¬¡

### ç¬¬0ç« : ç’°å¢ƒæº–å‚™ã¨APIã‚­ãƒ¼å–å¾—
- [0-1. å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«](#0-1-å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«)
- [0-2. APIã‚­ãƒ¼ã®å–å¾—æ–¹æ³•](#0-2-apiã‚­ãƒ¼ã®å–å¾—æ–¹æ³•)
- [0-3. APIã‚­ãƒ¼ã®è¨­å®šæ–¹æ³•](#0-3-apiã‚­ãƒ¼ã®è¨­å®šæ–¹æ³•)

### ç¬¬1ç« : LangGraphã®åŸºæœ¬æ¦‚å¿µ
- [1-1. LangGraphã¨ã¯ä½•ã‹](#1-1-langgraphã¨ã¯ä½•ã‹)
- [1-2. å¿…é ˆè¦ç´  vs ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¦ç´ ](#1-2-å¿…é ˆè¦ç´ -vs-ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¦ç´ )
- [1-3. StateGraphã®ä»•çµ„ã¿](#1-3-stategraphã®ä»•çµ„ã¿)
- [1-4. START/ENDã®çœŸå®Ÿ](#1-4-startendã®çœŸå®Ÿ)

### ç¬¬2ç« : æœ€å°æ§‹æˆã®å®Ÿè£…
- [2-1. 17è¡Œã§å‹•ãæœ€å°ã‚³ãƒ¼ãƒ‰](#2-1-17è¡Œã§å‹•ãæœ€å°ã‚³ãƒ¼ãƒ‰)
- [2-2. å®Ÿè·µçš„ãªåŸºæœ¬å®Ÿè£…](#2-2-å®Ÿè·µçš„ãªåŸºæœ¬å®Ÿè£…)
- [2-3. ã‚°ãƒ©ãƒ•å¯è¦–åŒ–ã®å®Ÿè£…](#2-3-ã‚°ãƒ©ãƒ•å¯è¦–åŒ–ã®å®Ÿè£…)
- [2-4. ã‚ˆãã‚ã‚‹è³ªå•30é¸](#2-4-ã‚ˆãã‚ã‚‹è³ªå•30é¸)

### ç¬¬3ç« : æ¡ä»¶åˆ†å²ã¨ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
- [3-1. æ¡ä»¶åˆ†å²ã®åŸºæœ¬](#3-1-æ¡ä»¶åˆ†å²ã®åŸºæœ¬)
- [3-2. è¤‡æ•°ãƒ«ãƒ¼ãƒˆã®å®Ÿè£…](#3-2-è¤‡æ•°ãƒ«ãƒ¼ãƒˆã®å®Ÿè£…)
- [3-3. å‹•çš„ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°](#3-3-å‹•çš„ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°)
- [3-4. ç¬¬3ç« å®Œå…¨å®Ÿè£…ä¾‹](#3-4-ç¬¬3ç« å®Œå…¨å®Ÿè£…ä¾‹)

### ç¬¬4ç« : ãƒ„ãƒ¼ãƒ«çµ±åˆ - Tavilyæ¤œç´¢
- [4-1. Tavilyæ¤œç´¢ã®åŸºæœ¬](#4-1-tavilyæ¤œç´¢ã®åŸºæœ¬)
- [4-2. LLMã¨ãƒ„ãƒ¼ãƒ«ã®é€£æº](#4-2-llmã¨ãƒ„ãƒ¼ãƒ«ã®é€£æº)
- [4-3. å®Ÿç”¨çš„ãªæ¤œç´¢ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ](#4-3-å®Ÿç”¨çš„ãªæ¤œç´¢ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ)
- [4-4. ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Ÿè¡Œ](#4-4-ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Ÿè¡Œ)
- [4-5. è¤‡æ•°ãƒ„ãƒ¼ãƒ«ã®çµ±åˆ](#4-5-è¤‡æ•°ãƒ„ãƒ¼ãƒ«ã®çµ±åˆ)

### ç¬¬5ç« : ãƒ«ãƒ¼ãƒ—å‡¦ç†ã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
- [5-1. ãƒ«ãƒ¼ãƒ—å‡¦ç†ã®åŸºæœ¬](#5-1-ãƒ«ãƒ¼ãƒ—å‡¦ç†ã®åŸºæœ¬)
- [5-2. æœ€å¤§è©¦è¡Œå›æ•°ã®åˆ¶å¾¡](#5-2-æœ€å¤§è©¦è¡Œå›æ•°ã®åˆ¶å¾¡)
- [5-3. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æˆ¦ç•¥](#5-3-ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æˆ¦ç•¥)
- [5-4. ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ã®å®Ÿè£…](#5-4-ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ã®å®Ÿè£…)
- [5-5. ç¬¬5ç« å®Œå…¨å®Ÿè£…ä¾‹](#5-5-ç¬¬5ç« å®Œå…¨å®Ÿè£…ä¾‹)

### ç¬¬6ç« : ä¸¦åˆ—å‡¦ç†ã¨éåŒæœŸå®Ÿè¡Œ
- [6-1. ä¸¦åˆ—ãƒãƒ¼ãƒ‰ã®å®Ÿè£…](#6-1-ä¸¦åˆ—ãƒãƒ¼ãƒ‰ã®å®Ÿè£…)
- [6-2. éåŒæœŸå‡¦ç†ã®ãƒ‘ã‚¿ãƒ¼ãƒ³](#6-2-éåŒæœŸå‡¦ç†ã®ãƒ‘ã‚¿ãƒ¼ãƒ³)
- [6-3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–](#6-3-ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–)
- [6-4. ç¬¬6ç« å®Œå…¨å®Ÿè£…ä¾‹](#6-4-ç¬¬6ç« å®Œå…¨å®Ÿè£…ä¾‹)

### ç¬¬7ç« : å®Ÿè·µçš„ãªã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰
- [7-1. ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ ](#7-1-ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ )
- [7-2. è¤‡é›‘ãªåˆ†å²ãƒ•ãƒ­ãƒ¼](#7-2-è¤‡é›‘ãªåˆ†å²ãƒ•ãƒ­ãƒ¼)
- [7-3. æœ¬ç•ªç’°å¢ƒã¸ã®å±•é–‹](#7-3-æœ¬ç•ªç’°å¢ƒã¸ã®å±•é–‹)
- [7-4. ç¬¬7ç« å®Œå…¨å®Ÿè£…ä¾‹](#7-4-ç¬¬7ç« å®Œå…¨å®Ÿè£…ä¾‹)

### ä»˜éŒ²
- [A. ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°å®Œå…¨ç‰ˆ](#ä»˜éŒ²a-ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°å®Œå…¨ç‰ˆ)
- [B. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°](#ä»˜éŒ²b-ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°)
- [C. ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹é›†](#ä»˜éŒ²c-ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹é›†)
- [3-4. ç¬¬3ç« å®Œå…¨å®Ÿè£…ä¾‹](#3-4-ç¬¬3ç« å®Œå…¨å®Ÿè£…ä¾‹)

---

# ç¬¬0ç« : ç’°å¢ƒæº–å‚™ã¨APIã‚­ãƒ¼å–å¾—

## 0-1. å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

### ã™ã¹ã¦ä¸€æ‹¬ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆæ¨å¥¨ï¼‰

```bash
# åŸºæœ¬ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ + å¯è¦–åŒ– + æ¤œç´¢ + éåŒæœŸ
pip install langgraph langchain-google-genai langchain-community \
            tavily-python pillow aiohttp tenacity

# å¯è¦–åŒ–ç”¨ï¼ˆã©ã¡ã‚‰ã‹ä¸€æ–¹ã§OKï¼‰
pip install pygraphviz  # æ¨å¥¨
# ã¾ãŸã¯
pip install grandalf    # pygraphvizãŒå‹•ã‹ãªã„å ´åˆ
```

### å€‹åˆ¥ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰

```bash
# ã‚³ã‚¢æ©Ÿèƒ½ï¼ˆå¿…é ˆï¼‰
pip install langgraph
pip install langchain-google-genai
pip install langchain-community

# æ¤œç´¢æ©Ÿèƒ½ï¼ˆç¬¬4ç« ä»¥é™ï¼‰
pip install tavily-python

# å¯è¦–åŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€å¼·ãæ¨å¥¨ï¼‰
pip install pillow
pip install pygraphviz  # ã¾ãŸã¯ grandalf

# éåŒæœŸå‡¦ç†ï¼ˆç¬¬6ç« ä»¥é™ï¼‰
pip install aiohttp

# ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ï¼ˆç¬¬5ç« ä»¥é™ï¼‰
pip install tenacity
```

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª

```python
# ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ã‚¨ãƒ©ãƒ¼ãŒå‡ºãªã‘ã‚Œã°OK
import langgraph
import langchain_google_genai
import tavily
print("âœ… ã™ã¹ã¦ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒæ­£ã—ãã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã™")
```

---

## 0-2. APIã‚­ãƒ¼ã®å–å¾—æ–¹æ³•

### Google AI Studio APIã‚­ãƒ¼å–å¾—

**æ‰€è¦æ™‚é–“: ç´„2åˆ†**

1. **Google AI Studioã«ã‚¢ã‚¯ã‚»ã‚¹**
   - URL: https://aistudio.google.com/apikey
   - Googleã‚¢ã‚«ã‚¦ãƒ³ãƒˆã§ãƒ­ã‚°ã‚¤ãƒ³

2. **APIã‚­ãƒ¼ã‚’ä½œæˆ**
   - ã€ŒCreate API Keyã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
   - ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’é¸æŠï¼ˆã¾ãŸã¯æ–°è¦ä½œæˆï¼‰
   - APIã‚­ãƒ¼ãŒç”Ÿæˆã•ã‚Œã‚‹ï¼ˆ`AIza...` ã§å§‹ã¾ã‚‹æ–‡å­—åˆ—ï¼‰

3. **APIã‚­ãƒ¼ã‚’ã‚³ãƒ”ãƒ¼**
   - è¡¨ç¤ºã•ã‚ŒãŸAPIã‚­ãƒ¼å…¨ä½“ã‚’ã‚³ãƒ”ãƒ¼
   - **é‡è¦**: ã“ã®ã‚­ãƒ¼ã¯å†è¡¨ç¤ºã§ããªã„ãŸã‚ã€å®‰å…¨ãªå ´æ‰€ã«ä¿å­˜

**æ³¨æ„äº‹é …:**
- APIã‚­ãƒ¼ã¯ä»–äººã«è¦‹ã›ãªã„
- GitHubãªã©ã«å…¬é–‹ã—ãªã„
- ç’°å¢ƒå¤‰æ•°ã¾ãŸã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§ç®¡ç†

### Tavily Search APIã‚­ãƒ¼å–å¾—

**æ‰€è¦æ™‚é–“: ç´„3åˆ†**

1. **Tavilyã«ã‚¢ã‚«ã‚¦ãƒ³ãƒˆç™»éŒ²**
   - URL: https://tavily.com/
   - ã€ŒSign Upã€ã‹ã‚‰ã‚¢ã‚«ã‚¦ãƒ³ãƒˆä½œæˆ

2. **APIã‚­ãƒ¼ã‚’å–å¾—**
   - ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã«ãƒ­ã‚°ã‚¤ãƒ³
   - ã€ŒAPI Keysã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ç§»å‹•
   - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§1ã¤ã®APIã‚­ãƒ¼ãŒç™ºè¡Œæ¸ˆã¿

3. **ç„¡æ–™ãƒ—ãƒ©ãƒ³ã®åˆ¶é™ç¢ºèª**
   - æœˆé–“1,000ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¾ã§ç„¡æ–™
   - ãã‚Œä»¥ä¸Šã¯æœ‰æ–™ãƒ—ãƒ©ãƒ³

---

## 0-3. APIã‚­ãƒ¼ã®è¨­å®šæ–¹æ³•

### æ–¹æ³•1: ã‚³ãƒ¼ãƒ‰å†…ã§ç›´æ¥è¨­å®šï¼ˆå­¦ç¿’ç”¨ï¼‰

```python
import os

# Gemini APIã‚­ãƒ¼ï¼ˆç’°å¢ƒå¤‰æ•°åã¯è‡ªç”±ã§ã™ãŒã€çµ±ä¸€æ¨å¥¨ï¼‰
os.environ["GEMINI_API_KEY"] = "AIza..."  # ã“ã“ã«å®Ÿéš›ã®ã‚­ãƒ¼ã‚’è²¼ã‚Šä»˜ã‘

# Tavily APIã‚­ãƒ¼ï¼ˆç¬¬4ç« ä»¥é™ã§ä½¿ç”¨ï¼‰
os.environ["TAVILY_API_KEY"] = "tvly-..."  # ã“ã“ã«å®Ÿéš›ã®ã‚­ãƒ¼ã‚’è²¼ã‚Šä»˜ã‘
```

**âš ï¸ ã“ã®æ–¹æ³•ã®æ³¨æ„ç‚¹:**
- ã‚³ãƒ¼ãƒ‰ã‚’GitHubã«ãƒ—ãƒƒã‚·ãƒ¥ã—ãªã„ã“ã¨
- å­¦ç¿’ãƒ»ãƒ†ã‚¹ãƒˆç›®çš„ã®ã¿ã§ä½¿ç”¨
- æœ¬ç•ªç’°å¢ƒã§ã¯æ–¹æ³•2ã‚’æ¨å¥¨

### æ–¹æ³•2: .envãƒ•ã‚¡ã‚¤ãƒ«ã§ç®¡ç†ï¼ˆæ¨å¥¨ï¼‰

#### ã‚¹ãƒ†ãƒƒãƒ—1: python-dotenvã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
pip install python-dotenv
```

#### ã‚¹ãƒ†ãƒƒãƒ—2: .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ

ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã« `.env` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ:

```bash
# .env
GEMINI_API_KEY=AIza...
TAVILY_API_KEY=tvly-...
```

#### ã‚¹ãƒ†ãƒƒãƒ—3: .gitignoreã«è¿½åŠ 

```bash
# .gitignore
.env
```

#### ã‚¹ãƒ†ãƒƒãƒ—4: ã‚³ãƒ¼ãƒ‰ã§èª­ã¿è¾¼ã¿

```python
import os
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

# ç’°å¢ƒå¤‰æ•°ã¨ã—ã¦åˆ©ç”¨å¯èƒ½
gemini_api_key = os.environ["GEMINI_API_KEY"]
tavily_api_key = os.environ["TAVILY_API_KEY"]

# ç¢ºèª
if not gemini_api_key:
    raise ValueError("âŒ GEMINI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
print("âœ… APIã‚­ãƒ¼ã®èª­ã¿è¾¼ã¿æˆåŠŸ")
```

### APIã‚­ãƒ¼è¨­å®šã®ç¢ºèªã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```python
import os

def check_api_keys():
    """APIã‚­ãƒ¼ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª"""
    
    issues = []
    
    # Gemini APIã‚­ãƒ¼ã®ç¢ºèª
    gemini_key = os.environ.get("GEMINI_API_KEY")
    if not gemini_key:
        issues.append("âŒ GEMINI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    elif gemini_key == "your-gemini-api-key-here":
        issues.append("âŒ GEMINI_API_KEY ãŒãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã®ã¾ã¾ã§ã™")
    else:
        print(f"âœ… GEMINI_API_KEY: {gemini_key[:10]}...")
    
    # Tavily APIã‚­ãƒ¼ã®ç¢ºèª
    tavily_key = os.environ.get("TAVILY_API_KEY")
    if not tavily_key:
        print("â„¹ï¸ TAVILY_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆç¬¬4ç« ä»¥é™ã§å¿…è¦ï¼‰")
    elif tavily_key == "your-tavily-api-key-here":
        print("â„¹ï¸ TAVILY_API_KEY ãŒãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã®ã¾ã¾ã§ã™")
    else:
        print(f"âœ… TAVILY_API_KEY: {tavily_key[:10]}...")
    
    if issues:
        print("\n".join(issues))
        raise ValueError("APIã‚­ãƒ¼ã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„")
    
    print("\nâœ… ã™ã¹ã¦ã®APIã‚­ãƒ¼ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã¾ã™")

# å®Ÿè¡Œ
check_api_keys()
```

---

# ç¬¬1ç« : LangGraphã®åŸºæœ¬æ¦‚å¿µ

## 1-1. LangGraphã¨ã¯ä½•ã‹

### LangGraphã®æœ¬è³ª

LangGraphã¯**ã‚¹ãƒ†ãƒ¼ãƒˆãƒ•ãƒ«ãªAIãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**ã§ã™ã€‚

**å¾“æ¥ã®ãƒã‚§ãƒ¼ãƒ³å‹AIã¨ã®é•ã„:**

```python
# âŒ å¾“æ¥ã®ãƒã‚§ãƒ¼ãƒ³å‹ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ã ãŒæŸ”è»Ÿæ€§ã«æ¬ ã‘ã‚‹ï¼‰
chain = prompt | llm | output_parser
result = chain.invoke({"input": "è³ªå•"})

# âœ… LangGraphï¼ˆè¤‡é›‘ãªãƒ•ãƒ­ãƒ¼ã‚’è¡¨ç¾å¯èƒ½ï¼‰
graph = StateGraph(State)
graph.add_node("analyze", analyze_input)
graph.add_node("search", search_web)
graph.add_node("summarize", summarize_results)
graph.add_conditional_edges("analyze", route_decision)
# ... æŸ”è»Ÿãªåˆ¶å¾¡ãƒ•ãƒ­ãƒ¼
```

### LangGraphãŒå¾—æ„ãªã“ã¨

1. **è¤‡é›‘ãªæ¡ä»¶åˆ†å²**
   - ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã«å¿œã˜ã¦ç•°ãªã‚‹å‡¦ç†ãƒ«ãƒ¼ãƒˆã‚’é¸æŠ
   - LLMã®åˆ¤æ–­ã§æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ±ºå®š

2. **ãƒ«ãƒ¼ãƒ—å‡¦ç†**
   - æº€è¶³ã„ãçµæœãŒå¾—ã‚‰ã‚Œã‚‹ã¾ã§ç¹°ã‚Šè¿”ã—
   - ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆçš„ãªæŒ¯ã‚‹èˆã„

3. **ä¸¦åˆ—å®Ÿè¡Œ**
   - è¤‡æ•°ã®ã‚¿ã‚¹ã‚¯ã‚’åŒæ™‚å®Ÿè¡Œ
   - çµæœã‚’çµ±åˆ

4. **ã‚¹ãƒ†ãƒ¼ãƒˆç®¡ç†**
   - ãƒãƒ¼ãƒ‰é–“ã§ãƒ‡ãƒ¼ã‚¿ã‚’å…±æœ‰
   - å®Ÿè¡Œå±¥æ­´ã®è¿½è·¡

### å®Ÿéš›ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹

```
# ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒœãƒƒãƒˆ
User Input â†’ Intentåˆ†é¡ â†’ 
    â”œâ”€ FAQæ¤œç´¢ â†’ å›ç­”ç”Ÿæˆ
    â”œâ”€ ãƒã‚±ãƒƒãƒˆä½œæˆ â†’ æ‹…å½“è€…ã‚¢ã‚µã‚¤ãƒ³
    â””â”€ ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ â†’ äººé–“ã«è»¢é€

# ãƒªã‚µãƒ¼ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
Query â†’ Webæ¤œç´¢ â†’ çµæœè©•ä¾¡ â†’
    â””â”€ ä¸ååˆ† â†’ è¿½åŠ æ¤œç´¢ï¼ˆãƒ«ãƒ¼ãƒ—ï¼‰
    â””â”€ ååˆ† â†’ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

# ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
Topic â†’ ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ³ç”Ÿæˆ â†’
    â”œâ”€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³1åŸ·ç­†ï¼ˆä¸¦åˆ—ï¼‰
    â”œâ”€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³2åŸ·ç­†ï¼ˆä¸¦åˆ—ï¼‰
    â””â”€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³3åŸ·ç­†ï¼ˆä¸¦åˆ—ï¼‰
    â†’ çµ±åˆ â†’ æ ¡æ­£ â†’ å®Œæˆ
```

---

## 1-2. å¿…é ˆè¦ç´  vs ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¦ç´ 

### ğŸ”´ çµ¶å¯¾ã«å¿…è¦ãª5ã¤ã®è¦ç´ 

#### 1. StateGraph

```python
from langgraph.graph import StateGraph

workflow = StateGraph(State)  # ã“ã‚ŒãŒãªã„ã¨ä½•ã‚‚å§‹ã¾ã‚‰ãªã„
```

**å½¹å‰²**: ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®è¨­è¨ˆå›³ã‚’ä½œæˆ

#### 2. State (TypedDict)

```python
from typing import TypedDict

class State(TypedDict):
    input: str
    output: str
```

**å½¹å‰²**: ãƒãƒ¼ãƒ‰é–“ã§å…±æœ‰ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®å‹ã‚’å®šç¾©

**ãªãœå¿…é ˆ?**
- ãƒãƒ¼ãƒ‰é–“ã§ãƒ‡ãƒ¼ã‚¿ã‚’å—ã‘æ¸¡ã™ãŸã‚
- å‹å®‰å…¨æ€§ã‚’ç¢ºä¿ã™ã‚‹ãŸã‚

#### 3. add_node()

```python
workflow.add_node("process", process_function)
```

**å½¹å‰²**: å®Ÿè¡Œã™ã‚‹å‡¦ç†ï¼ˆãƒãƒ¼ãƒ‰ï¼‰ã‚’è¿½åŠ 

#### 4. add_edge()

```python
workflow.add_edge(START, "process")
workflow.add_edge("process", END)
```

**å½¹å‰²**: ãƒãƒ¼ãƒ‰åŒå£«ã‚’æ¥ç¶š

#### 5. compile()

```python
app = workflow.compile()
```

**å½¹å‰²**: å®Ÿè¡Œå¯èƒ½ãªå½¢å¼ã«å¤‰æ›

**ãªãœå¿…é ˆ?**
- `workflow`ã¯è¨­è¨ˆå›³ï¼ˆå®Ÿè¡Œä¸å¯ï¼‰
- `app`ã¯å®Ÿè¡Œå¯èƒ½ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³

---

### ğŸŸ¡ è‡ªå‹•ã§ç”¨æ„ã•ã‚Œã‚‹ã‚‚ã®ï¼ˆæš—é»™çš„ï¼‰

#### START / END

```python
from langgraph.graph import START, END

# ã“ã‚Œã‚‰ã¯è‡ªå‹•çš„ã«åˆ©ç”¨å¯èƒ½
workflow.add_edge(START, "first_node")
workflow.add_edge("last_node", END)
```

**çœŸå®Ÿ:**
- `START`ã¨`END`ã¯**å®šæ•°**ã¨ã—ã¦æä¾›ã•ã‚Œã‚‹
- å†…éƒ¨çš„ã«ã¯`__start__`ã¨`__end__`ã¨ã„ã†åå‰ã«å¤‰æ›ã•ã‚Œã‚‹
- æ›¸ã‹ãªãã¦ã‚‚ã‚°ãƒ©ãƒ•ã¯å‹•ããŒã€**99%ã®ã‚±ãƒ¼ã‚¹ã§ä½¿ã†**

#### __start__ / __end__

```python
# âŒ ã‚³ãƒ¼ãƒ‰ã§æ›¸ã‹ãªã„
workflow.add_edge("__start__", "node")

# âœ… ä»£ã‚ã‚Šã«STARTã‚’ä½¿ã†
workflow.add_edge(START, "node")
```

**çœŸå®Ÿ:**
- ã“ã‚Œã‚‰ã¯**å†…éƒ¨å**
- ã‚°ãƒ©ãƒ•å¯è¦–åŒ–æ™‚ã«è¡¨ç¤ºã•ã‚Œã‚‹
- **ç›´æ¥ä½¿ã†ã“ã¨ã¯çµ¶å¯¾ã«ãªã„**

---

### ğŸŸ¢ ã‚ã‚‹ã¨ä¾¿åˆ©ãªã‚‚ã®ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

#### set_entry_point() / set_finish_point()

```python
# å¤ã„æ›¸ãæ–¹ï¼ˆéæ¨å¥¨ï¼‰
workflow.set_entry_point("start_node")
workflow.set_finish_point("end_node")

# âœ… æ¨å¥¨ã•ã‚Œã‚‹æ›¸ãæ–¹
workflow.add_edge(START, "start_node")
workflow.add_edge("end_node", END)
```

**ãªãœéæ¨å¥¨?**
- `add_edge(START, ...)`ã®æ–¹ãŒä¸€è²«æ€§ãŒã‚ã‚‹
- å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã‚‚`START`/`END`ã‚’æ¨å¥¨

#### ã‚°ãƒ©ãƒ•å¯è¦–åŒ–

```python
# ãªãã¦ã‚‚å‹•ããŒã€ã‚ã‚‹ã¨è¶…ä¾¿åˆ©
png_data = app.get_graph().draw_mermaid_png()
```

**æ¨å¥¨åº¦**: â­â­â­â­â­ï¼ˆè¤‡é›‘ãªã‚°ãƒ©ãƒ•ã§ã¯å¿…é ˆãƒ¬ãƒ™ãƒ«ï¼‰

---

## 1-3. StateGraphã®ä»•çµ„ã¿

### Stateã®å½¹å‰²

**Stateã¯ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å…¨ä½“ã§å…±æœ‰ã•ã‚Œã‚‹ã€Œãƒ¡ãƒ¢ãƒªã€**

```python
from typing import TypedDict

class State(TypedDict):
    input: str       # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
    output: str      # æœ€çµ‚çµæœ
    intermediate: str  # ä¸­é–“çµæœ
    count: int       # å‡¦ç†å›æ•°
```

### ãƒãƒ¼ãƒ‰é–¢æ•°ã®ãƒ«ãƒ¼ãƒ«

#### çµ¶å¯¾ã«å®ˆã‚‹ã¹ã2ã¤ã®ãƒ«ãƒ¼ãƒ«

**ãƒ«ãƒ¼ãƒ«1: å¼•æ•°ã¯å¿…ãš `state`**

```python
# âœ… æ­£ã—ã„
def my_node(state: State) -> dict:
    user_input = state["input"]
    return {"output": "å‡¦ç†æ¸ˆã¿"}

# âŒ é–“é•ã„ - å¼•æ•°åãŒé•ã†
def my_node(data: State) -> dict:
    return {"output": "å‡¦ç†æ¸ˆã¿"}

# âŒ é–“é•ã„ - å‹ãƒ’ãƒ³ãƒˆãªã—ï¼ˆå‹•ããŒéæ¨å¥¨ï¼‰
def my_node(state):
    return {"output": "å‡¦ç†æ¸ˆã¿"}
```

**ãƒ«ãƒ¼ãƒ«2: æˆ»ã‚Šå€¤ã¯å¿…ãš `dict`**

```python
# âœ… æ­£ã—ã„
def my_node(state: State) -> dict:
    return {"output": "çµæœ"}

# âŒ é–“é•ã„ - strã‚’è¿”ã™
def my_node(state: State) -> str:
    return "çµæœ"

# âŒ é–“é•ã„ - Stateã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™
def my_node(state: State) -> State:
    return state
```

### Stateã®æ›´æ–°ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 

**é‡è¦: æˆ»ã‚Šå€¤ã®`dict`ã¯è‡ªå‹•çš„ã«ãƒãƒ¼ã‚¸ã•ã‚Œã‚‹**

```python
from typing import TypedDict

class State(TypedDict):
    input: str
    output: str
    count: int

def increment_node(state: State) -> dict:
    # ç¾åœ¨ã® state
    # {"input": "test", "output": "", "count": 0}
    
    # countã ã‘æ›´æ–°ã—ã¦è¿”ã™
    return {"count": state["count"] + 1}
    
    # è‡ªå‹•ãƒãƒ¼ã‚¸å¾Œã® state
    # {"input": "test", "output": "", "count": 1}
    # â†‘ inputã¨outputã¯ãã®ã¾ã¾æ®‹ã‚‹ï¼
```

**ãƒãƒ¼ã‚¸ã®ãƒ«ãƒ¼ãƒ«:**

```python
# å®Ÿè¡Œå‰
state = {"input": "Hello", "output": "", "count": 0}

# ãƒãƒ¼ãƒ‰ãŒè¿”ã™
return {"output": "Processed", "count": 1}

# è‡ªå‹•ãƒãƒ¼ã‚¸å¾Œ
state = {"input": "Hello", "output": "Processed", "count": 1}
#        â†‘ ãã®ã¾ã¾    â†‘ æ›´æ–°          â†‘ æ›´æ–°
```

### è¤‡æ•°ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®æ›´æ–°

```python
def process_node(state: State) -> dict:
    # è¤‡æ•°ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’åŒæ™‚æ›´æ–°
    return {
        "output": "å®Œäº†",
        "count": state["count"] + 1,
        "intermediate": "ä¸­é–“ãƒ‡ãƒ¼ã‚¿"
    }
```

### ã‚ˆãã‚ã‚‹ç–‘å•: å…¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿”ã™å¿…è¦ã¯?

**A: ã‚ã‚Šã¾ã›ã‚“ï¼æ›´æ–°ã—ãŸã„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã ã‘è¿”ã›ã°OK**

```python
# âœ… ã“ã‚Œã§OKï¼ˆoutputã ã‘æ›´æ–°ï¼‰
def node1(state: State) -> dict:
    return {"output": "çµæœ"}

# âœ… ã“ã‚Œã‚‚OKï¼ˆcountã ã‘æ›´æ–°ï¼‰
def node2(state: State) -> dict:
    return {"count": state["count"] + 1}

# âŒ ã“ã‚Œã¯å†—é•·ï¼ˆä¸è¦ï¼‰
def node3(state: State) -> dict:
    return {
        "input": state["input"],    # æ›´æ–°ã—ã¦ãªã„ã®ã«è¿”ã—ã¦ã„ã‚‹
        "output": "çµæœ",
        "count": state["count"]     # æ›´æ–°ã—ã¦ãªã„ã®ã«è¿”ã—ã¦ã„ã‚‹
    }
```

---

## 1-4. START/ENDã®çœŸå®Ÿ

### ã‚ˆãã‚ã‚‹èª¤è§£ã‚’è§£æ¶ˆ

#### èª¤è§£1: "STARTã¨ENDã¯æ›¸ã‹ãªã„ã¨ã„ã‘ãªã„"

**çœŸå®Ÿ: æ›¸ã‹ãªãã¦ã‚‚å‹•ãã€ã§ã‚‚99%æ›¸ã**

```python
# âŒ ç†è«–ä¸Šã¯å‹•ããŒã€å®Ÿè·µã§ã¯ä½¿ã‚ãªã„
workflow = StateGraph(State)
workflow.add_node("node1", func1)
workflow.add_node("node2", func2)
workflow.add_edge("node1", "node2")
app = workflow.compile()

# âœ… å®Ÿè·µã§ã¯ã“ã†æ›¸ã
workflow = StateGraph(State)
workflow.add_node("node1", func1)
workflow.add_node("node2", func2)
workflow.add_edge(START, "node1")  # æ˜ç¤ºçš„ãªé–‹å§‹ç‚¹
workflow.add_edge("node2", END)     # æ˜ç¤ºçš„ãªçµ‚äº†ç‚¹
app = workflow.compile()
```

#### èª¤è§£2: "__start__ã‚’ä½¿ã†ã¹ã"

**çœŸå®Ÿ: çµ¶å¯¾ã«ä½¿ã‚ãªã„**

```python
# âŒ çµ¶å¯¾ã«ã“ã†æ›¸ã‹ãªã„
workflow.add_edge("__start__", "node1")

# âœ… å¿…ãšã“ã†æ›¸ã
workflow.add_edge(START, "node1")
```

**ãªãœ?**

| é …ç›® | `START` | `__start__` |
|------|---------|-------------|
| ç”¨é€” | ã‚³ãƒ¼ãƒ‰ã§ä½¿ã† | ã‚°ãƒ©ãƒ•è¡¨ç¤ºã®ã¿ |
| å…¬é–‹API | âœ… ã¯ã„ | âŒ ã„ã„ãˆ |
| æ¨å¥¨ | âœ… æ¨å¥¨ | âŒ éæ¨å¥¨ |

#### èª¤è§£3: "STARTã¨ENDã¯ç‰¹åˆ¥ãªãƒãƒ¼ãƒ‰"

**çœŸå®Ÿ: ãŸã ã®ãƒãƒ¼ã‚«ãƒ¼ï¼ˆå®šæ•°ï¼‰**

```python
from langgraph.graph import START, END

# å®Ÿéš›ã®å®Ÿè£…ï¼ˆç°¡ç•¥ç‰ˆï¼‰
START = "__start__"  # å˜ãªã‚‹æ–‡å­—åˆ—å®šæ•°
END = "__end__"      # å˜ãªã‚‹æ–‡å­—åˆ—å®šæ•°

# ã ã‹ã‚‰ã“ã†ä½¿ãˆã‚‹
workflow.add_edge(START, "first")  # "__start__" â†’ "first"
workflow.add_edge("last", END)     # "last" â†’ "__end__"
```

### ã‚°ãƒ©ãƒ•å¯è¦–åŒ–ã§ã®è¡¨ç¤º

**ã‚³ãƒ¼ãƒ‰ã¨ã‚°ãƒ©ãƒ•è¡¨ç¤ºã®å¯¾å¿œè¡¨**

| ã‚³ãƒ¼ãƒ‰ã§æ›¸ã | ã‚°ãƒ©ãƒ•ã§è¡¨ç¤º |
|------------|-------------|
| `START` | `__start__` |
| `"my_node"` | `my_node` |
| `"å‡¦ç†1"` | `å‡¦ç†1` |
| `END` | `__end__` |

**ä¾‹:**

```python
workflow.add_edge(START, "analyze")
workflow.add_edge("analyze", "process")
workflow.add_edge("process", END)
```

**ã‚°ãƒ©ãƒ•è¡¨ç¤º:**
```
__start__ â†’ analyze â†’ process â†’ __end__
```

### å®Ÿè·µçš„ãªä½¿ã„æ–¹

```python
import os
from langgraph.graph import StateGraph, START, END
from typing import TypedDict

# APIã‚­ãƒ¼è¨­å®š
os.environ["GEMINI_API_KEY"] = "your-api-key"

class State(TypedDict):
    data: str

def node_a(state: State) -> dict:
    return {"data": state["data"] + " â†’ A"}

def node_b(state: State) -> dict:
    return {"data": state["data"] + " â†’ B"}

# âœ… æ¨å¥¨ãƒ‘ã‚¿ãƒ¼ãƒ³
workflow = StateGraph(State)
workflow.add_node("a", node_a)
workflow.add_node("b", node_b)

# æ˜ç¤ºçš„ãªé–‹å§‹ã¨çµ‚äº†
workflow.add_edge(START, "a")
workflow.add_edge("a", "b")
workflow.add_edge("b", END)

app = workflow.compile()

result = app.invoke({"data": "é–‹å§‹"})
print(result["data"])  # "é–‹å§‹ â†’ A â†’ B"
```

---

# ç¬¬2ç« : æœ€å°æ§‹æˆã®å®Ÿè£…

## 2-1. 17è¡Œã§å‹•ãæœ€å°ã‚³ãƒ¼ãƒ‰

### çµ¶å¯¾æœ€å°é™ã®ã‚³ãƒ¼ãƒ‰

```python
import os
from typing import TypedDict
from langgraph.graph import StateGraph, START, END
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage

os.environ["GEMINI_API_KEY"] = "your-actual-api-key-here"

class State(TypedDict):
    output: str

llm = ChatGoogleGenerativeAI(
    google_api_key=os.environ["GEMINI_API_KEY"],
    model="gemini-2.0-flash-exp"
)

workflow = StateGraph(State)
workflow.add_node("llm", lambda s: {"output": llm.invoke([HumanMessage("ã“ã‚“ã«ã¡ã¯")]).content})
workflow.add_edge(START, "llm")
workflow.add_edge("llm", END)
app = workflow.compile()

print(app.invoke({"output": ""})["output"])
```

**ã“ã‚ŒãŒå‹•ãæœ€å°å˜ä½ã§ã™ã€‚ä»¥é™ã¯ã“ã‚Œã‚’èª­ã¿ã‚„ã™ãå±•é–‹ã—ã¦ã„ãã¾ã™ã€‚**

---

## 2-2. å®Ÿè·µçš„ãªåŸºæœ¬å®Ÿè£…

### ã‚¹ãƒ†ãƒƒãƒ—1: ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

```python
import os
from typing import TypedDict
from langgraph.graph import StateGraph, START, END
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage
```

**å„ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®å½¹å‰²:**

| ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« | å¿…é ˆ? | å½¹å‰² |
|-----------|------|------|
| `os` | ğŸ”´å¿…é ˆ | ç’°å¢ƒå¤‰æ•°ã®è¨­å®šãƒ»å–å¾— |
| `TypedDict` | ğŸ”´ | Stateã®å‹å®šç¾© |
| `StateGraph` | ğŸ”´ | ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ä½œæˆ |
| `START, END` | ğŸŸ¡æ¨å¥¨ | é–‹å§‹ãƒ»çµ‚äº†ãƒãƒ¼ã‚«ãƒ¼ |
| `ChatGoogleGenerativeAI` | ğŸ”´ | Gemini LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ |
| `HumanMessage` | ğŸ”´ | LLMã¸ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼ |

### ã‚¹ãƒ†ãƒƒãƒ—2: APIã‚­ãƒ¼è¨­å®š

```python
import os

# APIã‚­ãƒ¼è¨­å®šï¼ˆå¿…é ˆï¼‰
os.environ["GEMINI_API_KEY"] = "AIza..."

# æ¤œè¨¼
if not os.environ.get("GEMINI_API_KEY"):
    raise ValueError("âŒ APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

print(f"âœ… APIã‚­ãƒ¼è¨­å®šOK: {os.environ['GEMINI_API_KEY'][:10]}...")
```

### ã‚¹ãƒ†ãƒƒãƒ—3: Stateå®šç¾©

```python
class State(TypedDict):
    input: str   # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
    output: str  # LLMå‡ºåŠ›
```

**Stateã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹:**

```python
# âœ… æ˜ç¢ºãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å
class State(TypedDict):
    user_query: str
    llm_response: str
    search_results: str
    final_answer: str

# âŒ æ›–æ˜§ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å
class State(TypedDict):
    data: str
    result: str
    info: str
```

### ã‚¹ãƒ†ãƒƒãƒ—4: LLMåˆæœŸåŒ–

```python
llm = ChatGoogleGenerativeAI(
    google_api_key=os.environ["GEMINI_API_KEY"],  # APIã‚­ãƒ¼æ˜ç¤º
    model="gemini-2.0-flash-exp",                 # ãƒ¢ãƒ‡ãƒ«æŒ‡å®š
    temperature=0.7,                              # å‰µé€ æ€§ï¼ˆ0ã€œ1ï¼‰
    max_tokens=1024                               # æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
)
```

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è§£èª¬:**

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | èª¬æ˜ | æ¨å¥¨å€¤ |
|-----------|----------|------|-------|
| `google_api_key` | ç’°å¢ƒå¤‰æ•° | APIã‚­ãƒ¼ | æ˜ç¤ºæ¨å¥¨ |
| `model` | - | ä½¿ç”¨ãƒ¢ãƒ‡ãƒ« | `gemini-2.0-flash-exp` |
| `temperature` | 1.0 | ãƒ©ãƒ³ãƒ€ãƒ æ€§ | 0.7ï¼ˆãƒãƒ©ãƒ³ã‚¹å‹ï¼‰ |
| `max_tokens` | - | å‡ºåŠ›ä¸Šé™ | 1024ã€œ2048 |

**temperatureã®é¸ã³æ–¹:**

```python
# äº‹å®Ÿãƒ™ãƒ¼ã‚¹ã®å›ç­”
llm_factual = ChatGoogleGenerativeAI(
    google_api_key=os.environ["GEMINI_API_KEY"],
    model="gemini-2.0-flash-exp",
    temperature=0.1  # ã»ã¼æ±ºå®šçš„
)

# ãƒãƒ©ãƒ³ã‚¹å‹
llm_balanced = ChatGoogleGenerativeAI(
    google_api_key=os.environ["GEMINI_API_KEY"],
    model="gemini-2.0-flash-exp",
    temperature=0.7
)

# å‰µé€ çš„ãªå‡ºåŠ›
llm_creative = ChatGoogleGenerativeAI(
    google_api_key=os.environ["GEMINI_API_KEY"],
    model="gemini-2.0-flash-exp",
    temperature=0.9
)
```

### ã‚¹ãƒ†ãƒƒãƒ—5: ãƒãƒ¼ãƒ‰é–¢æ•°å®šç¾©

```python
def call_gemini(state: State) -> dict:
    """
    Gemini APIã‚’å‘¼ã³å‡ºã™ãƒãƒ¼ãƒ‰
    
    Args:
        state: ç¾åœ¨ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çŠ¶æ…‹
        
    Returns:
        dict: æ›´æ–°ã™ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
    """
    # 1. å…¥åŠ›å–å¾—
    user_input = state["input"]
    
    # 2. LLMå‘¼ã³å‡ºã—
    response = llm.invoke([HumanMessage(content=user_input)])
    
    # 3. çµæœã‚’è¿”ã™
    return {"output": response.content}
```

**HumanMessageã®ä½¿ã„æ–¹:**

```python
# åŸºæœ¬å½¢
message = HumanMessage(content="ã“ã‚“ã«ã¡ã¯")

# è¤‡æ•°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
from langchain_core.messages import SystemMessage

messages = [
    SystemMessage(content="ã‚ãªãŸã¯è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™"),
    HumanMessage(content="LangGraphã«ã¤ã„ã¦æ•™ãˆã¦")
]
response = llm.invoke(messages)
```

### ã‚¹ãƒ†ãƒƒãƒ—6: ã‚°ãƒ©ãƒ•æ§‹ç¯‰

```python
# 1. ã‚°ãƒ©ãƒ•ä½œæˆ
workflow = StateGraph(State)

# 2. ãƒãƒ¼ãƒ‰è¿½åŠ 
workflow.add_node("gemini", call_gemini)

# 3. ã‚¨ãƒƒã‚¸è¿½åŠ ï¼ˆãƒ•ãƒ­ãƒ¼å®šç¾©ï¼‰
workflow.add_edge(START, "gemini")  # é–‹å§‹ â†’ gemini
workflow.add_edge("gemini", END)     # gemini â†’ çµ‚äº†

# 4. ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
app = workflow.compile()
```

**add_nodeã®æŸ”è»Ÿæ€§:**

```python
# ãƒ‘ã‚¿ãƒ¼ãƒ³1: é–¢æ•°ã‚’æ¸¡ã™
def my_func(state: State) -> dict:
    return {"output": "çµæœ"}
workflow.add_node("node1", my_func)

# ãƒ‘ã‚¿ãƒ¼ãƒ³2: ãƒ©ãƒ ãƒ€å¼
workflow.add_node("node2", lambda s: {"output": "çµæœ"})

# ãƒ‘ã‚¿ãƒ¼ãƒ³3: ã‚¯ãƒ©ã‚¹ãƒ¡ã‚½ãƒƒãƒ‰
class MyProcessor:
    def process(self, state: State) -> dict:
        return {"output": "çµæœ"}

processor = MyProcessor()
workflow.add_node("node3", processor.process)
```

### ã‚¹ãƒ†ãƒƒãƒ—7: å®Ÿè¡Œ

```python
# æœ€å°å®Ÿè¡Œ
result = app.invoke({"input": "ã“ã‚“ã«ã¡ã¯", "output": ""})
print(result["output"])

# å®Ÿè·µçš„ãªå®Ÿè¡Œé–¢æ•°
def run_workflow(app, user_input: str, verbose: bool = True):
    """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œ"""
    if verbose:
        print(f"ğŸ“ å…¥åŠ›: {user_input}")
    
    result = app.invoke({"input": user_input, "output": ""})
    
    if verbose:
        print(f"âœ… å‡ºåŠ›: {result['output']}")
    
    return result

# ä½¿ç”¨ä¾‹
run_workflow(app, "LangGraphã®ç‰¹å¾´ã‚’æ•™ãˆã¦ãã ã•ã„")
```

---

## 2-3. ã‚°ãƒ©ãƒ•å¯è¦–åŒ–ã®å®Ÿè£…

### å¯è¦–åŒ–é–¢æ•°ï¼ˆæ±ç”¨ç‰ˆï¼‰

```python
def visualize_graph(app, filename: str = "workflow.png", show_ascii: bool = False):
    """
    ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚°ãƒ©ãƒ•ã‚’å¯è¦–åŒ–
    
    Args:
        app: ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ã‚¢ãƒ—ãƒª
        filename: ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«å
        show_ascii: PNGå¤±æ•—æ™‚ã«ASCIIç‰ˆã‚’è¡¨ç¤º
    """
    print(f"\nğŸ“Š ã‚°ãƒ©ãƒ•ã‚’å¯è¦–åŒ–ä¸­...")
    
    try:
        # PNGç”Ÿæˆ
        png_data = app.get_graph().draw_mermaid_png()
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        with open(filename, "wb") as f:
            f.write(png_data)
        
        print(f"âœ… '{filename}' ã«ä¿å­˜ã—ã¾ã—ãŸ")
        
    except Exception as e:
        print(f"âš ï¸ PNGä¿å­˜å¤±æ•—: {e}")
        
        if show_ascii:
            print("\nğŸ“ ASCIIç‰ˆã‚°ãƒ©ãƒ•:")
            print(app.get_graph().draw_ascii())
```

### ä½¿ç”¨ä¾‹

```python
# åŸºæœ¬ä½¿ç”¨
visualize_graph(app)

# ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚¡ã‚¤ãƒ«å
visualize_graph(app, filename="my_workflow.png")

# ASCIIç‰ˆã‚‚è¡¨ç¤º
visualize_graph(app, show_ascii=True)
```

### ã‚°ãƒ©ãƒ•æ§‹é€ ã®ç†è§£

```python
import os
from langgraph.graph import StateGraph, START, END
from typing import TypedDict

os.environ["GEMINI_API_KEY"] = "your-api-key"

class State(TypedDict):
    data: str

workflow = StateGraph(State)
workflow.add_node("process", lambda s: {"data": "å‡¦ç†æ¸ˆã¿"})
workflow.add_edge(START, "process")
workflow.add_edge("process", END)
app = workflow.compile()
```

**ç”Ÿæˆã•ã‚Œã‚‹ã‚°ãƒ©ãƒ•:**

```
__start__ â†’ process â†’ __end__
```

**é‡è¦ãƒã‚¤ãƒ³ãƒˆ:**
- `START` â†’ ã‚°ãƒ©ãƒ•ã§ã¯ `__start__`
- `"process"` â†’ ã‚°ãƒ©ãƒ•ã§ã‚‚ `process`
- `END` â†’ ã‚°ãƒ©ãƒ•ã§ã¯ `__end__`

---

## 2-4. ã‚ˆãã‚ã‚‹è³ªå•30é¸

### åŸºæœ¬æ¦‚å¿µ

#### Q1: LangGraphã¨LangChainã®é•ã„ã¯?

**A:** LangChainã¯**ç·šå½¢çš„ãªãƒã‚§ãƒ¼ãƒ³**ã€LangGraphã¯**ã‚°ãƒ©ãƒ•æ§‹é€ **ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æ‰±ã„ã¾ã™ã€‚

```python
# LangChain - ã‚·ãƒ³ãƒ—ãƒ«ã ãŒæŸ”è»Ÿæ€§ã«æ¬ ã‘ã‚‹
chain = prompt | llm | output_parser
result = chain.invoke({"input": "è³ªå•"})

# LangGraph - è¤‡é›‘ã ãŒæŸ”è»Ÿ
graph = StateGraph(State)
graph.add_conditional_edges(...)  # æ¡ä»¶åˆ†å²
graph.add_edge(..., ..., condition=...)  # ãƒ«ãƒ¼ãƒ—
```

#### Q2: StateGraphã¯å¿…é ˆ?

**A:** ã¯ã„ã€**çµ¶å¯¾å¿…é ˆ**ã§ã™ã€‚ã“ã‚ŒãŒãªã„ã¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ä½œã‚Œã¾ã›ã‚“ã€‚

#### Q3: Stateã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯ä½•å€‹ã¾ã§?

**A:** **åˆ¶é™ãªã—**ã€‚1å€‹ã§ã‚‚100å€‹ã§ã‚‚OKã§ã™ã€‚

```python
# âœ… æœ€å°ï¼ˆ1ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼‰
class MinimalState(TypedDict):
    data: str

# âœ… å®Ÿç”¨çš„ï¼ˆè¤‡æ•°ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼‰
class RealState(TypedDict):
    query: str
    search_results: list
    summary: str
    final_answer: str
    metadata: dict
```

#### Q4: TypedDictã¯å¿…é ˆ?

**A:** æŠ€è¡“çš„ã«ã¯`dict`ã§ã‚‚å‹•ãã¾ã™ãŒã€**TypedDictã‚’å¼·ãæ¨å¥¨**ã—ã¾ã™ã€‚

```python
# âŒ å‹•ããŒéæ¨å¥¨
class State(dict):
    pass

# âœ… æ¨å¥¨
from typing import TypedDict
class State(TypedDict):
    input: str
    output: str
```

**ç†ç”±:**
- å‹å®‰å…¨æ€§
- IDEã®è£œå®Œ
- ã‚¿ã‚¤ãƒé˜²æ­¢

#### Q5: STARTã¨ENDã¯çœç•¥ã§ãã‚‹?

**A:** æŠ€è¡“çš„ã«ã¯å¯èƒ½ã§ã™ãŒã€**99%ã®ã‚±ãƒ¼ã‚¹ã§ä½¿ã„ã¾ã™**ã€‚

```python
# âŒ å‹•ããŒå®Ÿè·µçš„ã§ãªã„
workflow.add_edge("node1", "node2")

# âœ… æ¨å¥¨
workflow.add_edge(START, "node1")
workflow.add_edge("node2", END)
```

---

### ãƒãƒ¼ãƒ‰é–¢æ•°

#### Q6: ãƒãƒ¼ãƒ‰é–¢æ•°ã®å¼•æ•°åã¯`state`å›ºå®š?

**A:** ã„ã„ãˆã€**ä»»æ„ã®åå‰ã§OK**ã§ã™ãŒã€`state`ãŒæ…£ä¾‹ã§ã™ã€‚

```python
# âœ… ã™ã¹ã¦æœ‰åŠ¹
def node1(state: State) -> dict:
    return {"output": "OK"}

def node2(data: State) -> dict:
    return {"output": "OK"}

def node3(s: State) -> dict:
    return {"output": "OK"}
```

#### Q7: æˆ»ã‚Šå€¤ã¯å¿…ãšdict?

**A:** ã¯ã„ã€**å¿…ãšdict**ã§ã™ã€‚

```python
# âœ… æ­£ã—ã„
def correct_node(state: State) -> dict:
    return {"output": "çµæœ"}

# âŒ ã‚¨ãƒ©ãƒ¼
def wrong_node(state: State) -> str:
    return "çµæœ"
```

#### Q8: ç©ºã®dictã‚’è¿”ã—ã¦ã‚‚ã„ã„?

**A:** ã¯ã„ã€**OK**ã§ã™ï¼ˆä½•ã‚‚æ›´æ–°ã—ãªã„å ´åˆï¼‰ã€‚

```python
def no_update_node(state: State) -> dict:
    # ä½•ã‹å‡¦ç†ã¯ã™ã‚‹ãŒã€Stateã¯æ›´æ–°ã—ãªã„
    print("å‡¦ç†ä¸­...")
    return {}  # âœ… OK
```

#### Q9: ã™ã¹ã¦ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿”ã™å¿…è¦ã¯?

**A:** ã„ã„ãˆã€**æ›´æ–°ã™ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã ã‘**è¿”ã›ã°OKã§ã™ã€‚

```python
class State(TypedDict):
    a: str
    b: str
    c: str

def node(state: State) -> dict:
    # aã ã‘æ›´æ–°
    return {"a": "æ–°ã—ã„å€¤"}  # âœ… bã¨cã¯ãã®ã¾ã¾
```

#### Q10: ãƒãƒ¼ãƒ‰é–¢æ•°å†…ã§ã‚¨ãƒ©ãƒ¼ãŒèµ·ããŸã‚‰?

**A:** **ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å…¨ä½“ãŒåœæ­¢**ã—ã¾ã™ï¼ˆç¬¬5ç« ã§å¯¾ç­–ã‚’å­¦ã³ã¾ã™ï¼‰ã€‚

```python
def risky_node(state: State) -> dict:
    result = 10 / 0  # ZeroDivisionError
    return {"output": result}

# â†’ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼åœæ­¢ã€ã‚¨ãƒ©ãƒ¼ãŒä¼æ’­
```

---

### ã‚°ãƒ©ãƒ•æ§‹é€ 

#### Q11: ãƒãƒ¼ãƒ‰ã®è¿½åŠ é †åºã¯é‡è¦?

**A:** ã„ã„ãˆã€**é †åºã¯ç„¡é–¢ä¿‚**ã§ã™ã€‚

```python
# ã©ã¡ã‚‰ã‚‚åŒã˜çµæœ
# ãƒ‘ã‚¿ãƒ¼ãƒ³1
workflow.add_node("a", func_a)
workflow.add_node("b", func_b)

# ãƒ‘ã‚¿ãƒ¼ãƒ³2
workflow.add_node("b", func_b)
workflow.add_node("a", func_a)
```

#### Q12: ã‚¨ãƒƒã‚¸ã®è¿½åŠ é †åºã¯?

**A:** ã„ã„ãˆã€**é †åºã¯ç„¡é–¢ä¿‚**ã§ã™ã€‚

```python
# ã©ã¡ã‚‰ã‚‚åŒã˜
# ãƒ‘ã‚¿ãƒ¼ãƒ³1
workflow.add_edge(START, "a")
workflow.add_edge("a", END)

# ãƒ‘ã‚¿ãƒ¼ãƒ³2
workflow.add_edge("a", END)
workflow.add_edge(START, "a")
```

#### Q13: åŒã˜ãƒãƒ¼ãƒ‰ã‚’è¤‡æ•°å›è¿½åŠ ã§ãã‚‹?

**A:** ã„ã„ãˆã€**ã‚¨ãƒ©ãƒ¼**ã«ãªã‚Šã¾ã™ã€‚

```python
workflow.add_node("process", func)
workflow.add_node("process", func)  # âŒ ã‚¨ãƒ©ãƒ¼
```

#### Q14: ãƒãƒ¼ãƒ‰åã¯æ—¥æœ¬èªOK?

**A:** ã¯ã„ã€**å®Œå…¨ã«OK**ã§ã™ã€‚

```python
workflow.add_node("å‡¦ç†1", func1)
workflow.add_node("æ¤œç´¢", func2)
workflow.add_node("è¦ç´„ç”Ÿæˆ", func3)
# âœ… ã™ã¹ã¦æœ‰åŠ¹
```

#### Q15: ãƒãƒ¼ãƒ‰åã®åˆ¶é™ã¯?

**A:** `__start__`ã¨`__end__`ã¯**äºˆç´„èª**ãªã®ã§ä½¿ãˆã¾ã›ã‚“ã€‚

```python
workflow.add_node("__start__", func)  # âŒ ã‚¨ãƒ©ãƒ¼
workflow.add_node("__end__", func)    # âŒ ã‚¨ãƒ©ãƒ¼
workflow.add_node("my_node", func)    # âœ… OK
```

---

### å®Ÿè¡Œã¨ãƒ‡ãƒãƒƒã‚°

#### Q16: invoke()ã®å¼•æ•°ã¯?

**A:** **Stateã®åˆæœŸå€¤**ã‚’è¾æ›¸ã§æ¸¡ã—ã¾ã™ã€‚

```python
class State(TypedDict):
    input: str
    output: str

# ã™ã¹ã¦ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æ¸¡ã™å¿…è¦ã¯ãªã„
result = app.invoke({"input": "è³ªå•"})  # âœ… OK
result = app.invoke({"input": "è³ªå•", "output": ""})  # âœ… ã“ã‚Œã‚‚OK
```

#### Q17: å®Ÿè¡Œçµæœã¯ä½•ãŒè¿”ã‚‹?

**A:** **æœ€çµ‚çš„ãªStateå…¨ä½“**ãŒè¿”ã‚Šã¾ã™ã€‚

```python
result = app.invoke({"input": "ã“ã‚“ã«ã¡ã¯"})
print(type(result))  # <class 'dict'>
print(result)  # {"input": "ã“ã‚“ã«ã¡ã¯", "output": "ã“ã‚“ã«ã¡ã¯ï¼..."}
```

#### Q18: é€”ä¸­çµŒéã¯å–å¾—ã§ãã‚‹?

**A:** `stream()`ãƒ¡ã‚½ãƒƒãƒ‰ã§å¯èƒ½ã§ã™ã€‚

```python
for chunk in app.stream({"input": "è³ªå•"}):
    print(chunk)  # å„ãƒãƒ¼ãƒ‰ã®å®Ÿè¡Œçµæœ
```

#### Q19: å®Ÿè¡Œæ™‚é–“ã‚’è¨ˆæ¸¬ã—ãŸã„

**A:** Pythonã®`time`ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½¿ã„ã¾ã™ã€‚

```python
import time

start_time = time.time()
result = app.invoke({"input": "è³ªå•"})
end_time = time.time()

print(f"å®Ÿè¡Œæ™‚é–“: {end_time - start_time:.2f}ç§’")
```

#### Q20: ãƒ‡ãƒãƒƒã‚°æ–¹æ³•ã¯?

**A:** ãƒãƒ¼ãƒ‰é–¢æ•°å†…ã§`print()`ã‚’ä½¿ã„ã¾ã™ã€‚

```python
def debug_node(state: State) -> dict:
    print(f"ç¾åœ¨ã®state: {state}")  # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
    result = some_processing(state)
    print(f"å‡¦ç†çµæœ: {result}")
    return {"output": result}
```

---

### APIã¨LLM

#### Q21: Geminiä»¥å¤–ã®LLMã¯ä½¿ãˆã‚‹?

**A:** ã¯ã„ã€**LangChainå¯¾å¿œLLMãªã‚‰ã™ã¹ã¦ä½¿ãˆã¾ã™**ã€‚

```python
# OpenAI
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(
    api_key=os.environ["OPENAI_API_KEY"],
    model="gpt-4"
)

# Anthropic Claude
from langchain_anthropic import ChatAnthropic
llm = ChatAnthropic(
    api_key=os.environ["ANTHROPIC_API_KEY"],
    model="claude-3-opus"
)

# ãƒ­ãƒ¼ã‚«ãƒ«LLM (Ollama)
from langchain_community.llms import Ollama
llm = Ollama(model="llama2")
```

#### Q22: APIã‚­ãƒ¼ã‚’ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã—ãŸããªã„

**A:** ç’°å¢ƒå¤‰æ•°ã¾ãŸã¯`.env`ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ã„ã¾ã™ï¼ˆ0-3ç« å‚ç…§ï¼‰ã€‚

```python
# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
from dotenv import load_dotenv
load_dotenv()

llm = ChatGoogleGenerativeAI(
    google_api_key=os.environ["GEMINI_API_KEY"],
    model="gemini-2.0-flash-exp"
)
```

#### Q23: APIã‚¨ãƒ©ãƒ¼ãŒèµ·ããŸã‚‰ã©ã†ãªã‚‹?

**A:** **ä¾‹å¤–ãŒç™ºç”Ÿã—ã€ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼åœæ­¢**ã—ã¾ã™ã€‚

```python
# ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä¾‹
def safe_llm_call(state: State) -> dict:
    try:
        response = llm.invoke([HumanMessage(content=state["input"])])
        return {"output": response.content}
    except Exception as e:
        return {"output": f"ã‚¨ãƒ©ãƒ¼: {str(e)}"}
```

#### Q24: LLMå‘¼ã³å‡ºã—ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã§ãã‚‹?

**A:** LangChainã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ã‚’ä½¿ã„ã¾ã™ã€‚

```python
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache

set_llm_cache(InMemoryCache())

# åŒã˜è³ªå•ã¯2å›ç›®ä»¥é™ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰è¿”ã‚‹
```

#### Q25: è¤‡æ•°ã®LLMã‚’åŒæ™‚ã«ä½¿ãˆã‚‹?

**A:** ã¯ã„ã€**ãƒãƒ¼ãƒ‰ã”ã¨ã«ç•°ãªã‚‹LLMã‚’ä½¿ãˆã¾ã™**ã€‚

```python
llm_fast = ChatGoogleGenerativeAI(
    google_api_key=os.environ["GEMINI_API_KEY"],
    model="gemini-1.5-flash"
)

llm_smart = ChatGoogleGenerativeAI(
    google_api_key=os.environ["GEMINI_API_KEY"],
    model="gemini-2.0-flash-exp"
)

def fast_node(state: State) -> dict:
    response = llm_fast.invoke([HumanMessage(content=state["input"])])
    return {"output": response.content}

def smart_node(state: State) -> dict:
    response = llm_smart.invoke([HumanMessage(content=state["input"])])
    return {"output": response.content}
```

---

### ã‚°ãƒ©ãƒ•å¯è¦–åŒ–

#### Q26: å¯è¦–åŒ–ã¯å¿…é ˆ?

**A:** ã„ã„ãˆã€**ã‚ªãƒ—ã‚·ãƒ§ãƒ³**ã§ã™ãŒã€è¤‡é›‘ãªã‚°ãƒ©ãƒ•ã§ã¯å¼·ãæ¨å¥¨ã—ã¾ã™ã€‚

#### Q27: PNGãŒç”Ÿæˆã§ããªã„å ´åˆã¯?

**A:** ASCIIç‰ˆã§è¡¨ç¤ºã§ãã¾ã™ã€‚

```python
try:
    png_data = app.get_graph().draw_mermaid_png()
except Exception:
    print(app.get_graph().draw_ascii())
```

#### Q28: ã‚°ãƒ©ãƒ•ã‚’Jupyter Notebookã§è¡¨ç¤ºã—ãŸã„

**A:** `IPython.display`ã‚’ä½¿ã„ã¾ã™ã€‚

```python
from IPython.display import Image, display

png_data = app.get_graph().draw_mermaid_png()
display(Image(png_data))
```

#### Q29: ã‚°ãƒ©ãƒ•ã®è‰²ã‚’å¤‰ãˆã‚‰ã‚Œã‚‹?

**A:** Mermaidã®è¨­å®šã§å¯èƒ½ã§ã™ãŒã€é«˜åº¦ãªãƒˆãƒ”ãƒƒã‚¯ã§ã™ã€‚

#### Q30: ã‚°ãƒ©ãƒ•ã‚’HTMLå‡ºåŠ›ã§ãã‚‹?

**A:** ã¯ã„ã€å¯èƒ½ã§ã™ã€‚

```python
# Mermaidå½¢å¼ã§å–å¾—
mermaid_code = app.get_graph().draw_mermaid()
print(mermaid_code)

# HTMLã«åŸ‹ã‚è¾¼ã¿å¯èƒ½
html = f"""
<script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
<div class="mermaid">
{mermaid_code}
</div>
"""
```

---

### å®Œå…¨ãªå®Ÿè£…ä¾‹ï¼ˆç¬¬2ç« ï¼‰

```python
"""
ç¬¬2ç« å®Œå…¨å®Ÿè£… - ã‚³ãƒ”ãƒ¼&ãƒšãƒ¼ã‚¹ãƒˆã§å‹•ãã¾ã™
"""

import os
from typing import TypedDict
from langgraph.graph import StateGraph, START, END
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage

# ============================================
# APIã‚­ãƒ¼è¨­å®š
# ============================================

os.environ["GEMINI_API_KEY"] = "your-actual-api-key"  # â† å®Ÿéš›ã®ã‚­ãƒ¼ã«å¤‰æ›´

if os.environ.get("GEMINI_API_KEY") == "your-actual-api-key":
    raise ValueError("âŒ APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„")

# ============================================
# Stateå®šç¾©
# ============================================

class State(TypedDict):
    input: str
    output: str

# ============================================
# LLMåˆæœŸåŒ–
# ============================================

llm = ChatGoogleGenerativeAI(
    google_api_key=os.environ["GEMINI_API_KEY"],
    model="gemini-2.0-flash-exp",
    temperature=0.7,
    max_tokens=1024
)

# ============================================
# ãƒãƒ¼ãƒ‰é–¢æ•°
# ============================================

def call_gemini(state: State) -> dict:
    """Gemini APIã‚’å‘¼ã³å‡ºã™"""
    print(f"ğŸ“ å…¥åŠ›: {state['input']}")
    
    response = llm.invoke([HumanMessage(content=state["input"])])
    
    print(f"âœ… å¿œç­”å—ä¿¡å®Œäº†")
    return {"output": response.content}

# ============================================
# ã‚°ãƒ©ãƒ•æ§‹ç¯‰
# ============================================

workflow = StateGraph(State)
workflow.add_node("gemini", call_gemini)
workflow.add_edge(START, "gemini")
workflow.add_edge("gemini", END)

app = workflow.compile()

print("âœ… ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æ§‹ç¯‰å®Œäº†")

# ============================================
# ã‚°ãƒ©ãƒ•å¯è¦–åŒ–
# ============================================

def visualize_graph(app, filename="workflow.png"):
    """ã‚°ãƒ©ãƒ•ã‚’PNGä¿å­˜"""
    try:
        png_data = app.get_graph().draw_mermaid_png()
        with open(filename, "wb") as f:
            f.write(png_data)
        print(f"âœ… ã‚°ãƒ©ãƒ•ã‚’ '{filename}' ã«ä¿å­˜")
    except Exception as e:
        print(f"âš ï¸ PNGä¿å­˜å¤±æ•—: {e}")
        print(app.get_graph().draw_ascii())

visualize_graph(app, "ch2_workflow.png")

# ============================================
# å®Ÿè¡Œé–¢æ•°
# ============================================

def run_workflow(app, user_input: str):
    """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œ"""
    print("\n" + "=" * 60)
    print("ğŸš€ å®Ÿè¡Œé–‹å§‹")
    print("=" * 60)
    
    result = app.invoke({"input": user_input, "output": ""})
    
    print("=" * 60)
    print("âœ… å®Œäº†")
    print("=" * 60)
    print(f"\nğŸ“¤ çµæœ:\n{result['output']}\n")
    
    return result

# ============================================
# å®Ÿè¡Œ
# ============================================

if __name__ == "__main__":
    run_workflow(app, "LangGraphã®ä¸»ãªç‰¹å¾´ã‚’3ã¤æ•™ãˆã¦ãã ã•ã„")
```

---

# ç¬¬3ç« : æ¡ä»¶åˆ†å²ã¨ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

## 3-1. æ¡ä»¶åˆ†å²ã®åŸºæœ¬

### ãªãœæ¡ä»¶åˆ†å²ãŒå¿…è¦?

**å˜ç´”ãªç›´ç·šãƒ•ãƒ­ãƒ¼:**
```
å…¥åŠ› â†’ å‡¦ç† â†’ å‡ºåŠ›
```

**æ¡ä»¶åˆ†å²ãƒ•ãƒ­ãƒ¼:**
```
å…¥åŠ› â†’ åˆ¤å®š â†’ 
    â”œâ”€ ãƒ«ãƒ¼ãƒˆA â†’ å‡¦ç†A
    â”œâ”€ ãƒ«ãƒ¼ãƒˆB â†’ å‡¦ç†B
    â””â”€ ãƒ«ãƒ¼ãƒˆC â†’ å‡¦ç†C
```

### æ¡ä»¶åˆ†å²ã®å®Ÿè£…æ–¹æ³•

LangGraphã§ã¯`add_conditional_edges()`ã‚’ä½¿ã„ã¾ã™ã€‚

```python
workflow.add_conditional_edges(
    "åˆ¤å®šãƒãƒ¼ãƒ‰",           # ã©ã®ãƒãƒ¼ãƒ‰ã®å¾Œã«åˆ†å²?
    routing_function,      # ã©ã†åˆ†å²ã™ã‚‹ã‹åˆ¤æ–­?
    {                      # å„ãƒ«ãƒ¼ãƒˆã®æ¥ç¶šå…ˆ
        "route_a": "node_a",
        "route_b": "node_b",
        "route_c": END
    }
)
```

### æœ€å°ã®æ¡ä»¶åˆ†å²ä¾‹

```python
import os
from typing import TypedDict, Literal, NotRequired
from langgraph.graph import StateGraph, START, END
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage

os.environ["GEMINI_API_KEY"] = "your-api-key"

# Stateå®šç¾©
class State(TypedDict):
    input: str                     # å¿…é ˆï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¸¡ã™ï¼‰
    category: NotRequired[str]     # ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ï¼ˆãƒãƒ¼ãƒ‰ãŒè¿½åŠ ï¼‰
    # output: str ã¯å®šç¾©ä¸è¦ï¼ˆæœ€çµ‚å‡ºåŠ›ã®ã¿ã§ä½¿ç”¨ï¼‰

# LLMåˆæœŸåŒ–
llm = ChatGoogleGenerativeAI(
    google_api_key=os.environ["GEMINI_API_KEY"],
    model="gemini-2.0-flash-exp",
    temperature=0.3
)

# ãƒãƒ¼ãƒ‰1: ã‚«ãƒ†ã‚´ãƒªåˆ†é¡
def classify_input(state: State) -> dict:
    """å…¥åŠ›ã‚’ã‚«ãƒ†ã‚´ãƒªåˆ†é¡"""
    user_input = state["input"]
    
    prompt = f"""
    ä»¥ä¸‹ã®è³ªå•ã‚’åˆ†é¡ã—ã¦ãã ã•ã„ã€‚
    - æŒ¨æ‹¶ãªã‚‰ã€Œgreetingã€
    - è³ªå•ãªã‚‰ã€Œquestionã€
    - ãã®ä»–ãªã‚‰ã€Œotherã€
    
    è³ªå•: {user_input}
    
    ã‚«ãƒ†ã‚´ãƒªåã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
    """
    
    response = llm.invoke([HumanMessage(content=prompt)])
    category = response.content.strip().lower()
    
    return {"category": category}

# ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é–¢æ•°
def route_by_category(state: State) -> Literal["greeting", "question", "other"]:
    """ã‚«ãƒ†ã‚´ãƒªã«å¿œã˜ã¦ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°"""
    return state["category"]

# ãƒãƒ¼ãƒ‰2: æŒ¨æ‹¶å‡¦ç†
def handle_greeting(state: State) -> dict:
    return {"output": "ã“ã‚“ã«ã¡ã¯ï¼ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"}

# ãƒãƒ¼ãƒ‰3: è³ªå•å‡¦ç†
def handle_question(state: State) -> dict:
    response = llm.invoke([HumanMessage(content=state["input"])])
    return {"output": response.content}

# ãƒãƒ¼ãƒ‰4: ãã®ä»–å‡¦ç†
def handle_other(state: State) -> dict:
    return {"output": "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€ç†è§£ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"}

# ã‚°ãƒ©ãƒ•æ§‹ç¯‰
workflow = StateGraph(State)

# ãƒãƒ¼ãƒ‰è¿½åŠ 
workflow.add_node("classify", classify_input)
workflow.add_node("greeting", handle_greeting)
workflow.add_node("question", handle_question)
workflow.add_node("other", handle_other)

# ã‚¨ãƒƒã‚¸è¿½åŠ 
workflow.add_edge(START, "classify")

# æ¡ä»¶åˆ†å²
workflow.add_conditional_edges(
    "classify",
    route_by_category,
    {
        "greeting": "greeting",
        "question": "question",
        "other": "other"
    }
)

# å„ãƒ«ãƒ¼ãƒˆã‹ã‚‰çµ‚äº†
workflow.add_edge("greeting", END)
workflow.add_edge("question", END)
workflow.add_edge("other", END)

app = workflow.compile()

# å®Ÿè¡Œï¼ˆcategoryã¯ä¸è¦ï¼ï¼‰
test_inputs = [
    "ã“ã‚“ã«ã¡ã¯",
    "LangGraphã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
    "ã‚ã„ã†ãˆãŠ"
]

for test_input in test_inputs:
    print(f"\n{'='*60}")
    print(f"å…¥åŠ›: {test_input}")
    result = app.invoke({"input": test_input})  # â† categoryã¯æ¸¡ã•ãªã„
    print(f"ã‚«ãƒ†ã‚´ãƒª: {result['category']}")
    print(f"å‡ºåŠ›: {result['output']}")
```

### ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é–¢æ•°ã®é‡è¦ãƒã‚¤ãƒ³ãƒˆ

**ãƒ«ãƒ¼ãƒ«1: æˆ»ã‚Šå€¤ã¯æ–‡å­—åˆ—**

```python
# âœ… æ­£ã—ã„
def route_func(state: State) -> str:
    return "route_a"

# âœ… å‹ãƒ’ãƒ³ãƒˆä»˜ãï¼ˆæ¨å¥¨ï¼‰
def route_func(state: State) -> Literal["route_a", "route_b"]:
    if state["score"] > 0.8:
        return "route_a"
    return "route_b"
```

**ãƒ«ãƒ¼ãƒ«2: æˆ»ã‚Šå€¤ã¯ãƒãƒƒãƒ”ãƒ³ã‚°ã®ã‚­ãƒ¼ã¨ä¸€è‡´**

```python
# ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é–¢æ•°
def route_func(state: State) -> Literal["high", "low"]:
    return "high" if state["score"] > 0.5 else "low"

# ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆã‚­ãƒ¼ãŒä¸€è‡´ï¼‰
workflow.add_conditional_edges(
    "classify",
    route_func,
    {
        "high": "premium_process",  # â† "high"ã¨ä¸€è‡´
        "low": "basic_process"      # â† "low"ã¨ä¸€è‡´
    }
)
```

### ğŸ”‘ Stateã®è¨­è¨ˆãƒã‚¤ãƒ³ãƒˆï¼ˆé‡è¦ï¼‰

```python
from typing import TypedDict, NotRequired

class State(TypedDict):
    # âœ… å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼šãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¸¡ã™ã‚‚ã®
    input: str
    
    # âœ… ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ï¼šãƒãƒ¼ãƒ‰ãŒè¿½åŠ ã™ã‚‹ã‚‚ã®ï¼ˆä»–ã®ãƒãƒ¼ãƒ‰ã§ä½¿ã†ï¼‰
    category: NotRequired[str]
    sentiment: NotRequired[str]
    
    # âŒ æœ€çµ‚å‡ºåŠ›ã®ã¿ã§ä½¿ã†ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯å®šç¾©ä¸è¦
    # output: str  â† ã“ã‚Œã¯ä¸è¦
```

**ç†ç”±:**
- `category`ã‚„`sentiment`ã¯**è¤‡æ•°ã®ãƒãƒ¼ãƒ‰**ã§å‚ç…§ã•ã‚Œã‚‹ â†’ å®šç¾©ã™ã¹ã
- `output`ã¯**æœ€çµ‚çµæœã¨ã—ã¦1å›ã ã‘**ä½¿ã‚ã‚Œã‚‹ â†’ å®šç¾©ä¸è¦

---

## 3-2. è¤‡æ•°ãƒ«ãƒ¼ãƒˆã®å®Ÿè£…

### å®Ÿç”¨çš„ãªä¾‹: è³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ 

```python
import os
from typing import TypedDict, Literal, NotRequired
from langgraph.graph import StateGraph, START, END
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, SystemMessage

os.environ["GEMINI_API_KEY"] = "your-api-key"

# Stateå®šç¾©
class State(TypedDict):
    question: str                  # å¿…é ˆï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¸¡ã™ï¼‰
    intent: NotRequired[str]       # ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ï¼ˆä»–ã®ãƒãƒ¼ãƒ‰ã§å‚ç…§ï¼‰
    # answer: str ã¯å®šç¾©ä¸è¦ï¼ˆæœ€çµ‚å‡ºåŠ›ã®ã¿ï¼‰

# LLMåˆæœŸåŒ–
llm = ChatGoogleGenerativeAI(
    google_api_key=os.environ["GEMINI_API_KEY"],
    model="gemini-2.0-flash-exp",
    temperature=0.3
)

# ãƒãƒ¼ãƒ‰1: Intentåˆ†é¡
def classify_intent(state: State) -> dict:
    """è³ªå•ã®æ„å›³ã‚’åˆ†é¡"""
    question = state["question"]
    
    messages = [
        SystemMessage(content="""
        è³ªå•ã‚’ä»¥ä¸‹ã®ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡ã—ã¦ãã ã•ã„:
        - factual: äº‹å®Ÿç¢ºèªã®è³ªå•
        - opinion: æ„è¦‹ã‚’æ±‚ã‚ã‚‹è³ªå•
        - howto: æ‰‹é †ã‚’å°‹ã­ã‚‹è³ªå•
        - other: ãã®ä»–
        
        ã‚«ãƒ†ã‚´ãƒªåã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
        """),
        HumanMessage(content=question)
    ]
    
    response = llm.invoke(messages)
    intent = response.content.strip().lower()
    
    return {"intent": intent}

# ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é–¢æ•°
def route_by_intent(state: State) -> Literal["factual", "opinion", "howto", "other"]:
    """æ„å›³ã«åŸºã¥ã„ã¦ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°"""
    return state["intent"]

# ãƒãƒ¼ãƒ‰2a: äº‹å®Ÿç¢ºèªå‡¦ç†
def handle_factual(state: State) -> dict:
    """äº‹å®Ÿç¢ºèªã®è³ªå•ã«å›ç­”"""
    messages = [
        SystemMessage(content="äº‹å®Ÿã«åŸºã¥ã„ã¦æ­£ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚"),
        HumanMessage(content=state["question"])
    ]
    
    response = llm.invoke(messages)
    return {"answer": response.content}

# ãƒãƒ¼ãƒ‰2b: æ„è¦‹å‡¦ç†
def handle_opinion(state: State) -> dict:
    """æ„è¦‹ã‚’æ±‚ã‚ã‚‹è³ªå•ã«å›ç­”"""
    messages = [
        SystemMessage(content="ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸè¦–ç‚¹ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"),
        HumanMessage(content=state["question"])
    ]
    
    response = llm.invoke(messages)
    return {"answer": response.content}

# ãƒãƒ¼ãƒ‰2c: æ‰‹é †èª¬æ˜å‡¦ç†
def handle_howto(state: State) -> dict:
    """æ‰‹é †ã‚’æ®µéšçš„ã«èª¬æ˜"""
    messages = [
        SystemMessage(content="ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"),
        HumanMessage(content=state["question"])
    ]
    
    response = llm.invoke(messages)
    return {"answer": response.content}

# ãƒãƒ¼ãƒ‰2d: ãã®ä»–å‡¦ç†
def handle_other(state: State) -> dict:
    """ãã®ä»–ã®è³ªå•ã«å¯¾å¿œ"""
    return {"answer": "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ã‚‚ã†å°‘ã—å…·ä½“çš„ã«ãŠèã‹ã›ã„ãŸã ã‘ã¾ã™ã‹ï¼Ÿ"}

# ã‚°ãƒ©ãƒ•æ§‹ç¯‰
workflow = StateGraph(State)

# ãƒãƒ¼ãƒ‰è¿½åŠ 
workflow.add_node("classify", classify_intent)
workflow.add_node("factual", handle_factual)
workflow.add_node("opinion", handle_opinion)
workflow.add_node("howto", handle_howto)
workflow.add_node("other", handle_other)

# ã‚¨ãƒƒã‚¸è¿½åŠ 
workflow.add_edge(START, "classify")

# æ¡ä»¶åˆ†å²
workflow.add_conditional_edges(
    "classify",
    route_by_intent,
    {
        "factual": "factual",
        "opinion": "opinion",
        "howto": "howto",
        "other": "other"
    }
)

# å„ãƒ«ãƒ¼ãƒˆã‹ã‚‰çµ‚äº†
workflow.add_edge("factual", END)
workflow.add_edge("opinion", END)
workflow.add_edge("howto", END)
workflow.add_edge("other", END)

app = workflow.compile()

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆintentã¯æ¸¡ã•ãªã„ï¼‰
test_questions = [
    "æ±äº¬ã®äººå£ã¯ä½•äººã§ã™ã‹ï¼Ÿ",
    "AIã®æœªæ¥ã«ã¤ã„ã¦ã©ã†æ€ã„ã¾ã™ã‹ï¼Ÿ",
    "Pythonã§ç’°å¢ƒæ§‹ç¯‰ã™ã‚‹æ–¹æ³•ã‚’æ•™ãˆã¦ãã ã•ã„",
    "ã‚ã„ã†ãˆãŠ"
]

for question in test_questions:
    print(f"\n{'='*60}")
    print(f"è³ªå•: {question}")
    result = app.invoke({"question": question})  # â† intentã¯ä¸è¦
    print(f"æ„å›³: {result['intent']}")
    print(f"å›ç­”: {result['answer']}")
```

---

## 3-3. å‹•çš„ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ã®ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã«åŸºã¥ã„ã¦å‡¦ç†ã‚’åˆ†å²ã™ã‚‹å®Ÿç”¨çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã™ã€‚

```python
import os
from typing import TypedDict, Literal, NotRequired
from langgraph.graph import StateGraph, START, END
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, SystemMessage
import json

os.environ["GEMINI_API_KEY"] = "your-api-key"

# Stateå®šç¾©
class State(TypedDict):
    query: str                      # å¿…é ˆ
    confidence: NotRequired[float]  # ä»–ã®ãƒãƒ¼ãƒ‰ã§å‚ç…§
    # answer: str ã¯å®šç¾©ä¸è¦

# LLMåˆæœŸåŒ–
llm = ChatGoogleGenerativeAI(
    google_api_key=os.environ["GEMINI_API_KEY"],
    model="gemini-2.0-flash-exp",
    temperature=0.3
)

# ãƒãƒ¼ãƒ‰1: ä¿¡é ¼åº¦è©•ä¾¡
def evaluate_confidence(state: State) -> dict:
    """è³ªå•ã®é›£æ˜“åº¦ã‚’è©•ä¾¡ã—ã¦ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‚’è¿”ã™"""
    query = state["query"]
    
    messages = [
        SystemMessage(content="""
        ä»¥ä¸‹ã®è³ªå•ã®é›£æ˜“åº¦ã‚’0.0ã€œ1.0ã§è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚
        - 0.0ã€œ0.3: ç°¡å˜ãªè³ªå•ï¼ˆæŒ¨æ‹¶ã€åŸºæœ¬çš„ãªäº‹å®Ÿï¼‰
        - 0.4ã€œ0.7: ä¸­ç¨‹åº¦ã®è³ªå•ï¼ˆèª¬æ˜ãŒå¿…è¦ï¼‰
        - 0.8ã€œ1.0: é›£ã—ã„è³ªå•ï¼ˆå°‚é–€çŸ¥è­˜ã€è¤‡é›‘ãªæ¨è«–ï¼‰
        
        æ•°å€¤ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚ä¾‹: 0.7
        """),
        HumanMessage(content=query)
    ]
    
    response = llm.invoke(messages)
    
    try:
        confidence = float(response.content.strip())
    except ValueError:
        confidence = 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    return {"confidence": confidence}

# ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é–¢æ•°
def route_by_confidence(state: State) -> Literal["simple", "moderate", "complex"]:
    """ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã«åŸºã¥ã„ã¦ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°"""
    score = state["confidence"]
    
    if score < 0.4:
        return "simple"
    elif score < 0.8:
        return "moderate"
    else:
        return "complex"

# ãƒãƒ¼ãƒ‰2a: ã‚·ãƒ³ãƒ—ãƒ«å‡¦ç†
def handle_simple(state: State) -> dict:
    """ç°¡å˜ãªè³ªå•ã«ç´ æ—©ãå›ç­”"""
    messages = [
        SystemMessage(content="ç°¡æ½”ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚"),
        HumanMessage(content=state["query"])
    ]
    
    response = llm.invoke(messages)
    return {"answer": response.content}

# ãƒãƒ¼ãƒ‰2b: æ¨™æº–å‡¦ç†
def handle_moderate(state: State) -> dict:
    """ä¸­ç¨‹åº¦ã®è³ªå•ã«è©³ã—ãå›ç­”"""
    messages = [
        SystemMessage(content="è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚"),
        HumanMessage(content=state["query"])
    ]
    
    response = llm.invoke(messages)
    return {"answer": response.content}

# ãƒãƒ¼ãƒ‰2c: è¤‡é›‘å‡¦ç†
def handle_complex(state: State) -> dict:
    """é›£ã—ã„è³ªå•ã«å°‚é–€çš„ã«å›ç­”"""
    messages = [
        SystemMessage(content="å°‚é–€çš„ã‹ã¤åŒ…æ‹¬çš„ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"),
        HumanMessage(content=state["query"])
    ]
    
    response = llm.invoke(messages)
    return {"answer": response.content}

# ã‚°ãƒ©ãƒ•æ§‹ç¯‰
workflow = StateGraph(State)

workflow.add_node("evaluate", evaluate_confidence)
workflow.add_node("simple", handle_simple)
workflow.add_node("moderate", handle_moderate)
workflow.add_node("complex", handle_complex)

workflow.add_edge(START, "evaluate")

workflow.add_conditional_edges(
    "evaluate",
    route_by_confidence,
    {
        "simple": "simple",
        "moderate": "moderate",
        "complex": "complex"
    }
)

workflow.add_edge("simple", END)
workflow.add_edge("moderate", END)
workflow.add_edge("complex", END)

app = workflow.compile()

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
test_queries = [
    "ã“ã‚“ã«ã¡ã¯",
    "LangGraphã®åŸºæœ¬çš„ãªä½¿ã„æ–¹ã‚’æ•™ãˆã¦ãã ã•ã„",
    "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã¨æ©Ÿæ¢°å­¦ç¿’ã®ç†è«–çš„ãªé–¢ä¿‚æ€§ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„"
]

for query in test_queries:
    print(f"\n{'='*60}")
    print(f"è³ªå•: {query}")
    result = app.invoke({"query": query})
    print(f"ä¿¡é ¼åº¦: {result['confidence']:.2f}")
    print(f"ãƒ«ãƒ¼ãƒˆ: {route_by_confidence(result)}")
    print(f"å›ç­”: {result['answer'][:100]}...")
```

### è¤‡æ•°æ¡ä»¶ã«ã‚ˆã‚‹è¤‡é›‘ãªãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

```python
import os
from typing import TypedDict, Literal, NotRequired
from langgraph.graph import StateGraph, START, END
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage

os.environ["GEMINI_API_KEY"] = "your-api-key"

# Stateå®šç¾©
class State(TypedDict):
    query: str                    # å¿…é ˆ
    language: NotRequired[str]    # ä»–ã®ãƒãƒ¼ãƒ‰ã§å‚ç…§
    priority: NotRequired[str]    # ä»–ã®ãƒãƒ¼ãƒ‰ã§å‚ç…§
    # response: str ã¯å®šç¾©ä¸è¦

llm = ChatGoogleGenerativeAI(
    google_api_key=os.environ["GEMINI_API_KEY"],
    model="gemini-2.0-flash-exp",
    temperature=0.3
)

# ãƒãƒ¼ãƒ‰1: åˆ†æ
def analyze_query(state: State) -> dict:
    """ã‚¯ã‚¨ãƒªã‚’åˆ†æ"""
    query = state["query"]
    
    # è¨€èªæ¤œå‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
    if any(c >= '\u4e00' and c <= '\u9fff' for c in query):
        language = "ja"
    elif any(c >= '\uac00' and c <= '\ud7a3' for c in query):
        language = "ko"
    else:
        language = "en"
    
    # å„ªå…ˆåº¦åˆ¤å®šï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰
    urgent_keywords = ["urgent", "emergency", "è‡³æ€¥", "ç·Šæ€¥"]
    priority = "high" if any(kw in query.lower() for kw in urgent_keywords) else "normal"
    
    return {
        "language": language,
        "priority": priority
    }

# ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é–¢æ•°
def route_by_analysis(state: State) -> Literal["urgent_ja", "urgent_en", "normal_ja", "normal_en"]:
    """è¨€èªã¨å„ªå…ˆåº¦ã§è¤‡åˆãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°"""
    lang = state["language"]
    priority = state["priority"]
    
    if priority == "high" and lang == "ja":
        return "urgent_ja"
    elif priority == "high" and lang == "en":
        return "urgent_en"
    elif lang == "ja":
        return "normal_ja"
    else:
        return "normal_en"

# ãƒãƒ¼ãƒ‰2a: ç·Šæ€¥ãƒ»æ—¥æœ¬èª
def handle_urgent_ja(state: State) -> dict:
    response = llm.invoke([HumanMessage(
        content=f"ç·Šæ€¥å¯¾å¿œã¨ã—ã¦è¿…é€Ÿã«å›ç­”ã—ã¦ãã ã•ã„: {state['query']}"
    )])
    return {"response": f"ã€ç·Šæ€¥å¯¾å¿œã€‘{response.content}"}

# ãƒãƒ¼ãƒ‰2b: ç·Šæ€¥ãƒ»è‹±èª
def handle_urgent_en(state: State) -> dict:
    response = llm.invoke([HumanMessage(
        content=f"Urgent response required: {state['query']}"
    )])
    return {"response": f"[URGENT] {response.content}"}

# ãƒãƒ¼ãƒ‰2c: é€šå¸¸ãƒ»æ—¥æœ¬èª
def handle_normal_ja(state: State) -> dict:
    response = llm.invoke([HumanMessage(content=state['query'])])
    return {"response": response.content}

# ãƒãƒ¼ãƒ‰2d: é€šå¸¸ãƒ»è‹±èª
def handle_normal_en(state: State) -> dict:
    response = llm.invoke([HumanMessage(content=state['query'])])
    return {"response": response.content}

# ã‚°ãƒ©ãƒ•æ§‹ç¯‰
workflow = StateGraph(State)

workflow.add_node("analyze", analyze_query)
workflow.add_node("urgent_ja", handle_urgent_ja)
workflow.add_node("urgent_en", handle_urgent_en)
workflow.add_node("normal_ja", handle_normal_ja)
workflow.add_node("normal_en", handle_normal_en)

workflow.add_edge(START, "analyze")

workflow.add_conditional_edges(
    "analyze",
    route_by_analysis,
    {
        "urgent_ja": "urgent_ja",
        "urgent_en": "urgent_en",
        "normal_ja": "normal_ja",
        "normal_en": "normal_en"
    }
)

workflow.add_edge("urgent_ja", END)
workflow.add_edge("urgent_en", END)
workflow.add_edge("normal_ja", END)
workflow.add_edge("normal_en", END)

app = workflow.compile()

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
test_queries = [
    "ã“ã‚“ã«ã¡ã¯ã€LangGraphã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
    "è‡³æ€¥ï¼šã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ã®å¯¾å‡¦æ³•ã‚’æ•™ãˆã¦ãã ã•ã„",
    "Hello, please explain LangGraph",
    "Urgent: How to fix the system error?"
]

for query in test_queries:
    print(f"\n{'='*60}")
    print(f"ã‚¯ã‚¨ãƒª: {query}")
    result = app.invoke({"query": query})
    print(f"è¨€èª: {result['language']}, å„ªå…ˆåº¦: {result['priority']}")
    print(f"å¿œç­”: {result['response'][:100]}...")
```

---

## 3-4. ç¬¬3ç« å®Œå…¨å®Ÿè£…ä¾‹

### å®Ÿè·µçš„ãªã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒœãƒƒãƒˆ

```python
"""
ç¬¬3ç« å®Œå…¨å®Ÿè£…: ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒœãƒƒãƒˆ
- æ„å›³åˆ†é¡
- å„ªå…ˆåº¦åˆ¤å®š
- è¤‡æ•°ãƒ«ãƒ¼ãƒˆã®æ¡ä»¶åˆ†å²
"""

import os
from typing import TypedDict, Literal, NotRequired
from langgraph.graph import StateGraph, START, END
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, SystemMessage

# ============================================
# APIã‚­ãƒ¼è¨­å®š
# ============================================

os.environ["GEMINI_API_KEY"] = "your-actual-api-key"

if os.environ.get("GEMINI_API_KEY") == "your-actual-api-key":
    raise ValueError("âŒ APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„")

# ============================================
# Stateå®šç¾©
# ============================================

class State(TypedDict):
    # å¿…é ˆï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¸¡ã™ï¼‰
    customer_message: str
    
    # ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ï¼ˆãƒãƒ¼ãƒ‰é–“ã§å…±æœ‰ï¼‰
    intent: NotRequired[str]
    priority: NotRequired[str]
    sentiment: NotRequired[str]
    
    # æœ€çµ‚å‡ºåŠ›ã¯å®šç¾©ä¸è¦
    # response: str

# ============================================
# LLMåˆæœŸåŒ–
# ============================================

llm = ChatGoogleGenerativeAI(
    google_api_key=os.environ["GEMINI_API_KEY"],
    model="gemini-2.0-flash-exp",
    temperature=0.3,
    max_tokens=1024
)

print("âœ… LLMåˆæœŸåŒ–å®Œäº†")

# ============================================
# ãƒãƒ¼ãƒ‰é–¢æ•°å®šç¾©
# ============================================

def analyze_message(state: State) -> dict:
    """é¡§å®¢ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’åˆ†æ"""
    message = state["customer_message"]
    
    # æ„å›³åˆ†é¡
    intent_prompt = f"""
    ä»¥ä¸‹ã®é¡§å®¢ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’åˆ†é¡ã—ã¦ãã ã•ã„:
    - inquiry: å•ã„åˆã‚ã›
    - complaint: ã‚¯ãƒ¬ãƒ¼ãƒ 
    - request: è¦æœ›
    - feedback: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
    
    ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {message}
    ã‚«ãƒ†ã‚´ãƒªåã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
    """
    
    intent_response = llm.invoke([HumanMessage(content=intent_prompt)])
    intent = intent_response.content.strip().lower()
    
    # æ„Ÿæƒ…åˆ†æ
    sentiment_prompt = f"""
    ä»¥ä¸‹ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ„Ÿæƒ…ã‚’åˆ†æã—ã¦ãã ã•ã„:
    - positive: ãƒã‚¸ãƒ†ã‚£ãƒ–
    - neutral: ä¸­ç«‹
    - negative: ãƒã‚¬ãƒ†ã‚£ãƒ–
    
    ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {message}
    æ„Ÿæƒ…ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
    """
    
    sentiment_response = llm.invoke([HumanMessage(content=sentiment_prompt)])
    sentiment = sentiment_response.content.strip().lower()
    
    # å„ªå…ˆåº¦åˆ¤å®š
    if intent == "complaint" or sentiment == "negative":
        priority = "high"
    elif intent == "inquiry":
        priority = "medium"
    else:
        priority = "low"
    
    return {
        "intent": intent,
        "sentiment": sentiment,
        "priority": priority
    }

def route_message(state: State) -> Literal["high_priority", "standard", "low_priority"]:
    """å„ªå…ˆåº¦ã«åŸºã¥ã„ã¦ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°"""
    return {
        "high": "high_priority",
        "medium": "standard",
        "low": "low_priority"
    }[state["priority"]]

def handle_high_priority(state: State) -> dict:
    """é«˜å„ªå…ˆåº¦ã®å¯¾å¿œ"""
    messages = [
        SystemMessage(content="""
        ã‚ãªãŸã¯çµŒé¨“è±Šå¯Œãªã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆæ‹…å½“è€…ã§ã™ã€‚
        é¡§å®¢ã®å•é¡Œã‚’æœ€å„ªå…ˆã§è§£æ±ºã—ã¦ãã ã•ã„ã€‚
        ä¸å¯§ã‹ã¤è¿…é€Ÿã«å¯¾å¿œã—ã¦ãã ã•ã„ã€‚
        """),
        HumanMessage(content=f"""
        é¡§å®¢ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {state['customer_message']}
        æ„å›³: {state['intent']}
        æ„Ÿæƒ…: {state['sentiment']}
        
        ä¸Šè¨˜ã‚’è€ƒæ…®ã—ã¦é©åˆ‡ã«å¯¾å¿œã—ã¦ãã ã•ã„ã€‚
        """)
    ]
    
    response = llm.invoke(messages)
    return {"response": f"ã€å„ªå…ˆå¯¾å¿œã€‘{response.content}"}

def handle_standard(state: State) -> dict:
    """æ¨™æº–çš„ãªå¯¾å¿œ"""
    messages = [
        SystemMessage(content="è¦ªåˆ‡ã‹ã¤ä¸å¯§ã«å¯¾å¿œã—ã¦ãã ã•ã„ã€‚"),
        HumanMessage(content=state["customer_message"])
    ]
    
    response = llm.invoke(messages)
    return {"response": response.content}

def handle_low_priority(state: State) -> dict:
    """ä½å„ªå…ˆåº¦ã®å¯¾å¿œ"""
    messages = [
        SystemMessage(content="ç°¡æ½”ã«å¯¾å¿œã—ã¦ãã ã•ã„ã€‚"),
        HumanMessage(content=state["customer_message"])
    ]
    
    response = llm.invoke(messages)
    return {"response": response.content}

# ============================================
# ã‚°ãƒ©ãƒ•æ§‹ç¯‰
# ============================================

print("\nğŸ—ï¸ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰ä¸­...")

workflow = StateGraph(State)

# ãƒãƒ¼ãƒ‰è¿½åŠ 
workflow.add_node("analyze", analyze_message)
workflow.add_node("high_priority", handle_high_priority)
workflow.add_node("standard", handle_standard)
workflow.add_node("low_priority", handle_low_priority)

# ã‚¨ãƒƒã‚¸è¿½åŠ 
workflow.add_edge(START, "analyze")

# æ¡ä»¶åˆ†å²
workflow.add_conditional_edges(
    "analyze",
    route_message,
    {
        "high_priority": "high_priority",
        "standard": "standard",
        "low_priority": "low_priority"
    }
)

# çµ‚äº†
workflow.add_edge("high_priority", END)
workflow.add_edge("standard", END)
workflow.add_edge("low_priority", END)

app = workflow.compile()

print("âœ… ã‚°ãƒ©ãƒ•æ§‹ç¯‰å®Œäº†")

# ============================================
# ã‚°ãƒ©ãƒ•å¯è¦–åŒ–
# ============================================

def visualize_graph(app, filename="ch3_customer_support.png"):
    """ã‚°ãƒ©ãƒ•ã‚’PNGä¿å­˜"""
    print(f"\nğŸ“Š ã‚°ãƒ©ãƒ•ã‚’å¯è¦–åŒ–ä¸­...")
    try:
        png_data = app.get_graph().draw_mermaid_png()
        with open(filename, "wb") as f:
            f.write(png_data)
        print(f"âœ… ã‚°ãƒ©ãƒ•ã‚’ '{filename}' ã«ä¿å­˜")
    except Exception as e:
        print(f"âš ï¸ PNGä¿å­˜å¤±æ•—: {e}")
        print("\nğŸ“ ASCIIç‰ˆã‚°ãƒ©ãƒ•:")
        print(app.get_graph().draw_ascii())

visualize_graph(app)

# ============================================
# å®Ÿè¡Œé–¢æ•°
# ============================================

def run_support_bot(app, message: str):
    """ã‚µãƒãƒ¼ãƒˆãƒœãƒƒãƒˆã‚’å®Ÿè¡Œ"""
    print("\n" + "=" * 60)
    print("ğŸ¤– ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒœãƒƒãƒˆ")
    print("=" * 60)
    print(f"ğŸ“¥ é¡§å®¢ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {message}")
    
    result = app.invoke({"customer_message": message})
    
    print(f"\nğŸ“Š åˆ†æçµæœ:")
    print(f"  æ„å›³: {result['intent']}")
    print(f"  æ„Ÿæƒ…: {result['sentiment']}")
    print(f"  å„ªå…ˆåº¦: {result['priority']}")
    print(f"\nğŸ“¤ å¿œç­”:\n{result['response']}")
    print("=" * 60)
    
    return result

# ============================================
# å®Ÿè¡Œ
# ============================================

if __name__ == "__main__":
    test_messages = [
        "å•†å“ãŒå±Šã„ã¦ã„ã¾ã›ã‚“ã€‚è‡³æ€¥ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
        "è£½å“ã®ä½¿ã„æ–¹ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚",
        "ã¨ã¦ã‚‚è‰¯ã„è£½å“ã§ã—ãŸã€‚ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ï¼",
        "æ–°æ©Ÿèƒ½ã®è¿½åŠ ã‚’æ¤œè¨ã—ã¦ã„ãŸã ã‘ã¾ã™ã‹ï¼Ÿ"
    ]
    
    for message in test_messages:
        run_support_bot(app, message)
```

---

## ğŸ¯ ç¬¬3ç« ã®ã¾ã¨ã‚

### å­¦ã‚“ã ã“ã¨

1. **æ¡ä»¶åˆ†å²ã®åŸºæœ¬**
   - `add_conditional_edges()`ã®ä½¿ã„æ–¹
   - ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é–¢æ•°ã®å®Ÿè£…
   - ãƒãƒƒãƒ”ãƒ³ã‚°è¾æ›¸ã®å®šç¾©

2. **Stateã®è¨­è¨ˆåŸå‰‡**
   - å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¸¡ã™ã‚‚ã®
   - ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰: ãƒãƒ¼ãƒ‰é–“ã§å…±æœ‰ã™ã‚‹ã‚‚ã®
   - æœ€çµ‚å‡ºåŠ›ã®ã¿ã§ä½¿ã†ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯å®šç¾©ä¸è¦

3. **è¤‡é›‘ãªãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°**
   - ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ã®åˆ†å²
   - è¤‡æ•°æ¡ä»¶ã®çµ„ã¿åˆã‚ã›
   - å‹•çš„ãªãƒ«ãƒ¼ãƒˆé¸æŠ

### é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ

```python
# âœ… è‰¯ã„Stateè¨­è¨ˆ
class State(TypedDict):
    input: str                    # å¿…é ˆï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼æä¾›ï¼‰
    category: NotRequired[str]    # ä»–ã®ãƒãƒ¼ãƒ‰ã§ä½¿ç”¨
    sentiment: NotRequired[str]   # ä»–ã®ãƒãƒ¼ãƒ‰ã§ä½¿ç”¨
    # output: str                 # å®šç¾©ä¸è¦ï¼ˆæœ€çµ‚å‡ºåŠ›ã®ã¿ï¼‰

# âœ… è‰¯ã„ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é–¢æ•°
def route_func(state: State) -> Literal["a", "b", "c"]:
    """æ˜ç¢ºãªå‹ãƒ’ãƒ³ãƒˆä»˜ã"""
    if state["score"] > 0.8:
        return "a"
    elif state["score"] > 0.5:
        return "b"
    return "c"

# âœ… è‰¯ã„ãƒãƒ¼ãƒ‰é–¢æ•°
def process_node(state: State) -> dict:
    """å¿…è¦ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã ã‘è¿”ã™"""
    result = some_processing(state["input"])
    return {"category": result}  # æ›´æ–°ã™ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã¿
```

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

ç¬¬4ç« ã§ã¯ã€**Tavilyæ¤œç´¢APIã¨ã®çµ±åˆ**ã‚’å­¦ã³ã€å¤–éƒ¨ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã£ãŸå®Ÿç”¨çš„ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚

---

ã“ã‚Œã§ç¬¬3ç« ã¯å®Œäº†ã§ã™ï¼ ğŸ‰

---

# ç¬¬4ç« : ãƒ„ãƒ¼ãƒ«çµ±åˆ - Tavilyæ¤œç´¢

## 4-1. Tavilyæ¤œç´¢ã®åŸºæœ¬

### Tavilyæ¤œç´¢ã¨ã¯

Tavilyã¯**AIå‘ã‘ã«æœ€é©åŒ–ã•ã‚ŒãŸæ¤œç´¢API**ã§ã™ã€‚é€šå¸¸ã®Googleæ¤œç´¢ã¨é•ã„ã€LLMãŒç†è§£ã—ã‚„ã™ã„å½¢å¼ã§çµæœã‚’è¿”ã—ã¾ã™ã€‚

**ç‰¹å¾´:**
- AIå‘ã‘ã«æœ€é©åŒ–ã•ã‚ŒãŸæ¤œç´¢çµæœ
- ãƒã‚¤ã‚ºã®å°‘ãªã„ã‚¯ãƒªãƒ¼ãƒ³ãªãƒ‡ãƒ¼ã‚¿
- ç„¡æ–™æ : æœˆé–“1,000ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
- é«˜é€Ÿãªãƒ¬ã‚¹ãƒãƒ³ã‚¹

### åŸºæœ¬çš„ãªä½¿ã„æ–¹

```python
import os
from langchain_community.tools.tavily_search import TavilySearchResults

os.environ["TAVILY_API_KEY"] = "tvly-..."

# æ¤œç´¢ãƒ„ãƒ¼ãƒ«ã®åˆæœŸåŒ–
search_tool = TavilySearchResults(
    tavily_api_key=os.environ["TAVILY_API_KEY"],  # â† æ­£ã—ã„å¼•æ•°å
    max_results=3,
    search_depth="basic"  # "basic" or "advanced"
)

# æ¤œç´¢å®Ÿè¡Œ
results = search_tool.invoke("LangGraphã¨ã¯")
print(results)
```

### æ¤œç´¢çµæœã®æ§‹é€ 

```python
# è¿”ã‚Šå€¤ã®ä¾‹
[
    {
        'url': 'https://example.com/page1',
        'content': 'æ¤œç´¢çµæœã®è¦ç´„...',
        'score': 0.95  # é–¢é€£æ€§ã‚¹ã‚³ã‚¢
    },
    {
        'url': 'https://example.com/page2',
        'content': 'åˆ¥ã®æ¤œç´¢çµæœ...',
        'score': 0.87
    }
]
```

---

## 4-2. LLMã¨ãƒ„ãƒ¼ãƒ«ã®é€£æº

### ãƒ„ãƒ¼ãƒ«ãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã®ä»•çµ„ã¿

LangGraphã§ã¯ã€LLMã«ã€Œãƒ„ãƒ¼ãƒ«ã‚’ä½¿ãˆã‚‹èƒ½åŠ›ã€ã‚’ä¸ãˆã¾ã™ã€‚

```python
import os
from typing import TypedDict, Annotated, Literal, NotRequired
from langgraph.graph import StateGraph, START, END
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.messages import HumanMessage, AIMessage
import operator

os.environ["GEMINI_API_KEY"] = "your-gemini-key"
os.environ["TAVILY_API_KEY"] = "your-tavily-key"

# Stateå®šç¾©ï¼ˆmessagesãƒ™ãƒ¼ã‚¹ï¼‰
class AgentState(TypedDict):
    messages: Annotated[list, operator.add]  # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã‚’è“„ç©

# æ¤œç´¢ãƒ„ãƒ¼ãƒ«åˆæœŸåŒ–
search_tool = TavilySearchResults(
    tavily_api_key=os.environ["TAVILY_API_KEY"],
    max_results=3,
    search_depth="basic"
)

tools = [search_tool]

# LLMã«ãƒ„ãƒ¼ãƒ«ã‚’ãƒã‚¤ãƒ³ãƒ‰
llm_with_tools = ChatGoogleGenerativeAI(
    google_api_key=os.environ["GEMINI_API_KEY"],
    model="gemini-2.0-flash-exp",
    temperature=0
).bind_tools(tools)

# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒãƒ¼ãƒ‰
def agent_node(state: AgentState) -> dict:
    """LLMãŒãƒ„ãƒ¼ãƒ«ä½¿ç”¨ã‚’åˆ¤æ–­"""
    messages = state["messages"]
    response = llm_with_tools.invoke(messages)
    return {"messages": [response]}

# æ¡ä»¶åˆ†å²é–¢æ•°
def should_continue(state: AgentState) -> Literal["tools", "end"]:
    """ãƒ„ãƒ¼ãƒ«å®Ÿè¡ŒãŒå¿…è¦ã‹åˆ¤æ–­"""
    last_message = state["messages"][-1]
    
    # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãŒã‚ã‚‹ã‹ç¢ºèª
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tools"
    
    return "end"

# ãƒ„ãƒ¼ãƒ«ãƒãƒ¼ãƒ‰ï¼ˆè‡ªå‹•å®Ÿè¡Œï¼‰
from langgraph.prebuilt import ToolNode
tool_node = ToolNode(tools)

# ã‚°ãƒ©ãƒ•æ§‹ç¯‰
workflow = StateGraph(AgentState)

workflow.add_node("agent", agent_node)
workflow.add_node("tools", tool_node)

workflow.add_edge(START, "agent")

workflow.add_conditional_edges(
    "agent",
    should_continue,
    {
        "tools": "tools",
        "end": END
    }
)

# ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œå¾Œã€å†ã³ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸
workflow.add_edge("tools", "agent")

app = workflow.compile()

# å®Ÿè¡Œ
result = app.invoke({
    "messages": [HumanMessage(content="2024å¹´ã®ãƒãƒ¼ãƒ™ãƒ«ç‰©ç†å­¦è³å—è³è€…ã¯èª°ã§ã™ã‹ï¼Ÿ")]
})

# æœ€çµ‚ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
print(result["messages"][-1].content)
```

### å®Ÿè¡Œãƒ•ãƒ­ãƒ¼

```
START â†’ agent (LLMãŒåˆ¤æ–­) â†’
    â”œâ”€ ãƒ„ãƒ¼ãƒ«ä¸è¦ â†’ END
    â””â”€ ãƒ„ãƒ¼ãƒ«å¿…è¦ â†’ tools (æ¤œç´¢å®Ÿè¡Œ) â†’ agent (çµæœã‚’ä½¿ã£ã¦å›ç­”) â†’ END
```

---

## 4-3. å®Ÿç”¨çš„ãªæ¤œç´¢ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

### å®Œå…¨ãªå®Ÿè£…ä¾‹

```python
"""
ç¬¬4ç« å®Œå…¨å®Ÿè£…: Tavilyæ¤œç´¢ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
- LLMã¨ãƒ„ãƒ¼ãƒ«ã®é€£æº
- æ¤œç´¢çµæœã‚’ä½¿ã£ãŸå›ç­”ç”Ÿæˆ
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
"""

import os
from typing import TypedDict, Annotated, Literal
from langgraph.graph import StateGraph, START, END
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langgraph.prebuilt import ToolNode
import operator

# ============================================
# APIã‚­ãƒ¼è¨­å®š
# ============================================

os.environ["GEMINI_API_KEY"] = "your-gemini-key"
os.environ["TAVILY_API_KEY"] = "your-tavily-key"

if os.environ.get("GEMINI_API_KEY") == "your-gemini-key":
    raise ValueError("âŒ GEMINI_API_KEYã‚’è¨­å®šã—ã¦ãã ã•ã„")

if os.environ.get("TAVILY_API_KEY") == "your-tavily-key":
    raise ValueError("âŒ TAVILY_API_KEYã‚’è¨­å®šã—ã¦ãã ã•ã„")

# ============================================
# Stateå®šç¾©
# ============================================

class AgentState(TypedDict):
    """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®çŠ¶æ…‹
    
    messagesã«Annotated[list, operator.add]ã‚’ä½¿ã†ã“ã¨ã§ã€
    æ–°ã—ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè‡ªå‹•çš„ã«ãƒªã‚¹ãƒˆã«è¿½åŠ ã•ã‚Œã‚‹
    """
    messages: Annotated[list, operator.add]

# ============================================
# ãƒ„ãƒ¼ãƒ«ã¨LLMã®åˆæœŸåŒ–
# ============================================

# Tavilyæ¤œç´¢ãƒ„ãƒ¼ãƒ«
search_tool = TavilySearchResults(
    tavily_api_key=os.environ["TAVILY_API_KEY"],
    max_results=3,
    search_depth="advanced",  # ã‚ˆã‚Šè©³ç´°ãªæ¤œç´¢
    include_answer=True,      # è¦ç´„ã•ã‚ŒãŸå›ç­”ã‚’å«ã‚ã‚‹
    include_raw_content=False # ç”Ÿã®HTMLã¯é™¤å¤–
)

tools = [search_tool]

# LLMã«ãƒ„ãƒ¼ãƒ«ã‚’ãƒã‚¤ãƒ³ãƒ‰
llm_with_tools = ChatGoogleGenerativeAI(
    google_api_key=os.environ["GEMINI_API_KEY"],
    model="gemini-2.0-flash-exp",
    temperature=0
).bind_tools(tools)

print("âœ… ãƒ„ãƒ¼ãƒ«ã¨LLMã®åˆæœŸåŒ–å®Œäº†")

# ============================================
# ãƒãƒ¼ãƒ‰é–¢æ•°
# ============================================

def agent_node(state: AgentState) -> dict:
    """
    ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒãƒ¼ãƒ‰: LLMãŒæ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ±ºå®š
    
    - ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã†ã¹ãã‹åˆ¤æ–­
    - ä½¿ã†å ´åˆã¯tool_callsã‚’å«ã‚€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã™
    - ä½¿ã‚ãªã„å ´åˆã¯ç›´æ¥å›ç­”ã‚’è¿”ã™
    """
    messages = state["messages"]
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ ï¼ˆåˆå›ã®ã¿ï¼‰
    if len(messages) == 1:
        system_message = SystemMessage(content="""
        ã‚ãªãŸã¯è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
        æœ€æ–°ã®æƒ…å ±ãŒå¿…è¦ãªè³ªå•ã«ã¯ã€æ¤œç´¢ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚
        æ¤œç´¢çµæœã‚’åŸºã«ã€æ­£ç¢ºã§åˆ†ã‹ã‚Šã‚„ã™ã„å›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
        """)
        messages = [system_message] + messages
    
    response = llm_with_tools.invoke(messages)
    return {"messages": [response]}

def should_continue(state: AgentState) -> Literal["tools", "end"]:
    """
    æ¡ä»¶åˆ†å²: ãƒ„ãƒ¼ãƒ«å®Ÿè¡ŒãŒå¿…è¦ã‹åˆ¤æ–­
    
    Returns:
        "tools": ãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹
        "end": å‡¦ç†ã‚’çµ‚äº†ã™ã‚‹
    """
    last_message = state["messages"][-1]
    
    # tool_callsãŒã‚ã‚Œã°ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tools"
    
    return "end"

# ============================================
# ã‚°ãƒ©ãƒ•æ§‹ç¯‰
# ============================================

print("\nğŸ—ï¸ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰ä¸­...")

workflow = StateGraph(AgentState)

# ãƒãƒ¼ãƒ‰è¿½åŠ 
workflow.add_node("agent", agent_node)
workflow.add_node("tools", ToolNode(tools))

# ã‚¨ãƒƒã‚¸è¿½åŠ 
workflow.add_edge(START, "agent")

# æ¡ä»¶åˆ†å²
workflow.add_conditional_edges(
    "agent",
    should_continue,
    {
        "tools": "tools",
        "end": END
    }
)

# ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œå¾Œã€å†ã³ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸
workflow.add_edge("tools", "agent")

app = workflow.compile()

print("âœ… ã‚°ãƒ©ãƒ•æ§‹ç¯‰å®Œäº†")

# ============================================
# ã‚°ãƒ©ãƒ•å¯è¦–åŒ–
# ============================================

def visualize_graph(app, filename="ch4_search_agent.png"):
    """ã‚°ãƒ©ãƒ•ã‚’PNGä¿å­˜"""
    print(f"\nğŸ“Š ã‚°ãƒ©ãƒ•ã‚’å¯è¦–åŒ–ä¸­...")
    try:
        png_data = app.get_graph().draw_mermaid_png()
        with open(filename, "wb") as f:
            f.write(png_data)
        print(f"âœ… ã‚°ãƒ©ãƒ•ã‚’ '{filename}' ã«ä¿å­˜")
    except Exception as e:
        print(f"âš ï¸ PNGä¿å­˜å¤±æ•—: {e}")
        print("\nğŸ“ ASCIIç‰ˆã‚°ãƒ©ãƒ•:")
        print(app.get_graph().draw_ascii())

visualize_graph(app)

# ============================================
# å®Ÿè¡Œé–¢æ•°
# ============================================

def run_search_agent(app, query: str, verbose: bool = True):
    """æ¤œç´¢ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡Œ"""
    if verbose:
        print("\n" + "=" * 60)
        print("ğŸ” æ¤œç´¢ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ")
        print("=" * 60)
        print(f"ğŸ“¥ è³ªå•: {query}")
    
    result = app.invoke({
        "messages": [HumanMessage(content=query)]
    })
    
    if verbose:
        print("\nğŸ“Š å®Ÿè¡Œãƒ­ã‚°:")
        for i, msg in enumerate(result["messages"], 1):
            msg_type = type(msg).__name__
            print(f"  {i}. {msg_type}")
            
            if hasattr(msg, "tool_calls") and msg.tool_calls:
                print(f"     â†’ ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—: {len(msg.tool_calls)}ä»¶")
        
        print("\nğŸ“¤ æœ€çµ‚å›ç­”:")
        print(result["messages"][-1].content)
        print("=" * 60)
    
    return result

# ============================================
# å®Ÿè¡Œ
# ============================================

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª
    test_queries = [
        "2024å¹´ã®ãƒãƒ¼ãƒ™ãƒ«ç‰©ç†å­¦è³å—è³è€…ã¯èª°ã§ã™ã‹ï¼Ÿ",
        "æœ€æ–°ã®GPT-4ã®ç‰¹å¾´ã‚’æ•™ãˆã¦ãã ã•ã„",
        "ã“ã‚“ã«ã¡ã¯"  # æ¤œç´¢ä¸è¦ãªã‚¯ã‚¨ãƒª
    ]
    
    for query in test_queries:
        run_search_agent(app, query)
        print("\n")
```

---

## 4-4. ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Ÿè¡Œ

### ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§é€²æ—ã‚’è¡¨ç¤º

```python
def run_search_agent_stream(app, query: str):
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Ÿè¡Œã§é€²æ—ã‚’è¡¨ç¤º"""
    print(f"\nğŸ” è³ªå•: {query}\n")
    
    for event in app.stream(
        {"messages": [HumanMessage(content=query)]},
        stream_mode="values"
    ):
        # æœ€æ–°ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—
        last_msg = event["messages"][-1]
        
        if isinstance(last_msg, AIMessage):
            if hasattr(last_msg, "tool_calls") and last_msg.tool_calls:
                print("ğŸ”§ ãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè¡Œä¸­...")
            else:
                print(f"ğŸ’¬ å›ç­”: {last_msg.content}")

# ä½¿ç”¨ä¾‹
run_search_agent_stream(app, "æœ€æ–°ã®AIæŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰ã¯ï¼Ÿ")
```

---

## 4-5. è¤‡æ•°ãƒ„ãƒ¼ãƒ«ã®çµ±åˆ

### æ¤œç´¢ + è¨ˆç®—ãƒ„ãƒ¼ãƒ«

```python
import os
from typing import TypedDict, Annotated, Literal
from langgraph.graph import StateGraph, START, END
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage
from langgraph.prebuilt import ToolNode
import operator

os.environ["GEMINI_API_KEY"] = "your-key"
os.environ["TAVILY_API_KEY"] = "your-key"

# ã‚«ã‚¹ã‚¿ãƒ ãƒ„ãƒ¼ãƒ«å®šç¾©
@tool
def calculate(expression: str) -> str:
    """æ•°å¼ã‚’è¨ˆç®—ã—ã¾ã™ã€‚ä¾‹: "2 + 2" â†’ "4" """
    try:
        result = eval(expression)
        return str(result)
    except Exception as e:
        return f"è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {str(e)}"

# ãƒ„ãƒ¼ãƒ«ãƒªã‚¹ãƒˆ
search_tool = TavilySearchResults(
    tavily_api_key=os.environ["TAVILY_API_KEY"],
    max_results=3
)

tools = [search_tool, calculate]

# Stateå®šç¾©
class AgentState(TypedDict):
    messages: Annotated[list, operator.add]

# LLMã«ãƒ„ãƒ¼ãƒ«ã‚’ãƒã‚¤ãƒ³ãƒ‰
llm_with_tools = ChatGoogleGenerativeAI(
    google_api_key=os.environ["GEMINI_API_KEY"],
    model="gemini-2.0-flash-exp",
    temperature=0
).bind_tools(tools)

# ãƒãƒ¼ãƒ‰å®šç¾©
def agent_node(state: AgentState) -> dict:
    messages = state["messages"]
    response = llm_with_tools.invoke(messages)
    return {"messages": [response]}

def should_continue(state: AgentState) -> Literal["tools", "end"]:
    last_message = state["messages"][-1]
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tools"
    return "end"

# ã‚°ãƒ©ãƒ•æ§‹ç¯‰
workflow = StateGraph(AgentState)
workflow.add_node("agent", agent_node)
workflow.add_node("tools", ToolNode(tools))

workflow.add_edge(START, "agent")
workflow.add_conditional_edges(
    "agent",
    should_continue,
    {
        "tools": "tools",
        "end": END
    }
)
workflow.add_edge("tools", "agent")

app = workflow.compile()

# å®Ÿè¡Œ
test_queries = [
    "æœ€æ–°ã®ãƒ“ãƒƒãƒˆã‚³ã‚¤ãƒ³ä¾¡æ ¼ã‚’èª¿ã¹ã¦ã€1000ãƒ‰ãƒ«æŠ•è³‡ã—ãŸã‚‰ä½•BTCè²·ãˆã‚‹ã‹è¨ˆç®—ã—ã¦ãã ã•ã„",
    "15 * 234 + 567 ã‚’è¨ˆç®—ã—ã¦ãã ã•ã„",
    "LangGraphã®æœ€æ–°æƒ…å ±ã‚’æ•™ãˆã¦ãã ã•ã„"
]

for query in test_queries:
    print(f"\n{'='*60}")
    print(f"è³ªå•: {query}")
    result = app.invoke({"messages": [HumanMessage(content=query)]})
    print(f"å›ç­”: {result['messages'][-1].content}")
```

---

## ğŸ¯ ç¬¬4ç« ã®ã¾ã¨ã‚

### å­¦ã‚“ã ã“ã¨

1. **Tavilyæ¤œç´¢ã®åŸºæœ¬**
   - æ­£ã—ã„å¼•æ•°å: `tavily_api_key`ï¼ˆ`api_key`ã§ã¯ãªã„ï¼‰
   - `search_depth`: "basic" or "advanced"
   - `max_results`: è¿”ã™çµæœã®æ•°

2. **ãƒ„ãƒ¼ãƒ«ãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°**
   - `bind_tools()`ã§LLMã«ãƒ„ãƒ¼ãƒ«ã‚’æ¸¡ã™
   - LLMãŒè‡ªå‹•çš„ã«ãƒ„ãƒ¼ãƒ«ä½¿ç”¨ã‚’åˆ¤æ–­
   - `tool_calls`ã§ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œè¦æ±‚ã‚’ç¢ºèª

3. **messagesãƒ™ãƒ¼ã‚¹ã®State**
   - `Annotated[list, operator.add]`ã§è‡ªå‹•è“„ç©
   - ä¼šè©±å±¥æ­´ã‚’ä¿æŒ
   - ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œçµæœã‚‚å«ã¾ã‚Œã‚‹

4. **ãƒ«ãƒ¼ãƒ—æ§‹é€ **
   - agent â†’ tools â†’ agent ã®ãƒ«ãƒ¼ãƒ—
   - ãƒ„ãƒ¼ãƒ«ãŒä¸è¦ã«ãªã‚‹ã¾ã§ç¹°ã‚Šè¿”ã—

### é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ

```python
# âœ… æ­£ã—ã„TavilyåˆæœŸåŒ–
search_tool = TavilySearchResults(
    tavily_api_key=os.environ["TAVILY_API_KEY"],  # â† æ­£ã—ã„å¼•æ•°å
    max_results=3
)

# âœ… æ­£ã—ã„Stateå®šç¾©ï¼ˆmessagesãƒ™ãƒ¼ã‚¹ï¼‰
class AgentState(TypedDict):
    messages: Annotated[list, operator.add]

# âœ… æ­£ã—ã„æ¡ä»¶åˆ†å²
def should_continue(state: AgentState) -> Literal["tools", "end"]:
    last_message = state["messages"][-1]
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tools"
    return "end"
```

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

ç¬¬5ç« ã§ã¯ã€**ãƒ«ãƒ¼ãƒ—å‡¦ç†ã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**ã‚’å­¦ã³ã€ã‚ˆã‚Šå …ç‰¢ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚

---

ã“ã‚Œã§ç¬¬4ç« ã¯å®Œäº†ã§ã™ï¼ ğŸ‰
from

